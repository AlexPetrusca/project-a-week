{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:27.640378Z",
     "start_time": "2025-03-12T07:44:26.714479Z"
    }
   },
   "source": [
    "# model performance from training on bigrams isn't great.\n",
    "#   - the only context we have is the last character.\n",
    "#   - evidently, this is not how names or words \"work\" - we need more context.\n",
    "# if we extend the technique from our \"manually tuned\" bigram model, things get out of hand quickly:\n",
    "#   - we can predict the next character from the last two characters, instead of just the last character.\n",
    "#   - however, now we need to tune 27 * 27 = 729 parameters instead of 27.\n",
    "#   - using the last three characters, this becomes 27 * 27 * 27 = 19683 parameters.\n",
    "#   - O(27^n) parameters - this quickly becomes infeasible.\n",
    "# what we'll be doing now is solving the problem with an MLP, following Bengio et al. 2003\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_default_device(\"mps\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:27.650334Z",
     "start_time": "2025-03-12T07:44:27.647136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# read in all the words\n",
    "words = open('res/names.txt', 'r').read().split()\n",
    "print(words[:8])\n",
    "print(len(words))"
   ],
   "id": "73905d2b25424c5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
      "32033\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:27.710438Z",
     "start_time": "2025-03-12T07:44:27.706820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build the vocabulary of characters and mappings to/from integer ids\n",
    "chars = sorted(list(set(''.join(words))))  # tokens a-z\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}  # map each token to unique id\n",
    "stoi['.'] = 0  # add encoding for terminating token\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}  # create the reverse mapping\n",
    "print(stoi)\n",
    "print(itos)"
   ],
   "id": "a2e76e86fdfa432d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:27.718301Z",
     "start_time": "2025-03-12T07:44:27.716711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define hyperparameters we'll use later\n",
    "vocab_size = 27   # vocabulary size: how big is our vocabulary of tokens?\n",
    "ngram_len = 3     # context length: how many characters do we take to predict the next one?\n",
    "embed_dim = 2     # embedding dimension: what is the length of the embedding vector for each token?\n",
    "hidden_dim = 100  # how many neurons are in the hidden layer of the MLP?"
   ],
   "id": "492e5698647ffef8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:27.993583Z",
     "start_time": "2025-03-12T07:44:27.724125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# build the dataset\n",
    "X, Y = [], []\n",
    "for i, w in enumerate(words):\n",
    "  context = [0] * ngram_len\n",
    "  for ch in w + '.':\n",
    "    ix = stoi[ch]\n",
    "    X.append(context)\n",
    "    Y.append(ix)\n",
    "    if i < 3:\n",
    "      print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "    context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ],
   "id": "6ec9e73eb16cf999",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... ---> e\n",
      "..e ---> m\n",
      ".em ---> m\n",
      "emm ---> a\n",
      "mma ---> .\n",
      "... ---> o\n",
      "..o ---> l\n",
      ".ol ---> i\n",
      "oli ---> v\n",
      "liv ---> i\n",
      "ivi ---> a\n",
      "via ---> .\n",
      "... ---> a\n",
      "..a ---> v\n",
      ".av ---> a\n",
      "ava ---> .\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:28.031469Z",
     "start_time": "2025-03-12T07:44:28.026404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# embedding lookup table - squeeze 1 hot encoded token into 2-dimensional space\n",
    "C = torch.randn((vocab_size, embed_dim), requires_grad=True)"
   ],
   "id": "863442cef8dba496",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:28.200251Z",
     "start_time": "2025-03-12T07:44:28.104471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# lookup the embedding for every input in the training set\n",
    "#   - the shape of our input becomes [batch_size, ngram_len, embed_dim]\n",
    "emb = C[X]\n",
    "print(emb)\n",
    "print(emb.shape)"
   ],
   "id": "7e860167b8be7162",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.6106,  1.8206],\n",
      "         [ 0.6106,  1.8206],\n",
      "         [ 0.6106,  1.8206]],\n",
      "\n",
      "        [[ 0.6106,  1.8206],\n",
      "         [ 0.6106,  1.8206],\n",
      "         [ 0.8544,  0.5877]],\n",
      "\n",
      "        [[ 0.6106,  1.8206],\n",
      "         [ 0.8544,  0.5877],\n",
      "         [ 0.3054,  2.0366]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.9114,  1.2000],\n",
      "         [ 1.9114,  1.2000],\n",
      "         [ 1.8408,  0.7235]],\n",
      "\n",
      "        [[ 1.9114,  1.2000],\n",
      "         [ 1.8408,  0.7235],\n",
      "         [ 1.9114,  1.2000]],\n",
      "\n",
      "        [[ 1.8408,  0.7235],\n",
      "         [ 1.9114,  1.2000],\n",
      "         [ 1.6874, -1.1933]]], device='mps:0', grad_fn=<IndexBackward0>)\n",
      "torch.Size([228146, 3, 2])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:28.255581Z",
     "start_time": "2025-03-12T07:44:28.242517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hidden layer of MLP\n",
    "W1 = torch.randn((ngram_len * embed_dim, hidden_dim), requires_grad=True)\n",
    "b1 = torch.randn(hidden_dim, requires_grad=True)\n",
    "\n",
    "emb1 = emb.view(emb.shape[0], -1)  # reshape embeddings\n",
    "a1 = torch.tanh(emb1 @ W1 + b1)  # calculate activation of hidden layer\n",
    "print(a1.shape)"
   ],
   "id": "a07dcc13180e2013",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 100])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:28.450423Z",
     "start_time": "2025-03-12T07:44:28.441178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# output layer of MLP\n",
    "W2 = torch.randn((hidden_dim, vocab_size), requires_grad=True)\n",
    "b2 = torch.randn(vocab_size, requires_grad=True)\n",
    "\n",
    "logits = a1 @ W2 + b2  # calculate output activation\n",
    "print(logits.shape)"
   ],
   "id": "fe5ad2678937ea05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:28.651344Z",
     "start_time": "2025-03-12T07:44:28.639105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# apply softmax to output\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "print(probs.shape)\n",
    "print(\"Sum of first row:\", probs[0].sum().item())"
   ],
   "id": "f67a1f04a3c15a43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n",
      "Sum of first row: 0.9999998807907104\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:29.046621Z",
     "start_time": "2025-03-12T07:44:28.844882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# calculate NLL loss\n",
    "Y_pred = probs[torch.arange(0, len(Y)), Y]  # from each sample, select probability that we output the correct next token\n",
    "loss = -Y_pred.log().mean()  # log, negate and mean the selected probabilities\n",
    "print(loss)\n",
    "# alternatively, we can just use pytorch's cross entropy loss\n",
    "print(F.cross_entropy(logits, Y))"
   ],
   "id": "6d5d7ab1c3681fe2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.0064, device='mps:0', grad_fn=<NegBackward0>)\n",
      "tensor(16.0064, device='mps:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:29.285113Z",
     "start_time": "2025-03-12T07:44:29.256058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# backward pass\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.grad = None  # zero grad\n",
    "loss.backward()"
   ],
   "id": "ed82ca6a2ae18da1",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:44:30.080354Z",
     "start_time": "2025-03-12T07:44:29.944374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# update parameters\n",
    "for p in parameters:\n",
    "    p.data -= 0.1 * p.grad\n",
    "    print(p.grad)"
   ],
   "id": "4b40cdda2fe3fb03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9688, -0.3917],\n",
      "        [ 0.3161,  0.0612],\n",
      "        [ 0.0135,  0.0149],\n",
      "        [-0.1218, -0.1740],\n",
      "        [ 0.1167,  0.0534],\n",
      "        [ 0.1910,  0.1317],\n",
      "        [ 0.0302,  0.0169],\n",
      "        [-0.0099,  0.0047],\n",
      "        [-0.0940, -0.2259],\n",
      "        [-0.0649, -0.0011],\n",
      "        [-0.1143, -0.0973],\n",
      "        [-0.0188, -0.0502],\n",
      "        [ 0.0030,  0.0919],\n",
      "        [ 0.1930,  0.0542],\n",
      "        [-0.0782, -0.4885],\n",
      "        [ 0.0415,  0.0446],\n",
      "        [-0.0245, -0.0345],\n",
      "        [-0.0052,  0.0054],\n",
      "        [-0.3049, -0.3419],\n",
      "        [ 0.0788,  0.0575],\n",
      "        [ 0.0718, -0.0259],\n",
      "        [-0.0677, -0.0289],\n",
      "        [-0.0300, -0.1069],\n",
      "        [ 0.0214,  0.0093],\n",
      "        [ 0.0051,  0.0044],\n",
      "        [ 0.1481, -0.0598],\n",
      "        [ 0.0296, -0.0349]], device='mps:0')\n",
      "tensor([[-1.3975e-02, -9.2984e-02, -5.6051e-02,  2.7991e-02,  4.9425e-02,\n",
      "          2.7716e-02,  3.0799e-02, -6.8132e-02,  1.8786e-01,  5.2439e-02,\n",
      "          8.9711e-03,  1.7737e-02, -6.3090e-02,  2.7649e-02,  4.3962e-02,\n",
      "         -1.6834e-01, -5.1853e-02,  4.0070e-02,  9.0036e-02, -8.7890e-02,\n",
      "          6.6895e-02, -2.8715e-02, -2.4588e-02,  9.0105e-03,  3.8718e-02,\n",
      "          2.4956e-02, -9.0153e-03, -4.4320e-02, -2.2689e-02, -1.5904e-02,\n",
      "          1.7232e-02,  3.6849e-02,  1.8577e-02, -8.0463e-02, -3.9740e-02,\n",
      "          1.1624e-02, -8.0395e-02,  7.0454e-03, -3.1837e-03,  5.9380e-02,\n",
      "         -3.6333e-02, -6.6001e-02,  9.3979e-03, -4.5120e-02,  8.0799e-02,\n",
      "          1.0594e-01,  4.1824e-02,  5.2272e-02,  1.1346e-02,  2.2334e-02,\n",
      "         -2.3951e-02,  2.2057e-02, -2.2191e-02, -1.2894e-02,  5.7386e-03,\n",
      "         -5.3221e-02,  6.2784e-02,  6.1178e-02,  8.7335e-02,  6.2316e-02,\n",
      "          3.0565e-02, -1.5250e-02,  6.0622e-02,  4.4618e-02,  4.5179e-02,\n",
      "         -4.3402e-02, -1.0750e-01,  8.0524e-02, -4.2673e-03,  5.0186e-02,\n",
      "         -5.7576e-02, -1.2396e-03,  1.2687e-02, -3.5650e-02, -1.7621e-01,\n",
      "          1.4752e-02, -2.4104e-02, -3.7489e-03, -1.0093e-02, -1.3790e-02,\n",
      "         -1.4102e-02, -1.2183e-01,  1.5110e-03,  2.0007e-03,  2.0507e-01,\n",
      "         -2.6862e-02,  9.8420e-02,  1.0161e-02, -5.4447e-02,  5.3468e-03,\n",
      "         -1.7717e-01,  8.5751e-03, -4.7427e-02, -2.1838e-02, -1.9274e-02,\n",
      "          8.2244e-03, -1.5509e-01, -4.7473e-02, -7.3262e-02,  1.6630e-02],\n",
      "        [-5.4584e-04, -2.8823e-02, -1.6837e-01,  1.7864e-01, -1.7420e-01,\n",
      "          4.5757e-02, -2.5668e-02, -6.6772e-02,  3.1391e-01,  1.2485e-01,\n",
      "          7.2825e-02, -2.0685e-02, -1.5716e-01,  6.6774e-02,  7.5413e-02,\n",
      "         -4.7835e-01, -1.4766e-01,  1.0589e-01,  1.2208e-01, -1.8129e-01,\n",
      "          4.8099e-02, -6.5295e-02,  6.6640e-02, -2.7393e-02,  8.3477e-02,\n",
      "          5.0420e-02, -1.8116e-01,  1.5292e-02,  5.0715e-02, -2.0505e-02,\n",
      "         -5.5780e-02,  9.4049e-02,  3.5014e-02, -2.9522e-01, -2.2476e-02,\n",
      "          1.1738e-02, -1.3951e-01, -2.7627e-02,  1.2258e-02,  9.3703e-02,\n",
      "         -2.7160e-02, -4.7241e-02,  7.1567e-02,  7.8514e-02,  1.3383e-01,\n",
      "          4.2215e-01,  8.2721e-02,  8.2908e-02,  1.0240e-01,  1.7399e-02,\n",
      "         -7.3731e-04,  1.2990e-02, -1.3283e-01,  7.1750e-02,  3.0020e-02,\n",
      "         -5.0527e-02,  1.8098e-01, -2.2900e-02,  1.4276e-01,  8.1691e-02,\n",
      "          1.7012e-02,  6.1481e-02,  4.4891e-02, -1.0798e-01,  3.0184e-02,\n",
      "         -3.8658e-02, -2.8549e-01,  2.6492e-02, -2.0643e-02,  2.1271e-01,\n",
      "         -1.6793e-01, -5.6226e-02,  1.8644e-02, -2.7419e-01, -4.6789e-01,\n",
      "          1.0823e-01,  8.5879e-02,  1.6711e-03,  1.3986e-01, -2.0077e-02,\n",
      "         -9.1717e-02, -1.1899e-01, -4.7828e-02,  1.6423e-03,  3.4640e-01,\n",
      "         -6.6130e-02,  3.0969e-02,  9.5151e-03, -3.0889e-01, -2.8931e-02,\n",
      "         -6.2532e-01, -9.3076e-02, -5.6809e-02, -1.9159e-02, -2.2166e-02,\n",
      "         -9.4106e-02, -4.9570e-01, -1.3065e-01, -6.4399e-02,  8.7086e-04],\n",
      "        [ 2.5774e-02, -8.7448e-02, -6.4747e-02, -2.7624e-03,  2.8223e-02,\n",
      "          7.7348e-03, -1.1138e-02, -4.4613e-03,  1.3394e-01,  3.1850e-02,\n",
      "          6.3788e-02, -5.3328e-02,  4.3152e-03,  2.2198e-02, -3.6076e-02,\n",
      "         -1.1971e-01, -1.6430e-02,  1.1946e-02,  2.6485e-02, -2.9655e-02,\n",
      "          2.7353e-02, -2.1052e-02, -1.6389e-03, -1.5801e-02,  3.0958e-02,\n",
      "          4.1225e-02,  1.2907e-02, -6.5296e-03, -3.5489e-02,  1.4687e-02,\n",
      "          4.4170e-03,  4.6478e-02,  3.1292e-02, -9.4403e-02, -3.3048e-03,\n",
      "          3.1653e-03,  2.7479e-02,  1.3337e-02, -1.3057e-02,  5.3602e-02,\n",
      "         -7.5090e-02, -6.5460e-02, -2.4531e-02,  2.3118e-03,  1.0167e-01,\n",
      "          1.0825e-01,  2.2083e-03,  8.7720e-02, -1.4994e-02,  2.8727e-02,\n",
      "          4.1109e-02,  1.2720e-02, -1.0406e-01, -8.5487e-03,  2.9336e-02,\n",
      "         -7.3915e-03,  1.0468e-01,  3.2154e-02,  8.0835e-02,  2.9034e-02,\n",
      "          7.7466e-03,  1.6635e-02,  8.0037e-02,  9.0821e-03,  3.3932e-02,\n",
      "         -2.7571e-03, -1.1560e-01,  2.2081e-02,  2.7871e-03,  5.3612e-02,\n",
      "         -1.4238e-02,  1.9632e-02, -5.4981e-02, -6.1362e-02, -7.6462e-02,\n",
      "          2.3618e-02,  3.1894e-02, -2.0501e-02, -3.7426e-02,  2.7715e-02,\n",
      "          3.2336e-04, -5.3164e-02,  2.4450e-02, -6.4527e-03,  1.5635e-01,\n",
      "         -2.9307e-03,  1.8113e-01, -3.0963e-03, -5.5864e-02,  1.0574e-02,\n",
      "         -2.0173e-01,  1.3771e-03, -1.8630e-02,  3.6529e-02, -8.5720e-03,\n",
      "         -9.3574e-03, -1.4920e-01, -7.1817e-02, -3.1357e-02,  2.0796e-02],\n",
      "        [ 5.5985e-02,  1.2201e-01, -1.8028e-01,  2.2125e-01, -2.1716e-01,\n",
      "         -3.8427e-02,  4.2604e-02, -6.1816e-02,  2.5112e-01,  6.4385e-04,\n",
      "          1.9245e-02,  1.1725e-01,  1.5550e-02,  1.9185e-02,  1.6212e-03,\n",
      "         -4.8444e-01, -1.2419e-02,  1.6177e-02,  9.4600e-02, -2.1410e-01,\n",
      "         -9.5966e-02, -1.4885e-06, -4.1668e-02, -1.8791e-01,  1.2049e-01,\n",
      "          3.2718e-02,  1.6838e-02, -1.1471e-02,  2.0840e-01, -9.0295e-03,\n",
      "         -8.6166e-02,  6.9009e-02,  1.0198e-02, -2.6777e-01,  2.3672e-02,\n",
      "          8.3551e-04, -2.4082e-03,  3.8531e-03, -1.5534e-02,  4.9396e-02,\n",
      "         -2.2692e-02,  8.8797e-02,  2.8019e-02,  1.3300e-01,  7.7244e-02,\n",
      "          3.9261e-01,  4.1072e-04,  1.2733e-01,  9.0669e-02, -6.2541e-02,\n",
      "         -1.6094e-02,  1.0189e-02, -2.7089e-01,  3.2863e-02,  3.1905e-03,\n",
      "          4.4761e-02,  2.6359e-01, -5.8507e-02,  7.5216e-02, -8.4209e-02,\n",
      "          1.9358e-02,  6.0092e-02,  6.3332e-02, -1.3626e-01,  4.2317e-02,\n",
      "         -1.4311e-02, -1.9832e-01,  6.4519e-03, -1.2904e-02,  2.0961e-01,\n",
      "         -2.3225e-01, -2.2271e-02, -8.0417e-02, -2.8297e-01, -1.6045e-01,\n",
      "          5.3616e-03,  5.1423e-02, -7.6694e-03,  1.6224e-02,  5.6147e-02,\n",
      "         -2.5175e-02, -8.4974e-02, -1.0102e-02,  9.4461e-03,  2.5457e-01,\n",
      "          2.1565e-02, -6.7089e-02, -3.5200e-02, -1.7810e-01, -5.0381e-03,\n",
      "         -8.8127e-01, -4.4423e-02, -3.2471e-04,  3.5871e-02,  1.5985e-02,\n",
      "         -9.0287e-02, -4.9548e-01, -1.0184e-01,  2.2326e-01,  2.3383e-02],\n",
      "        [-1.9304e-02,  3.9441e-02, -7.3708e-02,  1.5173e-02,  8.3518e-03,\n",
      "          3.2696e-02,  5.5940e-03, -6.4139e-02,  1.2497e-02, -6.4144e-02,\n",
      "         -4.1090e-03,  3.2255e-02,  1.9055e-02, -1.2886e-02,  5.7320e-02,\n",
      "         -1.4841e-01, -1.8817e-02, -3.6384e-02,  3.5685e-02, -8.8398e-02,\n",
      "         -4.3117e-02,  8.8288e-03, -2.5386e-02, -1.6214e-02,  5.7259e-02,\n",
      "          2.0872e-02, -3.3394e-03,  2.8037e-02,  3.8652e-02,  2.3185e-02,\n",
      "         -6.5974e-04, -1.2867e-01, -8.0922e-02,  2.2098e-02,  6.7282e-03,\n",
      "         -1.9403e-02,  4.8789e-02, -2.2121e-03, -4.8392e-02,  5.4693e-02,\n",
      "         -2.8793e-03,  7.0675e-02, -2.0651e-02,  5.6699e-02,  7.7654e-02,\n",
      "          1.2258e-01,  2.2273e-03, -6.0708e-03, -7.8706e-03, -4.1551e-02,\n",
      "         -6.4822e-02, -2.2647e-02, -6.5409e-02, -4.1535e-02,  1.6519e-02,\n",
      "          1.1889e-01,  4.1440e-02, -2.9641e-02,  6.1482e-02, -8.9847e-02,\n",
      "          4.5277e-02,  4.1489e-02,  2.8425e-02, -1.2149e-02, -3.8872e-02,\n",
      "          2.6546e-02, -3.8177e-02, -2.0959e-03,  1.2652e-02,  1.3044e-01,\n",
      "         -9.0908e-02,  1.1963e-02, -2.2232e-02, -8.3968e-02, -1.2581e-02,\n",
      "         -1.0434e-02, -2.1116e-03,  1.6271e-02,  2.3062e-03,  1.4317e-02,\n",
      "          4.0254e-02, -1.0031e-01, -4.6280e-02, -1.6174e-02,  8.5013e-02,\n",
      "          9.8642e-03, -3.7764e-02, -1.7890e-02, -4.8285e-02,  3.8215e-02,\n",
      "         -1.6736e-01,  1.3384e-02,  5.0879e-02, -4.1851e-03, -7.5937e-02,\n",
      "          3.2033e-02, -8.5422e-02, -2.1862e-02, -3.5048e-02, -5.4167e-02],\n",
      "        [ 6.1940e-02,  1.1379e-01, -1.6467e-01,  7.1944e-02, -4.0745e-03,\n",
      "          2.3025e-02,  3.1518e-02, -1.9572e-02,  1.1744e-01, -2.5672e-02,\n",
      "          1.0000e-02,  1.0107e-01, -8.0105e-03, -1.7742e-01,  3.7935e-02,\n",
      "         -4.9236e-01, -3.3859e-02,  2.0534e-02, -5.6915e-03, -1.1230e-01,\n",
      "         -6.6737e-02, -8.8986e-03,  2.9393e-02, -8.9848e-02,  1.3137e-01,\n",
      "          1.3670e-02, -4.9106e-03,  6.4727e-02,  1.4732e-01, -5.6830e-02,\n",
      "         -1.2917e-01, -2.2627e-02, -1.4673e-02, -6.9821e-02,  4.3840e-02,\n",
      "         -9.8055e-03, -6.8153e-03,  3.5896e-02, -6.7156e-02,  2.0701e-01,\n",
      "         -2.9400e-02,  1.2455e-01, -4.5620e-02,  1.2959e-01,  1.4229e-01,\n",
      "          3.7552e-01,  1.0118e-02, -8.2817e-03,  3.4848e-02, -5.0844e-02,\n",
      "         -3.3268e-03, -2.2658e-02, -1.7315e-01, -6.4825e-03,  6.4001e-02,\n",
      "          1.8119e-01,  7.1013e-02, -4.3863e-02,  7.4064e-02, -5.5154e-02,\n",
      "          2.3661e-02,  9.1188e-02,  8.2365e-02, -2.0544e-02,  2.3840e-02,\n",
      "          3.9490e-02, -2.1725e-01, -2.2165e-02, -7.3301e-04,  1.7894e-01,\n",
      "         -1.5187e-01,  6.4401e-02, -7.1092e-02, -2.2265e-01, -4.0335e-02,\n",
      "         -5.4053e-02,  3.1490e-02,  4.9792e-02, -2.2419e-03, -4.8943e-03,\n",
      "          3.9959e-03, -4.2774e-02,  2.4521e-03, -1.8567e-03,  2.0335e-01,\n",
      "          7.0571e-02, -8.2379e-02, -6.5270e-03, -1.2700e-01,  3.3423e-02,\n",
      "         -6.3010e-01, -6.0111e-03, -9.2496e-04, -4.6187e-02, -2.4408e-02,\n",
      "          3.7410e-02, -4.2367e-01, -7.2317e-02,  7.5829e-02, -6.3675e-03]],\n",
      "       device='mps:0')\n",
      "tensor([ 0.0070,  0.2522, -0.0791,  0.2212, -0.3170, -0.0292, -0.0014,  0.0759,\n",
      "         0.1710,  0.1423, -0.0923,  0.2063, -0.0934, -0.0148,  0.0657, -0.3348,\n",
      "        -0.0333, -0.0099,  0.1117, -0.2186, -0.1687,  0.0748,  0.0358, -0.1583,\n",
      "         0.0636, -0.0104,  0.0066, -0.0500,  0.2277, -0.0946, -0.1015,  0.1030,\n",
      "        -0.0370, -0.2939,  0.0686, -0.0091, -0.0737,  0.0323,  0.0193,  0.1152,\n",
      "         0.0712,  0.2123,  0.0414,  0.0925, -0.0703,  0.2621,  0.0197,  0.0274,\n",
      "         0.1003, -0.1394, -0.0141, -0.0673, -0.0336,  0.0629, -0.1044,  0.0809,\n",
      "         0.0843, -0.1277, -0.0633, -0.1066,  0.0747,  0.0263, -0.1168, -0.1567,\n",
      "         0.0046, -0.0259, -0.0317, -0.0821,  0.0372,  0.1473, -0.2162, -0.0283,\n",
      "         0.0213, -0.1899, -0.2008, -0.0837,  0.0928,  0.0660,  0.0858,  0.0813,\n",
      "        -0.0762,  0.0559, -0.1131, -0.0455,  0.0933,  0.0508, -0.2216, -0.0600,\n",
      "        -0.1522, -0.0111, -0.4840, -0.0726, -0.0642, -0.0862, -0.0614, -0.0605,\n",
      "        -0.3292, -0.0259,  0.2283, -0.0261], device='mps:0')\n",
      "tensor([[ 3.9314e-02,  9.7418e-02, -1.5110e+08,  ...,  2.5431e+02,\n",
      "          2.8140e-02,  5.1609e+05],\n",
      "        [ 6.3323e-02,  1.0010e-01, -2.8318e+09,  ..., -9.2770e+00,\n",
      "          2.7251e-02, -1.1543e+06],\n",
      "        [ 2.7659e-02,  4.3920e-02,  1.8419e+09,  ..., -1.9478e+02,\n",
      "          5.0776e-03,  8.1875e+05],\n",
      "        ...,\n",
      "        [-4.5450e-02,  2.7112e-04, -1.1989e+09,  ...,  1.5616e+01,\n",
      "         -1.1225e-02, -5.8290e+05],\n",
      "        [-4.7839e-02,  2.8660e-02,  2.1734e+09,  ...,  9.1568e+01,\n",
      "          1.2140e-03,  7.8479e+05],\n",
      "        [-7.3563e-02, -1.1177e-01,  4.9444e+08,  ...,  2.7864e+00,\n",
      "         -3.1546e-02,  5.2600e+04]], device='mps:0')\n",
      "tensor([-0.1188, -0.1377,  0.0045,  0.0334, -0.0240,  0.1645, -0.0039,  0.0011,\n",
      "        -0.0331,  0.0085, -0.0112,  0.0619,  0.0239, -0.0242, -0.0619,  0.0081,\n",
      "        -0.0040,  0.1715, -0.0292, -0.0333, -0.0211, -0.0132,  0.0393,  0.0523,\n",
      "        -0.0016, -0.0427, -0.0090], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T07:46:35.746501Z",
     "start_time": "2025-03-12T07:46:03.601525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# putting it all together... full batch gradient descent\n",
    "\n",
    "# create the layers of the model\n",
    "C = torch.randn((vocab_size, embed_dim), requires_grad=True)\n",
    "W1 = torch.randn((ngram_len * embed_dim, hidden_dim), requires_grad=True)\n",
    "b1 = torch.randn(hidden_dim, requires_grad=True)\n",
    "W2 = torch.randn((hidden_dim, vocab_size), requires_grad=True)\n",
    "b2 = torch.randn(vocab_size, requires_grad=True)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "# define a forward pass for the model\n",
    "def forward(xs):\n",
    "    # embedding\n",
    "    emb = C[xs]\n",
    "    # hidden layer\n",
    "    flat_emb = emb.view(emb.shape[0], -1)\n",
    "    h = torch.tanh(flat_emb @ W1 + b1)\n",
    "    # output layer\n",
    "    return h @ W2 + b2\n",
    "\n",
    "# train the model\n",
    "for epoch in range(100):\n",
    "    # forward pass\n",
    "    logits = forward(X)\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    print(f'Epoch {epoch}: Loss= {loss.item()}')\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data -= 0.1 * p.grad\n",
    "\n",
    "# Note: this seems to get numerically unstable sometimes..."
   ],
   "id": "bae9f40525e684ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss= 17.79652214050293\n",
      "Epoch 1: Loss= 16.011938095092773\n",
      "Epoch 2: Loss= 14.524364471435547\n",
      "Epoch 3: Loss= 13.348714828491211\n",
      "Epoch 4: Loss= 12.470897674560547\n",
      "Epoch 5: Loss= 11.78131103515625\n",
      "Epoch 6: Loss= 11.183290481567383\n",
      "Epoch 7: Loss= 10.648771286010742\n",
      "Epoch 8: Loss= 10.170660018920898\n",
      "Epoch 9: Loss= 9.744149208068848\n",
      "Epoch 10: Loss= 9.362197875976562\n",
      "Epoch 11: Loss= 9.01722240447998\n",
      "Epoch 12: Loss= 8.702160835266113\n",
      "Epoch 13: Loss= 8.412109375\n",
      "Epoch 14: Loss= 8.143003463745117\n",
      "Epoch 15: Loss= 7.8919830322265625\n",
      "Epoch 16: Loss= 7.656604766845703\n",
      "Epoch 17: Loss= 7.435487270355225\n",
      "Epoch 18: Loss= 7.2281575202941895\n",
      "Epoch 19: Loss= 7.034721851348877\n",
      "Epoch 20: Loss= 6.855053424835205\n",
      "Epoch 21: Loss= 6.687909126281738\n",
      "Epoch 22: Loss= 6.531892776489258\n",
      "Epoch 23: Loss= 6.385890960693359\n",
      "Epoch 24: Loss= 6.249068737030029\n",
      "Epoch 25: Loss= 6.120635509490967\n",
      "Epoch 26: Loss= 5.999819278717041\n",
      "Epoch 27: Loss= 5.8858962059021\n",
      "Epoch 28: Loss= 5.778189182281494\n",
      "Epoch 29: Loss= 5.676074981689453\n",
      "Epoch 30: Loss= 5.57898473739624\n",
      "Epoch 31: Loss= 5.486420631408691\n",
      "Epoch 32: Loss= 5.3979668617248535\n",
      "Epoch 33: Loss= 5.313292980194092\n",
      "Epoch 34: Loss= 5.232159614562988\n",
      "Epoch 35: Loss= 5.1544108390808105\n",
      "Epoch 36: Loss= 5.0799665451049805\n",
      "Epoch 37: Loss= 5.008793830871582\n",
      "Epoch 38: Loss= 4.9408745765686035\n",
      "Epoch 39: Loss= 4.876161575317383\n",
      "Epoch 40: Loss= 4.8145432472229\n",
      "Epoch 41: Loss= 4.7558417320251465\n",
      "Epoch 42: Loss= 4.699840068817139\n",
      "Epoch 43: Loss= 4.646321773529053\n",
      "Epoch 44: Loss= 4.595094680786133\n",
      "Epoch 45: Loss= 4.545992851257324\n",
      "Epoch 46: Loss= 4.498878002166748\n",
      "Epoch 47: Loss= 4.453625679016113\n",
      "Epoch 48: Loss= 4.410125732421875\n",
      "Epoch 49: Loss= 4.368276119232178\n",
      "Epoch 50: Loss= 4.327986240386963\n",
      "Epoch 51: Loss= 4.289172649383545\n",
      "Epoch 52: Loss= 4.251763343811035\n",
      "Epoch 53: Loss= 4.215691566467285\n",
      "Epoch 54: Loss= 4.1809000968933105\n",
      "Epoch 55: Loss= 4.147335052490234\n",
      "Epoch 56: Loss= 4.114947319030762\n",
      "Epoch 57: Loss= 4.083691120147705\n",
      "Epoch 58: Loss= 4.053521156311035\n",
      "Epoch 59: Loss= 4.024392604827881\n",
      "Epoch 60: Loss= 3.9962639808654785\n",
      "Epoch 61: Loss= 3.969092607498169\n",
      "Epoch 62: Loss= 3.9428374767303467\n",
      "Epoch 63: Loss= 3.9174585342407227\n",
      "Epoch 64: Loss= 3.892916202545166\n",
      "Epoch 65: Loss= 3.8691728115081787\n",
      "Epoch 66: Loss= 3.8461923599243164\n",
      "Epoch 67: Loss= 3.8239388465881348\n",
      "Epoch 68: Loss= 3.8023793697357178\n",
      "Epoch 69: Loss= 3.7814810276031494\n",
      "Epoch 70: Loss= 3.7612128257751465\n",
      "Epoch 71: Loss= 3.741544723510742\n",
      "Epoch 72: Loss= 3.722447395324707\n",
      "Epoch 73: Loss= 3.7038941383361816\n",
      "Epoch 74: Loss= 3.6858577728271484\n",
      "Epoch 75: Loss= 3.6683127880096436\n",
      "Epoch 76: Loss= 3.651236057281494\n",
      "Epoch 77: Loss= 3.6346023082733154\n",
      "Epoch 78: Loss= 3.618391275405884\n",
      "Epoch 79: Loss= 3.602581024169922\n",
      "Epoch 80: Loss= 3.5871524810791016\n",
      "Epoch 81: Loss= 3.572087049484253\n",
      "Epoch 82: Loss= 3.5573673248291016\n",
      "Epoch 83: Loss= 3.5429775714874268\n",
      "Epoch 84: Loss= 3.5289037227630615\n",
      "Epoch 85: Loss= 3.515131711959839\n",
      "Epoch 86: Loss= 3.501650810241699\n",
      "Epoch 87: Loss= 3.488449811935425\n",
      "Epoch 88: Loss= 3.475520372390747\n",
      "Epoch 89: Loss= 3.4628539085388184\n",
      "Epoch 90: Loss= 3.450443983078003\n",
      "Epoch 91: Loss= 3.4382851123809814\n",
      "Epoch 92: Loss= 3.426372766494751\n",
      "Epoch 93: Loss= 3.414702892303467\n",
      "Epoch 94: Loss= 3.4032723903656006\n",
      "Epoch 95: Loss= 3.3920795917510986\n",
      "Epoch 96: Loss= 3.381120204925537\n",
      "Epoch 97: Loss= 3.370394468307495\n",
      "Epoch 98: Loss= 3.359898328781128\n",
      "Epoch 99: Loss= 3.3496310710906982\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
