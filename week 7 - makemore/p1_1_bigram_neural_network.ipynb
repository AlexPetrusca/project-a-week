{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:31:59.490018Z",
     "start_time": "2025-03-12T03:31:58.760267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's use a simple neural network to recreate what we did in p1_bigram_lookup_table. We'll be trying\n",
    "# to learn the same 27 x 27 probability lookup table from the last notebook using gradient descent on\n",
    "# a single layer neural network.\n",
    "\n",
    "# we'll need some stuff from the last notebook.\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.set_default_device(\"mps\")\n",
    "\n",
    "words = open('res/names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))  # tokens a-z\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}  # map each token to unique id\n",
    "stoi['.'] = 0  # add encoding for terminating token\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}  # create the reverse mapping"
   ],
   "id": "838ea290656a8a8f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:31:59.621416Z",
     "start_time": "2025-03-12T03:31:59.493782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create the training set of bigrams\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        idx1 = stoi[ch1]\n",
    "        idx2 = stoi[ch2]\n",
    "        xs.append(idx1)\n",
    "        ys.append(idx2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(xs)\n",
    "print(ys)"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  5, 13,  ..., 25, 26, 24], device='mps:0')\n",
      "tensor([ 5, 13, 13,  ..., 26, 24,  0], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:31:59.785069Z",
     "start_time": "2025-03-12T03:31:59.678866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "x_enc = F.one_hot(xs, num_classes=27).float()\n",
    "print(x_enc.shape)\n",
    "print(x_enc)"
   ],
   "id": "5bc6ae61c1f88340",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 27])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:31:59.849450Z",
     "start_time": "2025-03-12T03:31:59.801214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# visualize first 10 one-hot-encoded vectors\n",
    "print(x_enc.cpu()[0:10].shape)\n",
    "plt.imshow(x_enc.cpu()[0:10])"
   ],
   "id": "17b414ed5959fe5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10fd40d70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:31:59.911732Z",
     "start_time": "2025-03-12T03:31:59.877906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's create a \"neuron\"\n",
    "W = torch.randn(27, 1)\n",
    "a = x_enc[0:10] @ W  # (10, 27) @ (27, 1) -> (10, 1)\n",
    "\n",
    "print(a)\n",
    "print(a.shape)"
   ],
   "id": "6512d0c94c846695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6894],\n",
      "        [ 1.2910],\n",
      "        [ 0.5924],\n",
      "        [ 0.5924],\n",
      "        [ 0.1887],\n",
      "        [ 0.6894],\n",
      "        [ 1.2367],\n",
      "        [ 0.0932],\n",
      "        [-0.7403],\n",
      "        [-0.5791]], device='mps:0')\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:31:59.979745Z",
     "start_time": "2025-03-12T03:31:59.914754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# let's create a \"layer\" of 27 neurons (each column in W is one neuron)\n",
    "W = torch.randn(27, 27)\n",
    "a = x_enc[0:10] @ W  # (10, 27) @ (27, 27) -> (10, 27)\n",
    "\n",
    "print(a)\n",
    "print(a.shape)"
   ],
   "id": "df3d33a22bcdbbdb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0586,  0.6362,  2.1142,  2.3780,  0.9444,  0.8647, -0.8298,  1.9725,\n",
      "          0.3329, -1.6866,  0.9905, -0.9820, -0.1480,  1.3446,  0.8581, -0.3066,\n",
      "          0.2603,  2.7071, -1.8849, -1.5907,  2.4338, -0.9289,  0.4504, -0.0195,\n",
      "         -0.7536, -0.5649, -1.0130],\n",
      "        [-0.8139,  0.4150,  0.2339,  0.4533, -0.0938, -0.8165,  1.2347,  0.4995,\n",
      "          2.0179,  0.0289, -0.4977, -0.2959,  0.4746, -0.3270,  1.9066,  2.6202,\n",
      "          0.6514,  1.1816, -0.4126, -0.4150, -0.1506, -1.8668,  0.5241,  1.5906,\n",
      "         -1.6751, -1.9510, -1.2522],\n",
      "        [ 1.1713,  0.3099, -0.6206, -0.6400, -0.4006,  0.2353,  1.3758,  0.2120,\n",
      "          0.7008, -1.2774, -0.7671,  1.7352, -0.4360, -0.6071,  1.7922, -0.1479,\n",
      "         -0.0845, -0.6646,  2.1960,  0.0810, -1.0350,  0.4021,  1.1551,  2.0620,\n",
      "         -0.7581, -1.3429,  1.6943],\n",
      "        [ 1.1713,  0.3099, -0.6206, -0.6400, -0.4006,  0.2353,  1.3758,  0.2120,\n",
      "          0.7008, -1.2774, -0.7671,  1.7352, -0.4360, -0.6071,  1.7922, -0.1479,\n",
      "         -0.0845, -0.6646,  2.1960,  0.0810, -1.0350,  0.4021,  1.1551,  2.0620,\n",
      "         -0.7581, -1.3429,  1.6943],\n",
      "        [ 1.7071, -1.7246,  0.2010, -0.1794,  0.9658, -0.9602, -0.9635, -0.9694,\n",
      "         -0.9489, -0.1295, -0.7615,  0.2295, -0.1672, -0.2850,  0.6921,  0.1670,\n",
      "          0.0251, -0.7920,  0.3593,  0.5864, -0.3367, -0.5303,  1.0566,  1.5420,\n",
      "          0.1958,  0.7067, -0.0648],\n",
      "        [ 0.0586,  0.6362,  2.1142,  2.3780,  0.9444,  0.8647, -0.8298,  1.9725,\n",
      "          0.3329, -1.6866,  0.9905, -0.9820, -0.1480,  1.3446,  0.8581, -0.3066,\n",
      "          0.2603,  2.7071, -1.8849, -1.5907,  2.4338, -0.9289,  0.4504, -0.0195,\n",
      "         -0.7536, -0.5649, -1.0130],\n",
      "        [ 0.4349, -0.4937, -0.1178, -0.5791, -1.2331, -0.2687,  1.0865, -1.5169,\n",
      "          0.7086, -0.6044,  0.0154, -1.0247,  0.2407,  0.7179,  0.1069, -0.7501,\n",
      "          0.0445, -0.9521, -0.4152,  0.7858, -0.7563,  1.8489, -0.3310,  1.1437,\n",
      "          1.3465, -0.8671, -1.1315],\n",
      "        [-0.3017,  0.1389,  0.4745,  1.6044,  0.9830,  1.4401, -0.3272, -1.3608,\n",
      "          0.2109, -1.3438,  1.8317, -0.0820, -0.1846,  0.5237,  0.2135, -0.4182,\n",
      "         -1.2997,  0.8516, -1.3004,  1.6170, -1.8133, -0.9877,  2.3911,  0.8024,\n",
      "          2.2942,  0.3245,  1.1340],\n",
      "        [-0.3862,  0.0876, -0.4793, -0.9403, -1.1845,  0.9168,  0.6036, -1.3573,\n",
      "         -1.3932,  0.9519, -0.7509, -0.3063, -0.4978,  1.0599, -0.2379,  0.9267,\n",
      "          0.0919,  0.5078, -0.7198,  0.2648,  0.0444, -1.0608,  1.8924,  0.4230,\n",
      "         -0.2432,  0.4331, -1.1609],\n",
      "        [-0.7228,  1.5497,  0.1040, -2.6879,  2.0218, -3.0125,  0.6151,  0.4004,\n",
      "         -0.7155,  0.5629,  0.3829,  0.7264,  1.3857,  0.2840, -0.3473,  0.5918,\n",
      "         -0.9309,  1.0247, -0.4023, -1.1883,  1.0926, -0.5536, -2.4232,  1.3165,\n",
      "         -0.5508, -0.1324, -0.0336]], device='mps:0')\n",
      "torch.Size([10, 27])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:32:00.178516Z",
     "start_time": "2025-03-12T03:32:00.136483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# output of the 13th neuron when fed the 3rd input\n",
    "#   - i.e. the dot product of the 3rd row of the input with the 13th column of the weights matrix\n",
    "print((x_enc[0:10] @ W)[3, 13])\n",
    "print((x_enc[3] * W[:, 13]).sum()) # equivalent"
   ],
   "id": "19a77ce5dfa4654d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6071, device='mps:0')\n",
      "tensor(-0.6071, device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:32:00.478980Z",
     "start_time": "2025-03-12T03:32:00.339278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ok so we have a basic neural network now.\n",
    "#   - it takes batched input of shape `[batch_size, vocab_size]`.\n",
    "#   - it has a hidden layer of `indims=vocab_size` and `outdims=vocab_size`.\n",
    "#   - it outputs a tensor of shape `[batch_size, vocab_size]`.\n",
    "\n",
    "# for each input, the output is a list of `vocab_size`-many random floating point numbers. we'd like to\n",
    "# interpret the output as a probability distribution of the next token.\n",
    "#   - in comes softmax to the rescue!\n",
    "#   - by using softmax, we're basically saying that the network is outputting `log(counts)` (i.e. logits)\n",
    "#     and we need exponentiate them to retrieve the `counts`.\n",
    "\n",
    "# feed forward\n",
    "logits = x_enc[0:10] @ W  # log-counts\n",
    "# softmax\n",
    "counts = logits.exp()  # counts (equivalent to N from p1_bigram_lookup_table)\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)  # probabilities for next character (equivalent to P from p1_bigram_lookup_table)\n",
    "\n",
    "print(\"Logits:\", logits)\n",
    "print(\"Counts:\", counts)\n",
    "print(\"Probabilities:\", probs)\n",
    "print(\"Sum of probabilities over samples:\", probs.sum(dim=1, keepdim=True))"
   ],
   "id": "e09e9fe1bae8e34e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[ 0.0586,  0.6362,  2.1142,  2.3780,  0.9444,  0.8647, -0.8298,  1.9725,\n",
      "          0.3329, -1.6866,  0.9905, -0.9820, -0.1480,  1.3446,  0.8581, -0.3066,\n",
      "          0.2603,  2.7071, -1.8849, -1.5907,  2.4338, -0.9289,  0.4504, -0.0195,\n",
      "         -0.7536, -0.5649, -1.0130],\n",
      "        [-0.8139,  0.4150,  0.2339,  0.4533, -0.0938, -0.8165,  1.2347,  0.4995,\n",
      "          2.0179,  0.0289, -0.4977, -0.2959,  0.4746, -0.3270,  1.9066,  2.6202,\n",
      "          0.6514,  1.1816, -0.4126, -0.4150, -0.1506, -1.8668,  0.5241,  1.5906,\n",
      "         -1.6751, -1.9510, -1.2522],\n",
      "        [ 1.1713,  0.3099, -0.6206, -0.6400, -0.4006,  0.2353,  1.3758,  0.2120,\n",
      "          0.7008, -1.2774, -0.7671,  1.7352, -0.4360, -0.6071,  1.7922, -0.1479,\n",
      "         -0.0845, -0.6646,  2.1960,  0.0810, -1.0350,  0.4021,  1.1551,  2.0620,\n",
      "         -0.7581, -1.3429,  1.6943],\n",
      "        [ 1.1713,  0.3099, -0.6206, -0.6400, -0.4006,  0.2353,  1.3758,  0.2120,\n",
      "          0.7008, -1.2774, -0.7671,  1.7352, -0.4360, -0.6071,  1.7922, -0.1479,\n",
      "         -0.0845, -0.6646,  2.1960,  0.0810, -1.0350,  0.4021,  1.1551,  2.0620,\n",
      "         -0.7581, -1.3429,  1.6943],\n",
      "        [ 1.7071, -1.7246,  0.2010, -0.1794,  0.9658, -0.9602, -0.9635, -0.9694,\n",
      "         -0.9489, -0.1295, -0.7615,  0.2295, -0.1672, -0.2850,  0.6921,  0.1670,\n",
      "          0.0251, -0.7920,  0.3593,  0.5864, -0.3367, -0.5303,  1.0566,  1.5420,\n",
      "          0.1958,  0.7067, -0.0648],\n",
      "        [ 0.0586,  0.6362,  2.1142,  2.3780,  0.9444,  0.8647, -0.8298,  1.9725,\n",
      "          0.3329, -1.6866,  0.9905, -0.9820, -0.1480,  1.3446,  0.8581, -0.3066,\n",
      "          0.2603,  2.7071, -1.8849, -1.5907,  2.4338, -0.9289,  0.4504, -0.0195,\n",
      "         -0.7536, -0.5649, -1.0130],\n",
      "        [ 0.4349, -0.4937, -0.1178, -0.5791, -1.2331, -0.2687,  1.0865, -1.5169,\n",
      "          0.7086, -0.6044,  0.0154, -1.0247,  0.2407,  0.7179,  0.1069, -0.7501,\n",
      "          0.0445, -0.9521, -0.4152,  0.7858, -0.7563,  1.8489, -0.3310,  1.1437,\n",
      "          1.3465, -0.8671, -1.1315],\n",
      "        [-0.3017,  0.1389,  0.4745,  1.6044,  0.9830,  1.4401, -0.3272, -1.3608,\n",
      "          0.2109, -1.3438,  1.8317, -0.0820, -0.1846,  0.5237,  0.2135, -0.4182,\n",
      "         -1.2997,  0.8516, -1.3004,  1.6170, -1.8133, -0.9877,  2.3911,  0.8024,\n",
      "          2.2942,  0.3245,  1.1340],\n",
      "        [-0.3862,  0.0876, -0.4793, -0.9403, -1.1845,  0.9168,  0.6036, -1.3573,\n",
      "         -1.3932,  0.9519, -0.7509, -0.3063, -0.4978,  1.0599, -0.2379,  0.9267,\n",
      "          0.0919,  0.5078, -0.7198,  0.2648,  0.0444, -1.0608,  1.8924,  0.4230,\n",
      "         -0.2432,  0.4331, -1.1609],\n",
      "        [-0.7228,  1.5497,  0.1040, -2.6879,  2.0218, -3.0125,  0.6151,  0.4004,\n",
      "         -0.7155,  0.5629,  0.3829,  0.7264,  1.3857,  0.2840, -0.3473,  0.5918,\n",
      "         -0.9309,  1.0247, -0.4023, -1.1883,  1.0926, -0.5536, -2.4232,  1.3165,\n",
      "         -0.5508, -0.1324, -0.0336]], device='mps:0')\n",
      "Counts: tensor([[ 1.0603,  1.8892,  8.2827, 10.7838,  2.5714,  2.3742,  0.4361,  7.1889,\n",
      "          1.3950,  0.1852,  2.6927,  0.3746,  0.8624,  3.8366,  2.3586,  0.7360,\n",
      "          1.2974, 14.9862,  0.1518,  0.2038, 11.4027,  0.3950,  1.5689,  0.9807,\n",
      "          0.4707,  0.5684,  0.3631],\n",
      "        [ 0.4431,  1.5144,  1.2636,  1.5735,  0.9105,  0.4420,  3.4373,  1.6478,\n",
      "          7.5227,  1.0293,  0.6079,  0.7439,  1.6074,  0.7211,  6.7305, 13.7378,\n",
      "          1.9183,  3.2596,  0.6619,  0.6603,  0.8602,  0.1546,  1.6889,  4.9065,\n",
      "          0.1873,  0.1421,  0.2859],\n",
      "        [ 3.2262,  1.3634,  0.5376,  0.5273,  0.6699,  1.2653,  3.9584,  1.2362,\n",
      "          2.0153,  0.2788,  0.4643,  5.6700,  0.6466,  0.5449,  6.0024,  0.8625,\n",
      "          0.9189,  0.5145,  8.9890,  1.0844,  0.3552,  1.4950,  3.1745,  7.8618,\n",
      "          0.4686,  0.2611,  5.4430],\n",
      "        [ 3.2262,  1.3634,  0.5376,  0.5273,  0.6699,  1.2653,  3.9584,  1.2362,\n",
      "          2.0153,  0.2788,  0.4643,  5.6700,  0.6466,  0.5449,  6.0024,  0.8625,\n",
      "          0.9189,  0.5145,  8.9890,  1.0844,  0.3552,  1.4950,  3.1745,  7.8618,\n",
      "          0.4686,  0.2611,  5.4430],\n",
      "        [ 5.5129,  0.1782,  1.2226,  0.8358,  2.6269,  0.3828,  0.3815,  0.3793,\n",
      "          0.3872,  0.8785,  0.4670,  1.2580,  0.8461,  0.7520,  1.9978,  1.1818,\n",
      "          1.0254,  0.4529,  1.4323,  1.7974,  0.7141,  0.5885,  2.8765,  4.6739,\n",
      "          1.2163,  2.0274,  0.9373],\n",
      "        [ 1.0603,  1.8892,  8.2827, 10.7838,  2.5714,  2.3742,  0.4361,  7.1889,\n",
      "          1.3950,  0.1852,  2.6927,  0.3746,  0.8624,  3.8366,  2.3586,  0.7360,\n",
      "          1.2974, 14.9862,  0.1518,  0.2038, 11.4027,  0.3950,  1.5689,  0.9807,\n",
      "          0.4707,  0.5684,  0.3631],\n",
      "        [ 1.5448,  0.6104,  0.8889,  0.5604,  0.2914,  0.7644,  2.9639,  0.2194,\n",
      "          2.0312,  0.5464,  1.0155,  0.3589,  1.2721,  2.0501,  1.1128,  0.4723,\n",
      "          1.0455,  0.3859,  0.6602,  2.1942,  0.4694,  6.3528,  0.7182,  3.1384,\n",
      "          3.8439,  0.4202,  0.3225],\n",
      "        [ 0.7396,  1.1490,  1.6072,  4.9748,  2.6724,  4.2210,  0.7210,  0.2565,\n",
      "          1.2348,  0.2608,  6.2444,  0.9213,  0.8315,  1.6882,  1.2381,  0.6582,\n",
      "          0.2726,  2.3435,  0.2724,  5.0379,  0.1631,  0.3724, 10.9252,  2.2309,\n",
      "          9.9160,  1.3834,  3.1081],\n",
      "        [ 0.6796,  1.0915,  0.6192,  0.3905,  0.3059,  2.5012,  1.8287,  0.2574,\n",
      "          0.2483,  2.5906,  0.4719,  0.7362,  0.6078,  2.8860,  0.7883,  2.5261,\n",
      "          1.0962,  1.6617,  0.4868,  1.3032,  1.0454,  0.3462,  6.6355,  1.5265,\n",
      "          0.7841,  1.5420,  0.3132],\n",
      "        [ 0.4854,  4.7099,  1.1096,  0.0680,  7.5516,  0.0492,  1.8499,  1.4924,\n",
      "          0.4890,  1.7557,  1.4665,  2.0677,  3.9977,  1.3285,  0.7066,  1.8073,\n",
      "          0.3942,  2.7864,  0.6687,  0.3047,  2.9820,  0.5749,  0.0886,  3.7304,\n",
      "          0.5765,  0.8760,  0.9670]], device='mps:0')\n",
      "Probabilities: tensor([[0.0134, 0.0238, 0.1043, 0.1358, 0.0324, 0.0299, 0.0055, 0.0905, 0.0176,\n",
      "         0.0023, 0.0339, 0.0047, 0.0109, 0.0483, 0.0297, 0.0093, 0.0163, 0.1887,\n",
      "         0.0019, 0.0026, 0.1436, 0.0050, 0.0198, 0.0123, 0.0059, 0.0072, 0.0046],\n",
      "        [0.0076, 0.0258, 0.0215, 0.0268, 0.0155, 0.0075, 0.0586, 0.0281, 0.1282,\n",
      "         0.0175, 0.0104, 0.0127, 0.0274, 0.0123, 0.1147, 0.2342, 0.0327, 0.0556,\n",
      "         0.0113, 0.0113, 0.0147, 0.0026, 0.0288, 0.0836, 0.0032, 0.0024, 0.0049],\n",
      "        [0.0539, 0.0228, 0.0090, 0.0088, 0.0112, 0.0211, 0.0662, 0.0207, 0.0337,\n",
      "         0.0047, 0.0078, 0.0948, 0.0108, 0.0091, 0.1003, 0.0144, 0.0154, 0.0086,\n",
      "         0.1502, 0.0181, 0.0059, 0.0250, 0.0531, 0.1314, 0.0078, 0.0044, 0.0910],\n",
      "        [0.0539, 0.0228, 0.0090, 0.0088, 0.0112, 0.0211, 0.0662, 0.0207, 0.0337,\n",
      "         0.0047, 0.0078, 0.0948, 0.0108, 0.0091, 0.1003, 0.0144, 0.0154, 0.0086,\n",
      "         0.1502, 0.0181, 0.0059, 0.0250, 0.0531, 0.1314, 0.0078, 0.0044, 0.0910],\n",
      "        [0.1489, 0.0048, 0.0330, 0.0226, 0.0709, 0.0103, 0.0103, 0.0102, 0.0105,\n",
      "         0.0237, 0.0126, 0.0340, 0.0228, 0.0203, 0.0540, 0.0319, 0.0277, 0.0122,\n",
      "         0.0387, 0.0485, 0.0193, 0.0159, 0.0777, 0.1262, 0.0328, 0.0547, 0.0253],\n",
      "        [0.0134, 0.0238, 0.1043, 0.1358, 0.0324, 0.0299, 0.0055, 0.0905, 0.0176,\n",
      "         0.0023, 0.0339, 0.0047, 0.0109, 0.0483, 0.0297, 0.0093, 0.0163, 0.1887,\n",
      "         0.0019, 0.0026, 0.1436, 0.0050, 0.0198, 0.0123, 0.0059, 0.0072, 0.0046],\n",
      "        [0.0426, 0.0168, 0.0245, 0.0155, 0.0080, 0.0211, 0.0818, 0.0061, 0.0560,\n",
      "         0.0151, 0.0280, 0.0099, 0.0351, 0.0565, 0.0307, 0.0130, 0.0288, 0.0106,\n",
      "         0.0182, 0.0605, 0.0129, 0.1752, 0.0198, 0.0866, 0.1060, 0.0116, 0.0089],\n",
      "        [0.0113, 0.0176, 0.0246, 0.0760, 0.0408, 0.0645, 0.0110, 0.0039, 0.0189,\n",
      "         0.0040, 0.0954, 0.0141, 0.0127, 0.0258, 0.0189, 0.0101, 0.0042, 0.0358,\n",
      "         0.0042, 0.0770, 0.0025, 0.0057, 0.1669, 0.0341, 0.1515, 0.0211, 0.0475],\n",
      "        [0.0193, 0.0309, 0.0176, 0.0111, 0.0087, 0.0709, 0.0518, 0.0073, 0.0070,\n",
      "         0.0734, 0.0134, 0.0209, 0.0172, 0.0818, 0.0223, 0.0716, 0.0311, 0.0471,\n",
      "         0.0138, 0.0369, 0.0296, 0.0098, 0.1881, 0.0433, 0.0222, 0.0437, 0.0089],\n",
      "        [0.0108, 0.1049, 0.0247, 0.0015, 0.1682, 0.0011, 0.0412, 0.0333, 0.0109,\n",
      "         0.0391, 0.0327, 0.0461, 0.0891, 0.0296, 0.0157, 0.0403, 0.0088, 0.0621,\n",
      "         0.0149, 0.0068, 0.0664, 0.0128, 0.0020, 0.0831, 0.0128, 0.0195, 0.0215]],\n",
      "       device='mps:0')\n",
      "Sum of probabilities over samples: tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], device='mps:0')\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:32:00.835704Z",
     "start_time": "2025-03-12T03:32:00.745354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# dig into how the network weights affect the loss function.\n",
    "nlls = torch.zeros(10)\n",
    "for i in range(10):\n",
    "    # i-th bigram:\n",
    "    x = xs[i].item()  # input character index\n",
    "    y = ys[i].item()  # label character index\n",
    "    print('--------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:', x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print( 'label (actual next character):', y)\n",
    "    p = probs[i, y]\n",
    "    print('probability assigned by the net to the the correct character:', p. item())\n",
    "    logp = torch. log (p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = -logp\n",
    "    print( 'negative log likelihood:', nll.item())\n",
    "    nlls[i] = nll\n",
    "print('========')\n",
    "print( 'average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ],
   "id": "326b3f7cdccc6835",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0134, 0.0238, 0.1043, 0.1358, 0.0324, 0.0299, 0.0055, 0.0905, 0.0176,\n",
      "        0.0023, 0.0339, 0.0047, 0.0109, 0.0483, 0.0297, 0.0093, 0.0163, 0.1887,\n",
      "        0.0019, 0.0026, 0.1436, 0.0050, 0.0198, 0.0123, 0.0059, 0.0072, 0.0046],\n",
      "       device='mps:0')\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.029895735904574394\n",
      "log likelihood: -3.5100393295288086\n",
      "negative log likelihood: 3.5100393295288086\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0076, 0.0258, 0.0215, 0.0268, 0.0155, 0.0075, 0.0586, 0.0281, 0.1282,\n",
      "        0.0175, 0.0104, 0.0127, 0.0274, 0.0123, 0.1147, 0.2342, 0.0327, 0.0556,\n",
      "        0.0113, 0.0113, 0.0147, 0.0026, 0.0288, 0.0836, 0.0032, 0.0024, 0.0049],\n",
      "       device='mps:0')\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.012292702682316303\n",
      "log likelihood: -4.398749351501465\n",
      "negative log likelihood: 4.398749351501465\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0539, 0.0228, 0.0090, 0.0088, 0.0112, 0.0211, 0.0662, 0.0207, 0.0337,\n",
      "        0.0047, 0.0078, 0.0948, 0.0108, 0.0091, 0.1003, 0.0144, 0.0154, 0.0086,\n",
      "        0.1502, 0.0181, 0.0059, 0.0250, 0.0531, 0.1314, 0.0078, 0.0044, 0.0910],\n",
      "       device='mps:0')\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.009106972254812717\n",
      "log likelihood: -4.6987152099609375\n",
      "negative log likelihood: 4.6987152099609375\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0539, 0.0228, 0.0090, 0.0088, 0.0112, 0.0211, 0.0662, 0.0207, 0.0337,\n",
      "        0.0047, 0.0078, 0.0948, 0.0108, 0.0091, 0.1003, 0.0144, 0.0154, 0.0086,\n",
      "        0.1502, 0.0181, 0.0059, 0.0250, 0.0531, 0.1314, 0.0078, 0.0044, 0.0910],\n",
      "       device='mps:0')\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.02278517186641693\n",
      "log likelihood: -3.7816452980041504\n",
      "negative log likelihood: 3.7816452980041504\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.1489, 0.0048, 0.0330, 0.0226, 0.0709, 0.0103, 0.0103, 0.0102, 0.0105,\n",
      "        0.0237, 0.0126, 0.0340, 0.0228, 0.0203, 0.0540, 0.0319, 0.0277, 0.0122,\n",
      "        0.0387, 0.0485, 0.0193, 0.0159, 0.0777, 0.1262, 0.0328, 0.0547, 0.0253],\n",
      "       device='mps:0')\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.14887477457523346\n",
      "log likelihood: -1.9046497344970703\n",
      "negative log likelihood: 1.9046497344970703\n",
      "--------\n",
      "bigram example 6: .o (indexes 0,15)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0134, 0.0238, 0.1043, 0.1358, 0.0324, 0.0299, 0.0055, 0.0905, 0.0176,\n",
      "        0.0023, 0.0339, 0.0047, 0.0109, 0.0483, 0.0297, 0.0093, 0.0163, 0.1887,\n",
      "        0.0019, 0.0026, 0.1436, 0.0050, 0.0198, 0.0123, 0.0059, 0.0072, 0.0046],\n",
      "       device='mps:0')\n",
      "label (actual next character): 15\n",
      "probability assigned by the net to the the correct character: 0.009267275221645832\n",
      "log likelihood: -4.681265830993652\n",
      "negative log likelihood: 4.681265830993652\n",
      "--------\n",
      "bigram example 7: ol (indexes 15,12)\n",
      "input to the neural net: 15\n",
      "output probabilities from the neural net: tensor([0.0426, 0.0168, 0.0245, 0.0155, 0.0080, 0.0211, 0.0818, 0.0061, 0.0560,\n",
      "        0.0151, 0.0280, 0.0099, 0.0351, 0.0565, 0.0307, 0.0130, 0.0288, 0.0106,\n",
      "        0.0182, 0.0605, 0.0129, 0.1752, 0.0198, 0.0866, 0.1060, 0.0116, 0.0089],\n",
      "       device='mps:0')\n",
      "label (actual next character): 12\n",
      "probability assigned by the net to the the correct character: 0.03508882597088814\n",
      "log likelihood: -3.349872589111328\n",
      "negative log likelihood: 3.349872589111328\n",
      "--------\n",
      "bigram example 8: li (indexes 12,9)\n",
      "input to the neural net: 12\n",
      "output probabilities from the neural net: tensor([0.0113, 0.0176, 0.0246, 0.0760, 0.0408, 0.0645, 0.0110, 0.0039, 0.0189,\n",
      "        0.0040, 0.0954, 0.0141, 0.0127, 0.0258, 0.0189, 0.0101, 0.0042, 0.0358,\n",
      "        0.0042, 0.0770, 0.0025, 0.0057, 0.1669, 0.0341, 0.1515, 0.0211, 0.0475],\n",
      "       device='mps:0')\n",
      "label (actual next character): 9\n",
      "probability assigned by the net to the the correct character: 0.0039857858791947365\n",
      "log likelihood: -5.525020599365234\n",
      "negative log likelihood: 5.525020599365234\n",
      "--------\n",
      "bigram example 9: iv (indexes 9,22)\n",
      "input to the neural net: 9\n",
      "output probabilities from the neural net: tensor([0.0193, 0.0309, 0.0176, 0.0111, 0.0087, 0.0709, 0.0518, 0.0073, 0.0070,\n",
      "        0.0734, 0.0134, 0.0209, 0.0172, 0.0818, 0.0223, 0.0716, 0.0311, 0.0471,\n",
      "        0.0138, 0.0369, 0.0296, 0.0098, 0.1881, 0.0433, 0.0222, 0.0437, 0.0089],\n",
      "       device='mps:0')\n",
      "label (actual next character): 22\n",
      "probability assigned by the net to the the correct character: 0.1881328672170639\n",
      "log likelihood: -1.6706068515777588\n",
      "negative log likelihood: 1.6706068515777588\n",
      "--------\n",
      "bigram example 10: vi (indexes 22,9)\n",
      "input to the neural net: 22\n",
      "output probabilities from the neural net: tensor([0.0108, 0.1049, 0.0247, 0.0015, 0.1682, 0.0011, 0.0412, 0.0333, 0.0109,\n",
      "        0.0391, 0.0327, 0.0461, 0.0891, 0.0296, 0.0157, 0.0403, 0.0088, 0.0621,\n",
      "        0.0149, 0.0068, 0.0664, 0.0128, 0.0020, 0.0831, 0.0128, 0.0195, 0.0215],\n",
      "       device='mps:0')\n",
      "label (actual next character): 9\n",
      "probability assigned by the net to the the correct character: 0.0391155481338501\n",
      "log likelihood: -3.2412352561950684\n",
      "negative log likelihood: 3.2412352561950684\n",
      "========\n",
      "average negative log likelihood, i.e. loss = 3.676179885864258\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:32:01.135781Z",
     "start_time": "2025-03-12T03:32:01.133871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we need to find weights that minimize the loss function.\n",
    "#   - in comes gradient descent and pytorch's autograd to save the day.\n",
    "\n",
    "# hyperparameters\n",
    "eta = 50  # learning rate\n",
    "\n",
    "# initialize model - single-layer, 27 neuron network\n",
    "W = torch.randn(27, 27, requires_grad=True)  # track gradients on this layer"
   ],
   "id": "f449299128873b5d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:32:01.466732Z",
     "start_time": "2025-03-12T03:32:01.437180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# single gradient descent pass on batch of size 10\n",
    "\n",
    "# feed forward\n",
    "logits = x_enc[0:10] @ W  # log-counts\n",
    "counts = logits.exp()  # counts\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)  # probabilities for next character\n",
    "\n",
    "# calculate NLL loss\n",
    "ys_pred = probs[torch.arange(0, 10), ys[0:10]]  # from each sample, select probability that we output the correct next token\n",
    "loss = -ys_pred.log().mean()  # log, negate and mean the selected probabilities\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "W.grad = None  # zero the gradient\n",
    "loss.backward()\n",
    "\n",
    "# update model\n",
    "W.data -= eta * W.grad"
   ],
   "id": "9a4fcb38db7c3ad2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.0901, device='mps:0', grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:32:04.133306Z",
     "start_time": "2025-03-12T03:32:01.797572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# full batch gradient descent (putting everything together)\n",
    "for epoch in range(500):\n",
    "    # feed forward\n",
    "    logits = x_enc @ W  # log-counts\n",
    "    counts = logits.exp()  # counts\n",
    "    probs = counts / counts.sum(dim=1, keepdim=True)  # probabilities for next character\n",
    "\n",
    "    # compute NLL loss\n",
    "    ys_pred = probs[torch.arange(0, len(ys)), ys]  # from each batch, select probability that we output the correct next token\n",
    "    loss = -ys_pred.log().mean()  # log, negate and mean the selected probabilities\n",
    "    # loss += 0.01 * (W**2).mean()  # L2 regularization term (keep weights small by adding penalty)\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None  # zero the gradient\n",
    "    loss.backward()\n",
    "\n",
    "    # update model\n",
    "    W.data -= eta * W.grad\n",
    "\n",
    "    print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")"
   ],
   "id": "a2ec25d1cd588c9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss = 4.7517\n",
      "Epoch 1: loss = 3.5674\n",
      "Epoch 2: loss = 3.2590\n",
      "Epoch 3: loss = 3.0933\n",
      "Epoch 4: loss = 2.9757\n",
      "Epoch 5: loss = 2.8898\n",
      "Epoch 6: loss = 2.8271\n",
      "Epoch 7: loss = 2.7800\n",
      "Epoch 8: loss = 2.7430\n",
      "Epoch 9: loss = 2.7131\n",
      "Epoch 10: loss = 2.6885\n",
      "Epoch 11: loss = 2.6678\n",
      "Epoch 12: loss = 2.6503\n",
      "Epoch 13: loss = 2.6353\n",
      "Epoch 14: loss = 2.6223\n",
      "Epoch 15: loss = 2.6110\n",
      "Epoch 16: loss = 2.6010\n",
      "Epoch 17: loss = 2.5922\n",
      "Epoch 18: loss = 2.5843\n",
      "Epoch 19: loss = 2.5772\n",
      "Epoch 20: loss = 2.5708\n",
      "Epoch 21: loss = 2.5649\n",
      "Epoch 22: loss = 2.5596\n",
      "Epoch 23: loss = 2.5547\n",
      "Epoch 24: loss = 2.5502\n",
      "Epoch 25: loss = 2.5461\n",
      "Epoch 26: loss = 2.5423\n",
      "Epoch 27: loss = 2.5387\n",
      "Epoch 28: loss = 2.5354\n",
      "Epoch 29: loss = 2.5324\n",
      "Epoch 30: loss = 2.5295\n",
      "Epoch 31: loss = 2.5269\n",
      "Epoch 32: loss = 2.5244\n",
      "Epoch 33: loss = 2.5220\n",
      "Epoch 34: loss = 2.5198\n",
      "Epoch 35: loss = 2.5177\n",
      "Epoch 36: loss = 2.5158\n",
      "Epoch 37: loss = 2.5139\n",
      "Epoch 38: loss = 2.5121\n",
      "Epoch 39: loss = 2.5105\n",
      "Epoch 40: loss = 2.5089\n",
      "Epoch 41: loss = 2.5074\n",
      "Epoch 42: loss = 2.5059\n",
      "Epoch 43: loss = 2.5046\n",
      "Epoch 44: loss = 2.5033\n",
      "Epoch 45: loss = 2.5020\n",
      "Epoch 46: loss = 2.5008\n",
      "Epoch 47: loss = 2.4997\n",
      "Epoch 48: loss = 2.4986\n",
      "Epoch 49: loss = 2.4976\n",
      "Epoch 50: loss = 2.4966\n",
      "Epoch 51: loss = 2.4956\n",
      "Epoch 52: loss = 2.4947\n",
      "Epoch 53: loss = 2.4938\n",
      "Epoch 54: loss = 2.4929\n",
      "Epoch 55: loss = 2.4921\n",
      "Epoch 56: loss = 2.4913\n",
      "Epoch 57: loss = 2.4906\n",
      "Epoch 58: loss = 2.4898\n",
      "Epoch 59: loss = 2.4891\n",
      "Epoch 60: loss = 2.4885\n",
      "Epoch 61: loss = 2.4878\n",
      "Epoch 62: loss = 2.4872\n",
      "Epoch 63: loss = 2.4865\n",
      "Epoch 64: loss = 2.4860\n",
      "Epoch 65: loss = 2.4854\n",
      "Epoch 66: loss = 2.4848\n",
      "Epoch 67: loss = 2.4843\n",
      "Epoch 68: loss = 2.4838\n",
      "Epoch 69: loss = 2.4833\n",
      "Epoch 70: loss = 2.4828\n",
      "Epoch 71: loss = 2.4823\n",
      "Epoch 72: loss = 2.4818\n",
      "Epoch 73: loss = 2.4814\n",
      "Epoch 74: loss = 2.4809\n",
      "Epoch 75: loss = 2.4805\n",
      "Epoch 76: loss = 2.4801\n",
      "Epoch 77: loss = 2.4797\n",
      "Epoch 78: loss = 2.4793\n",
      "Epoch 79: loss = 2.4789\n",
      "Epoch 80: loss = 2.4786\n",
      "Epoch 81: loss = 2.4782\n",
      "Epoch 82: loss = 2.4778\n",
      "Epoch 83: loss = 2.4775\n",
      "Epoch 84: loss = 2.4772\n",
      "Epoch 85: loss = 2.4769\n",
      "Epoch 86: loss = 2.4765\n",
      "Epoch 87: loss = 2.4762\n",
      "Epoch 88: loss = 2.4759\n",
      "Epoch 89: loss = 2.4756\n",
      "Epoch 90: loss = 2.4753\n",
      "Epoch 91: loss = 2.4751\n",
      "Epoch 92: loss = 2.4748\n",
      "Epoch 93: loss = 2.4745\n",
      "Epoch 94: loss = 2.4743\n",
      "Epoch 95: loss = 2.4740\n",
      "Epoch 96: loss = 2.4738\n",
      "Epoch 97: loss = 2.4735\n",
      "Epoch 98: loss = 2.4733\n",
      "Epoch 99: loss = 2.4731\n",
      "Epoch 100: loss = 2.4728\n",
      "Epoch 101: loss = 2.4726\n",
      "Epoch 102: loss = 2.4724\n",
      "Epoch 103: loss = 2.4722\n",
      "Epoch 104: loss = 2.4720\n",
      "Epoch 105: loss = 2.4718\n",
      "Epoch 106: loss = 2.4716\n",
      "Epoch 107: loss = 2.4714\n",
      "Epoch 108: loss = 2.4712\n",
      "Epoch 109: loss = 2.4710\n",
      "Epoch 110: loss = 2.4708\n",
      "Epoch 111: loss = 2.4706\n",
      "Epoch 112: loss = 2.4705\n",
      "Epoch 113: loss = 2.4703\n",
      "Epoch 114: loss = 2.4701\n",
      "Epoch 115: loss = 2.4700\n",
      "Epoch 116: loss = 2.4698\n",
      "Epoch 117: loss = 2.4696\n",
      "Epoch 118: loss = 2.4695\n",
      "Epoch 119: loss = 2.4693\n",
      "Epoch 120: loss = 2.4692\n",
      "Epoch 121: loss = 2.4690\n",
      "Epoch 122: loss = 2.4689\n",
      "Epoch 123: loss = 2.4687\n",
      "Epoch 124: loss = 2.4686\n",
      "Epoch 125: loss = 2.4685\n",
      "Epoch 126: loss = 2.4683\n",
      "Epoch 127: loss = 2.4682\n",
      "Epoch 128: loss = 2.4681\n",
      "Epoch 129: loss = 2.4679\n",
      "Epoch 130: loss = 2.4678\n",
      "Epoch 131: loss = 2.4677\n",
      "Epoch 132: loss = 2.4676\n",
      "Epoch 133: loss = 2.4674\n",
      "Epoch 134: loss = 2.4673\n",
      "Epoch 135: loss = 2.4672\n",
      "Epoch 136: loss = 2.4671\n",
      "Epoch 137: loss = 2.4670\n",
      "Epoch 138: loss = 2.4669\n",
      "Epoch 139: loss = 2.4668\n",
      "Epoch 140: loss = 2.4667\n",
      "Epoch 141: loss = 2.4666\n",
      "Epoch 142: loss = 2.4665\n",
      "Epoch 143: loss = 2.4664\n",
      "Epoch 144: loss = 2.4663\n",
      "Epoch 145: loss = 2.4662\n",
      "Epoch 146: loss = 2.4661\n",
      "Epoch 147: loss = 2.4660\n",
      "Epoch 148: loss = 2.4659\n",
      "Epoch 149: loss = 2.4658\n",
      "Epoch 150: loss = 2.4657\n",
      "Epoch 151: loss = 2.4656\n",
      "Epoch 152: loss = 2.4655\n",
      "Epoch 153: loss = 2.4654\n",
      "Epoch 154: loss = 2.4653\n",
      "Epoch 155: loss = 2.4653\n",
      "Epoch 156: loss = 2.4652\n",
      "Epoch 157: loss = 2.4651\n",
      "Epoch 158: loss = 2.4650\n",
      "Epoch 159: loss = 2.4649\n",
      "Epoch 160: loss = 2.4649\n",
      "Epoch 161: loss = 2.4648\n",
      "Epoch 162: loss = 2.4647\n",
      "Epoch 163: loss = 2.4646\n",
      "Epoch 164: loss = 2.4646\n",
      "Epoch 165: loss = 2.4645\n",
      "Epoch 166: loss = 2.4644\n",
      "Epoch 167: loss = 2.4643\n",
      "Epoch 168: loss = 2.4643\n",
      "Epoch 169: loss = 2.4642\n",
      "Epoch 170: loss = 2.4641\n",
      "Epoch 171: loss = 2.4641\n",
      "Epoch 172: loss = 2.4640\n",
      "Epoch 173: loss = 2.4639\n",
      "Epoch 174: loss = 2.4639\n",
      "Epoch 175: loss = 2.4638\n",
      "Epoch 176: loss = 2.4637\n",
      "Epoch 177: loss = 2.4637\n",
      "Epoch 178: loss = 2.4636\n",
      "Epoch 179: loss = 2.4635\n",
      "Epoch 180: loss = 2.4635\n",
      "Epoch 181: loss = 2.4634\n",
      "Epoch 182: loss = 2.4634\n",
      "Epoch 183: loss = 2.4633\n",
      "Epoch 184: loss = 2.4632\n",
      "Epoch 185: loss = 2.4632\n",
      "Epoch 186: loss = 2.4631\n",
      "Epoch 187: loss = 2.4631\n",
      "Epoch 188: loss = 2.4630\n",
      "Epoch 189: loss = 2.4630\n",
      "Epoch 190: loss = 2.4629\n",
      "Epoch 191: loss = 2.4629\n",
      "Epoch 192: loss = 2.4628\n",
      "Epoch 193: loss = 2.4628\n",
      "Epoch 194: loss = 2.4627\n",
      "Epoch 195: loss = 2.4626\n",
      "Epoch 196: loss = 2.4626\n",
      "Epoch 197: loss = 2.4625\n",
      "Epoch 198: loss = 2.4625\n",
      "Epoch 199: loss = 2.4624\n",
      "Epoch 200: loss = 2.4624\n",
      "Epoch 201: loss = 2.4624\n",
      "Epoch 202: loss = 2.4623\n",
      "Epoch 203: loss = 2.4623\n",
      "Epoch 204: loss = 2.4622\n",
      "Epoch 205: loss = 2.4622\n",
      "Epoch 206: loss = 2.4621\n",
      "Epoch 207: loss = 2.4621\n",
      "Epoch 208: loss = 2.4620\n",
      "Epoch 209: loss = 2.4620\n",
      "Epoch 210: loss = 2.4619\n",
      "Epoch 211: loss = 2.4619\n",
      "Epoch 212: loss = 2.4619\n",
      "Epoch 213: loss = 2.4618\n",
      "Epoch 214: loss = 2.4618\n",
      "Epoch 215: loss = 2.4617\n",
      "Epoch 216: loss = 2.4617\n",
      "Epoch 217: loss = 2.4616\n",
      "Epoch 218: loss = 2.4616\n",
      "Epoch 219: loss = 2.4616\n",
      "Epoch 220: loss = 2.4615\n",
      "Epoch 221: loss = 2.4615\n",
      "Epoch 222: loss = 2.4615\n",
      "Epoch 223: loss = 2.4614\n",
      "Epoch 224: loss = 2.4614\n",
      "Epoch 225: loss = 2.4613\n",
      "Epoch 226: loss = 2.4613\n",
      "Epoch 227: loss = 2.4613\n",
      "Epoch 228: loss = 2.4612\n",
      "Epoch 229: loss = 2.4612\n",
      "Epoch 230: loss = 2.4612\n",
      "Epoch 231: loss = 2.4611\n",
      "Epoch 232: loss = 2.4611\n",
      "Epoch 233: loss = 2.4610\n",
      "Epoch 234: loss = 2.4610\n",
      "Epoch 235: loss = 2.4610\n",
      "Epoch 236: loss = 2.4609\n",
      "Epoch 237: loss = 2.4609\n",
      "Epoch 238: loss = 2.4609\n",
      "Epoch 239: loss = 2.4608\n",
      "Epoch 240: loss = 2.4608\n",
      "Epoch 241: loss = 2.4608\n",
      "Epoch 242: loss = 2.4607\n",
      "Epoch 243: loss = 2.4607\n",
      "Epoch 244: loss = 2.4607\n",
      "Epoch 245: loss = 2.4607\n",
      "Epoch 246: loss = 2.4606\n",
      "Epoch 247: loss = 2.4606\n",
      "Epoch 248: loss = 2.4606\n",
      "Epoch 249: loss = 2.4605\n",
      "Epoch 250: loss = 2.4605\n",
      "Epoch 251: loss = 2.4605\n",
      "Epoch 252: loss = 2.4604\n",
      "Epoch 253: loss = 2.4604\n",
      "Epoch 254: loss = 2.4604\n",
      "Epoch 255: loss = 2.4604\n",
      "Epoch 256: loss = 2.4603\n",
      "Epoch 257: loss = 2.4603\n",
      "Epoch 258: loss = 2.4603\n",
      "Epoch 259: loss = 2.4602\n",
      "Epoch 260: loss = 2.4602\n",
      "Epoch 261: loss = 2.4602\n",
      "Epoch 262: loss = 2.4602\n",
      "Epoch 263: loss = 2.4601\n",
      "Epoch 264: loss = 2.4601\n",
      "Epoch 265: loss = 2.4601\n",
      "Epoch 266: loss = 2.4600\n",
      "Epoch 267: loss = 2.4600\n",
      "Epoch 268: loss = 2.4600\n",
      "Epoch 269: loss = 2.4600\n",
      "Epoch 270: loss = 2.4599\n",
      "Epoch 271: loss = 2.4599\n",
      "Epoch 272: loss = 2.4599\n",
      "Epoch 273: loss = 2.4599\n",
      "Epoch 274: loss = 2.4598\n",
      "Epoch 275: loss = 2.4598\n",
      "Epoch 276: loss = 2.4598\n",
      "Epoch 277: loss = 2.4598\n",
      "Epoch 278: loss = 2.4597\n",
      "Epoch 279: loss = 2.4597\n",
      "Epoch 280: loss = 2.4597\n",
      "Epoch 281: loss = 2.4597\n",
      "Epoch 282: loss = 2.4597\n",
      "Epoch 283: loss = 2.4596\n",
      "Epoch 284: loss = 2.4596\n",
      "Epoch 285: loss = 2.4596\n",
      "Epoch 286: loss = 2.4596\n",
      "Epoch 287: loss = 2.4595\n",
      "Epoch 288: loss = 2.4595\n",
      "Epoch 289: loss = 2.4595\n",
      "Epoch 290: loss = 2.4595\n",
      "Epoch 291: loss = 2.4594\n",
      "Epoch 292: loss = 2.4594\n",
      "Epoch 293: loss = 2.4594\n",
      "Epoch 294: loss = 2.4594\n",
      "Epoch 295: loss = 2.4594\n",
      "Epoch 296: loss = 2.4593\n",
      "Epoch 297: loss = 2.4593\n",
      "Epoch 298: loss = 2.4593\n",
      "Epoch 299: loss = 2.4593\n",
      "Epoch 300: loss = 2.4593\n",
      "Epoch 301: loss = 2.4592\n",
      "Epoch 302: loss = 2.4592\n",
      "Epoch 303: loss = 2.4592\n",
      "Epoch 304: loss = 2.4592\n",
      "Epoch 305: loss = 2.4592\n",
      "Epoch 306: loss = 2.4591\n",
      "Epoch 307: loss = 2.4591\n",
      "Epoch 308: loss = 2.4591\n",
      "Epoch 309: loss = 2.4591\n",
      "Epoch 310: loss = 2.4591\n",
      "Epoch 311: loss = 2.4590\n",
      "Epoch 312: loss = 2.4590\n",
      "Epoch 313: loss = 2.4590\n",
      "Epoch 314: loss = 2.4590\n",
      "Epoch 315: loss = 2.4590\n",
      "Epoch 316: loss = 2.4590\n",
      "Epoch 317: loss = 2.4589\n",
      "Epoch 318: loss = 2.4589\n",
      "Epoch 319: loss = 2.4589\n",
      "Epoch 320: loss = 2.4589\n",
      "Epoch 321: loss = 2.4589\n",
      "Epoch 322: loss = 2.4588\n",
      "Epoch 323: loss = 2.4588\n",
      "Epoch 324: loss = 2.4588\n",
      "Epoch 325: loss = 2.4588\n",
      "Epoch 326: loss = 2.4588\n",
      "Epoch 327: loss = 2.4588\n",
      "Epoch 328: loss = 2.4587\n",
      "Epoch 329: loss = 2.4587\n",
      "Epoch 330: loss = 2.4587\n",
      "Epoch 331: loss = 2.4587\n",
      "Epoch 332: loss = 2.4587\n",
      "Epoch 333: loss = 2.4587\n",
      "Epoch 334: loss = 2.4586\n",
      "Epoch 335: loss = 2.4586\n",
      "Epoch 336: loss = 2.4586\n",
      "Epoch 337: loss = 2.4586\n",
      "Epoch 338: loss = 2.4586\n",
      "Epoch 339: loss = 2.4586\n",
      "Epoch 340: loss = 2.4585\n",
      "Epoch 341: loss = 2.4585\n",
      "Epoch 342: loss = 2.4585\n",
      "Epoch 343: loss = 2.4585\n",
      "Epoch 344: loss = 2.4585\n",
      "Epoch 345: loss = 2.4585\n",
      "Epoch 346: loss = 2.4585\n",
      "Epoch 347: loss = 2.4584\n",
      "Epoch 348: loss = 2.4584\n",
      "Epoch 349: loss = 2.4584\n",
      "Epoch 350: loss = 2.4584\n",
      "Epoch 351: loss = 2.4584\n",
      "Epoch 352: loss = 2.4584\n",
      "Epoch 353: loss = 2.4583\n",
      "Epoch 354: loss = 2.4583\n",
      "Epoch 355: loss = 2.4583\n",
      "Epoch 356: loss = 2.4583\n",
      "Epoch 357: loss = 2.4583\n",
      "Epoch 358: loss = 2.4583\n",
      "Epoch 359: loss = 2.4583\n",
      "Epoch 360: loss = 2.4582\n",
      "Epoch 361: loss = 2.4582\n",
      "Epoch 362: loss = 2.4582\n",
      "Epoch 363: loss = 2.4582\n",
      "Epoch 364: loss = 2.4582\n",
      "Epoch 365: loss = 2.4582\n",
      "Epoch 366: loss = 2.4582\n",
      "Epoch 367: loss = 2.4582\n",
      "Epoch 368: loss = 2.4581\n",
      "Epoch 369: loss = 2.4581\n",
      "Epoch 370: loss = 2.4581\n",
      "Epoch 371: loss = 2.4581\n",
      "Epoch 372: loss = 2.4581\n",
      "Epoch 373: loss = 2.4581\n",
      "Epoch 374: loss = 2.4581\n",
      "Epoch 375: loss = 2.4581\n",
      "Epoch 376: loss = 2.4580\n",
      "Epoch 377: loss = 2.4580\n",
      "Epoch 378: loss = 2.4580\n",
      "Epoch 379: loss = 2.4580\n",
      "Epoch 380: loss = 2.4580\n",
      "Epoch 381: loss = 2.4580\n",
      "Epoch 382: loss = 2.4580\n",
      "Epoch 383: loss = 2.4580\n",
      "Epoch 384: loss = 2.4579\n",
      "Epoch 385: loss = 2.4579\n",
      "Epoch 386: loss = 2.4579\n",
      "Epoch 387: loss = 2.4579\n",
      "Epoch 388: loss = 2.4579\n",
      "Epoch 389: loss = 2.4579\n",
      "Epoch 390: loss = 2.4579\n",
      "Epoch 391: loss = 2.4579\n",
      "Epoch 392: loss = 2.4578\n",
      "Epoch 393: loss = 2.4578\n",
      "Epoch 394: loss = 2.4578\n",
      "Epoch 395: loss = 2.4578\n",
      "Epoch 396: loss = 2.4578\n",
      "Epoch 397: loss = 2.4578\n",
      "Epoch 398: loss = 2.4578\n",
      "Epoch 399: loss = 2.4578\n",
      "Epoch 400: loss = 2.4578\n",
      "Epoch 401: loss = 2.4577\n",
      "Epoch 402: loss = 2.4577\n",
      "Epoch 403: loss = 2.4577\n",
      "Epoch 404: loss = 2.4577\n",
      "Epoch 405: loss = 2.4577\n",
      "Epoch 406: loss = 2.4577\n",
      "Epoch 407: loss = 2.4577\n",
      "Epoch 408: loss = 2.4577\n",
      "Epoch 409: loss = 2.4577\n",
      "Epoch 410: loss = 2.4576\n",
      "Epoch 411: loss = 2.4576\n",
      "Epoch 412: loss = 2.4576\n",
      "Epoch 413: loss = 2.4576\n",
      "Epoch 414: loss = 2.4576\n",
      "Epoch 415: loss = 2.4576\n",
      "Epoch 416: loss = 2.4576\n",
      "Epoch 417: loss = 2.4576\n",
      "Epoch 418: loss = 2.4576\n",
      "Epoch 419: loss = 2.4576\n",
      "Epoch 420: loss = 2.4575\n",
      "Epoch 421: loss = 2.4575\n",
      "Epoch 422: loss = 2.4575\n",
      "Epoch 423: loss = 2.4575\n",
      "Epoch 424: loss = 2.4575\n",
      "Epoch 425: loss = 2.4575\n",
      "Epoch 426: loss = 2.4575\n",
      "Epoch 427: loss = 2.4575\n",
      "Epoch 428: loss = 2.4575\n",
      "Epoch 429: loss = 2.4575\n",
      "Epoch 430: loss = 2.4574\n",
      "Epoch 431: loss = 2.4574\n",
      "Epoch 432: loss = 2.4574\n",
      "Epoch 433: loss = 2.4574\n",
      "Epoch 434: loss = 2.4574\n",
      "Epoch 435: loss = 2.4574\n",
      "Epoch 436: loss = 2.4574\n",
      "Epoch 437: loss = 2.4574\n",
      "Epoch 438: loss = 2.4574\n",
      "Epoch 439: loss = 2.4574\n",
      "Epoch 440: loss = 2.4574\n",
      "Epoch 441: loss = 2.4573\n",
      "Epoch 442: loss = 2.4573\n",
      "Epoch 443: loss = 2.4573\n",
      "Epoch 444: loss = 2.4573\n",
      "Epoch 445: loss = 2.4573\n",
      "Epoch 446: loss = 2.4573\n",
      "Epoch 447: loss = 2.4573\n",
      "Epoch 448: loss = 2.4573\n",
      "Epoch 449: loss = 2.4573\n",
      "Epoch 450: loss = 2.4573\n",
      "Epoch 451: loss = 2.4573\n",
      "Epoch 452: loss = 2.4572\n",
      "Epoch 453: loss = 2.4572\n",
      "Epoch 454: loss = 2.4572\n",
      "Epoch 455: loss = 2.4572\n",
      "Epoch 456: loss = 2.4572\n",
      "Epoch 457: loss = 2.4572\n",
      "Epoch 458: loss = 2.4572\n",
      "Epoch 459: loss = 2.4572\n",
      "Epoch 460: loss = 2.4572\n",
      "Epoch 461: loss = 2.4572\n",
      "Epoch 462: loss = 2.4572\n",
      "Epoch 463: loss = 2.4572\n",
      "Epoch 464: loss = 2.4571\n",
      "Epoch 465: loss = 2.4571\n",
      "Epoch 466: loss = 2.4571\n",
      "Epoch 467: loss = 2.4571\n",
      "Epoch 468: loss = 2.4571\n",
      "Epoch 469: loss = 2.4571\n",
      "Epoch 470: loss = 2.4571\n",
      "Epoch 471: loss = 2.4571\n",
      "Epoch 472: loss = 2.4571\n",
      "Epoch 473: loss = 2.4571\n",
      "Epoch 474: loss = 2.4571\n",
      "Epoch 475: loss = 2.4571\n",
      "Epoch 476: loss = 2.4571\n",
      "Epoch 477: loss = 2.4570\n",
      "Epoch 478: loss = 2.4570\n",
      "Epoch 479: loss = 2.4570\n",
      "Epoch 480: loss = 2.4570\n",
      "Epoch 481: loss = 2.4570\n",
      "Epoch 482: loss = 2.4570\n",
      "Epoch 483: loss = 2.4570\n",
      "Epoch 484: loss = 2.4570\n",
      "Epoch 485: loss = 2.4570\n",
      "Epoch 486: loss = 2.4570\n",
      "Epoch 487: loss = 2.4570\n",
      "Epoch 488: loss = 2.4570\n",
      "Epoch 489: loss = 2.4570\n",
      "Epoch 490: loss = 2.4570\n",
      "Epoch 491: loss = 2.4569\n",
      "Epoch 492: loss = 2.4569\n",
      "Epoch 493: loss = 2.4569\n",
      "Epoch 494: loss = 2.4569\n",
      "Epoch 495: loss = 2.4569\n",
      "Epoch 496: loss = 2.4569\n",
      "Epoch 497: loss = 2.4569\n",
      "Epoch 498: loss = 2.4569\n",
      "Epoch 499: loss = 2.4569\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:32:04.336807Z",
     "start_time": "2025-03-12T03:32:04.334909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # minibatch gradient descent (putting everything together)\n",
    "# bsz = 1000  # batch size\n",
    "# for epoch in range(5):\n",
    "#     for i in range(0, len(x_enc), bsz):\n",
    "#         x_batch = x_enc[i:i+bsz]\n",
    "#         y_batch = ys[i:i+bsz]\n",
    "#\n",
    "#         # feed forward\n",
    "#         logits = x_batch @ W  # log-counts\n",
    "#         counts = logits.exp()  # counts\n",
    "#         probs = counts / counts.sum(dim=1, keepdim=True)  # probabilities for next character\n",
    "#\n",
    "#         # compute NLL loss\n",
    "#         ys_pred = probs[torch.arange(0, len(y_batch)), y_batch]  # from each batch, select probability that we output the correct next token\n",
    "#         loss = -ys_pred.log().mean()  # log, negate and mean the selected probabilities\n",
    "#         loss += 0.01 * (W**2).mean()  # L2 regularization term (keep weights small by adding penalty)\n",
    "#\n",
    "#         # backward pass\n",
    "#         W.grad = None  # zero the gradient\n",
    "#         loss.backward()\n",
    "#\n",
    "#         # update model\n",
    "#         W.data -= eta * W.grad\n",
    "#\n",
    "#     print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")"
   ],
   "id": "433b4113d6fc594d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-12T03:32:04.647735Z",
     "start_time": "2025-03-12T03:32:04.506631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we expect to get roughly the same loss as in p1_bigram_lookup_table with our \"manually tuned\" bigram model (~2.45).\n",
    "#   - it turns out that our original bigram model is trivially optimal.\n",
    "#   - gradient descent has learned the same optimal strategy.\n",
    "#       - W approximates N from the \"manually tuned\" model.\n",
    "#   - this is the best the model can do with the data we've made available to it (bigrams), but the approach is flexible.\n",
    "#       - if we feed it more information and add more neurons, it will find more complex relationships and perform better\n",
    "#         than any \"manually tuned\" model we could come up with.\n",
    "\n",
    "# To wrap things up, lets generate some samples from our neural net model.\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0  # id of first token '.'\n",
    "    while True:\n",
    "        # fetch probability distribution from neural network\n",
    "        x = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = x @ W  # predict log-counts\n",
    "        counts = logits.exp()  # counts, equivalent to N\n",
    "        p = counts / counts.sum(dim=1, keepdim=True)  # probabilities for next character\n",
    "        # draw next token from multinomial distribution\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "\n",
    "    print(''.join(out))"
   ],
   "id": "ddc8c2e1ff9bf8ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eian.\n",
      "ely.\n",
      "fan.\n",
      "rhmariaifaho.\n",
      "a.\n",
      "an.\n",
      "shadaemalahei.\n",
      "dr.\n",
      "may.\n",
      "asenbren.\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
