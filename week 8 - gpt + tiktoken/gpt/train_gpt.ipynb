{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T23:26:55.886710Z",
     "start_time": "2025-03-16T23:26:54.927817Z"
    }
   },
   "source": [
    "# The way to think about this is that:\n",
    "#   - self-attention is the communication between the tokens\n",
    "#   - then once they've gathered all the data\n",
    "#   - now they need to \"think\" on that data individually\n",
    "#       - i.e. compute in the Linear followed by Relu\n",
    "\n",
    "# We can stack these layers now:\n",
    "#   - attention block 1\n",
    "#       - MultiHeadAttention 1\n",
    "#       - Linear 1\n",
    "#       - Relu 1\n",
    "#   - attention block 2\n",
    "#       - MultiHeadAttention 2\n",
    "#       - Linear 2\n",
    "#       - Relu 2\n",
    "#   - ...\n",
    "\n",
    "# communication/computation sandwiches:\n",
    "#   - communication -> computation -> communication -> computation -> ...\n",
    "\n",
    "# This network ends up getting pretty deep, hence \"deep learning\".\n",
    "# Without residual connections and normalization layers, the network is unstable and hard to train.\n",
    "# Just adding the residual connections stabilizes the network (and it performs way better... wtf!).\n",
    "# Slightly better stability with layer norm\n",
    "# Dropout helps with overfitting\n",
    "# Scaling hyperparameters makes the language model far more powerful, but promotes overfitting\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import datetime\n",
    "\n",
    "torch.set_default_device(\"mps\")  # use gpu\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 32 # how many independent sequences will we process in parallel?\n",
    "block_size = 512 # what is the maximum context length for predictions?\n",
    "n_embd = 768\n",
    "n_head = 12  # every head is 64 dimensional\n",
    "n_transformer_blocks = 12\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('res/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    m.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = m(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    m.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei_logits = q @ k.transpose(-2, -1) * self.head_size**-0.5  # (B, T, head_size) @ (B, head_size, T)  -->  (B, T, T)\n",
    "        wei_logits = wei_logits.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei_logits, dim=-1)\n",
    "        wei = self.dropout(wei)  # prevent some of the nodes from communicating with dropout (avoids overfitting) (creates ensemble)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, head_size)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, head_size)  -->  (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "# todo: implementing this was very simple but what does it mean and why is better than single head?\n",
    "#   - you simply have the same attention computation happening multiple times per head in parallel\n",
    "#   - each attention head learns to communicate a different thing between the tokens\n",
    "#       - i.e. each head learns different affinities for relationships between the current token and previous tokens\n",
    "#       - i.e. each head learns different ways the context affects the next token in the sequence\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])  # heads of attention\n",
    "        self.proj = nn.Linear(n_embd, n_embd)  # \"projection back into the residual pathway\"\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # concatenate over the channel dimension (B, T, C)\n",
    "        out = self.proj(out)  # \"project back into the residual pathway\"\n",
    "        out = self.dropout(out)   # apply droupout on residual path\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.proj = nn.Linear(4 * n_embd, n_embd)  # \"project back into the residual pathway\"\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        out = self.proj(out)  # \"project back into the residual pathway\"\n",
    "        out = self.dropout(out)  # apply droupout on residual path\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)  # feed forward per token (cuz applies only to last dimension)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)  # batch norm per token (cuz applies only to last dimension)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)  # batch norm per token (cuz applies only to last dimension)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # `x = x + <some computation>` is the residual pathway...\n",
    "        x = x + self.sa(self.ln1(x))  # MultiHeadAttention now also \"projects back into the residual pathway\"\n",
    "        x = x + self.ffwd(self.ln2(x))  # FeedForward now also \"projects back into the residual pathway\"\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)  # token information (in token embedding space)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # positional information (in position embedding space)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head) for _ in range(n_transformer_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)  # embedding space --> vocabulary space\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C)\n",
    "        x = self.blocks(x) # (B, T, C)\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx (B, T) array of indices in the current context\n",
    "            # (never pass more than block_size tokens)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:11:57.869936Z",
     "start_time": "2025-03-17T16:37:19.785332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "max_iters = 5000\n",
    "ping_interval = 50\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 50\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "torch.set_default_device(\"cpu\")  # use cpu\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n",
    "torch.set_default_device(\"mps\")  # back to gpu\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % ping_interval == 0:\n",
    "        print(f\"Iteration {iter} - {datetime.datetime.now()}\")\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "losses = estimate_loss()\n",
    "print(f\"Final: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
   ],
   "id": "ed3c17182aa4a3c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - 2025-03-17 09:37:19.792961\n",
      "step 0: train loss 0.0788, val loss 3.1145\n",
      "Iteration 50 - 2025-03-17 09:41:13.630140\n",
      "Iteration 100 - 2025-03-17 09:42:58.828082\n",
      "Iteration 150 - 2025-03-17 09:44:43.702476\n",
      "Iteration 200 - 2025-03-17 09:46:29.231300\n",
      "Iteration 250 - 2025-03-17 09:49:45.625336\n",
      "Iteration 300 - 2025-03-17 09:51:35.061981\n",
      "Iteration 350 - 2025-03-17 09:53:22.197388\n",
      "Iteration 400 - 2025-03-17 09:55:07.869515\n",
      "Iteration 450 - 2025-03-17 09:56:52.600465\n",
      "Iteration 500 - 2025-03-17 09:58:37.116374\n",
      "step 500: train loss 0.0716, val loss 3.2624\n",
      "Iteration 550 - 2025-03-17 10:01:24.741699\n",
      "Iteration 600 - 2025-03-17 10:03:08.923660\n",
      "Iteration 650 - 2025-03-17 10:04:55.466170\n",
      "Iteration 700 - 2025-03-17 10:06:42.796788\n",
      "Iteration 750 - 2025-03-17 10:08:30.047997\n",
      "Iteration 800 - 2025-03-17 10:10:16.773955\n",
      "Iteration 850 - 2025-03-17 10:12:02.040344\n",
      "Iteration 900 - 2025-03-17 10:13:48.262719\n",
      "Iteration 950 - 2025-03-17 10:15:36.336642\n",
      "Iteration 1000 - 2025-03-17 10:17:23.571704\n",
      "step 1000: train loss 0.0653, val loss 3.3411\n",
      "Iteration 1050 - 2025-03-17 10:20:16.538170\n",
      "Iteration 1100 - 2025-03-17 10:22:19.656095\n",
      "Iteration 1150 - 2025-03-17 10:24:28.966633\n",
      "Iteration 1200 - 2025-03-17 10:26:39.421965\n",
      "Iteration 1250 - 2025-03-17 10:28:46.242946\n",
      "Iteration 1300 - 2025-03-17 10:30:56.199952\n",
      "Iteration 1350 - 2025-03-17 10:33:00.997668\n",
      "Iteration 1400 - 2025-03-17 10:35:03.636621\n",
      "Iteration 1450 - 2025-03-17 10:37:00.122257\n",
      "Iteration 1500 - 2025-03-17 10:38:59.615104\n",
      "step 1500: train loss 0.0611, val loss 3.4997\n",
      "Iteration 1550 - 2025-03-17 10:42:08.941070\n",
      "Iteration 1600 - 2025-03-17 10:44:07.098407\n",
      "Iteration 1650 - 2025-03-17 10:46:04.394382\n",
      "Iteration 1700 - 2025-03-17 10:48:04.770478\n",
      "Iteration 1750 - 2025-03-17 10:50:03.232386\n",
      "Iteration 1800 - 2025-03-17 10:51:56.962419\n",
      "Iteration 1850 - 2025-03-17 10:53:50.044067\n",
      "Iteration 1900 - 2025-03-17 10:55:44.477089\n",
      "Iteration 1950 - 2025-03-17 10:57:51.591336\n",
      "Iteration 2000 - 2025-03-17 10:59:53.192557\n",
      "step 2000: train loss 0.0585, val loss 3.5414\n",
      "Iteration 2050 - 2025-03-17 11:03:01.510941\n",
      "Iteration 2100 - 2025-03-17 11:05:01.994278\n",
      "Iteration 2150 - 2025-03-17 11:07:58.050066\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     25\u001B[39m     logits, loss = m(xb, yb)\n\u001B[32m     26\u001B[39m     optimizer.zero_grad(set_to_none=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m     \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m     optimizer.step()\n\u001B[32m     30\u001B[39m losses = estimate_loss()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 7 - makemore/.venv/lib/python3.13/site-packages/torch/_tensor.py:639\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    595\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001B[39;00m\n\u001B[32m    596\u001B[39m \n\u001B[32m    597\u001B[39m \u001B[33;03mThe graph is differentiated using the chain rule. If the tensor is\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    636\u001B[39m \u001B[33;03m        used to compute the :attr:`tensors`.\u001B[39;00m\n\u001B[32m    637\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m--> \u001B[39m\u001B[32m639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mhandle_torch_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    640\u001B[39m \u001B[43m        \u001B[49m\u001B[43mTensor\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    641\u001B[39m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    642\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    643\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    644\u001B[39m \u001B[43m        \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m=\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    645\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    646\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    647\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    648\u001B[39m torch.autograd.backward(\n\u001B[32m    649\u001B[39m     \u001B[38;5;28mself\u001B[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001B[32m    650\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 7 - makemore/.venv/lib/python3.13/site-packages/torch/overrides.py:1721\u001B[39m, in \u001B[36mhandle_torch_function\u001B[39m\u001B[34m(public_api, relevant_args, *args, **kwargs)\u001B[39m\n\u001B[32m   1717\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m _is_torch_function_mode_enabled():\n\u001B[32m   1718\u001B[39m     \u001B[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001B[39;00m\n\u001B[32m   1719\u001B[39m     \u001B[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001B[39;00m\n\u001B[32m   1720\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m _pop_mode_temporarily() \u001B[38;5;28;01mas\u001B[39;00m mode:\n\u001B[32m-> \u001B[39m\u001B[32m1721\u001B[39m         result = \u001B[43mmode\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__torch_function__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpublic_api\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtypes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1722\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mNotImplemented\u001B[39m:\n\u001B[32m   1723\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 7 - makemore/.venv/lib/python3.13/site-packages/torch/utils/_device.py:104\u001B[39m, in \u001B[36mDeviceContext.__torch_function__\u001B[39m\u001B[34m(self, func, types, args, kwargs)\u001B[39m\n\u001B[32m    102\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m func \u001B[38;5;129;01min\u001B[39;00m _device_constructors() \u001B[38;5;129;01mand\u001B[39;00m kwargs.get(\u001B[33m'\u001B[39m\u001B[33mdevice\u001B[39m\u001B[33m'\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    103\u001B[39m     kwargs[\u001B[33m'\u001B[39m\u001B[33mdevice\u001B[39m\u001B[33m'\u001B[39m] = \u001B[38;5;28mself\u001B[39m.device\n\u001B[32m--> \u001B[39m\u001B[32m104\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 7 - makemore/.venv/lib/python3.13/site-packages/torch/_tensor.py:648\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    638\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    639\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    640\u001B[39m         Tensor.backward,\n\u001B[32m    641\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    646\u001B[39m         inputs=inputs,\n\u001B[32m    647\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m648\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    649\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    650\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 7 - makemore/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:353\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    348\u001B[39m     retain_graph = create_graph\n\u001B[32m    350\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m353\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 7 - makemore/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:824\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    822\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    823\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m824\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    825\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    826\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    827\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    828\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T18:56:35.961813Z",
     "start_time": "2025-03-17T18:12:07.869760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate from the model\n",
    "context_tokens = encode(\"\"\"To be, or not to be: that is the question:\n",
    "Whether 'tis nobler in the mind to suffer\n",
    "The slings and arrows of outrageous fortune,\n",
    "Or to take arms against a sea of troubles,\n",
    "And by opposing end them? To die: to sleep;\n",
    "No more; and by a sleep to say we end\n",
    "The heart-ache and the thousand natural shocks\n",
    "That flesh is heir to, 'tis a consummation\n",
    "Devoutly to be wish'd. To die, to sleep;\n",
    "To sleep: perchance to dream: ay, there's the rub;\"\"\")\n",
    "print(len(context_tokens))\n",
    "# context = torch.zeros((1, 1), dtype=torch.long)\n",
    "context = torch.tensor(context_tokens, dtype=torch.long).reshape(1, -1)\n",
    "print(context.shape)\n",
    "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
   ],
   "id": "6d98f9c8521ed201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436\n",
      "torch.Size([1, 436])\n",
      "To be, or not to be: that is the question:\n",
      "Whether 'tis nobler in the mind to suffer\n",
      "The slings and arrows of outrageous fortune,\n",
      "Or to take arms against a sea of troubles,\n",
      "And by opposing end them? To die: to sleep;\n",
      "No more; and by a sleep to say we end\n",
      "The heart-ache and the thousand natural shocks\n",
      "That flesh is heir to, 'tis a consummation\n",
      "Devoutly to be wish'd. To die, to sleep;\n",
      "To sleep: perchance to dream: ay, there's the rub;\n",
      "Or in thy poor breath, still thy land and bosom,\n",
      "That then ranks thought to bear them at the heel\n",
      "With indistrumenting this feast. There is not find\n",
      "Mis hate no exile of us: stay we the counter\n",
      "And see the noble gone, and with them follow.\n",
      "\n",
      "HORTENSIO:\n",
      "How! traitor, go Mercutio!\n",
      "\n",
      "MARCIUS:\n",
      "A said I would\n",
      "Not lose thee, but that 'tis hate, the one\n",
      "Of my poor heart's her means: 'twas a noble wench'd\n",
      "When I have said, but not honey: but I'll doubt not\n",
      "With that he would said in her before keeping a\n",
      "For the earth and natural for complainty.\n",
      "\n",
      "BALTHASAR:\n",
      "We do deny, sir; to the devil not the depers.\n",
      "\n",
      "ROMEO:\n",
      "Know make me treason, of the day, sir, to you.\n",
      "\n",
      "BENVOLIO:\n",
      "I aid, sir, to you.\n",
      "\n",
      "MERCUTIO:\n",
      "Ay, a sad you good house: promised your hands:\n",
      "'Tis Aufidius, did entreat his tentreaties\n",
      "Our wisdom agony.\n",
      "\n",
      "MERCUTIO:\n",
      "Tell me in safe.\n",
      "\n",
      "ROMEO:\n",
      "What is my reason?\n",
      "\n",
      "MERCUTIO:\n",
      "By my troth, I beseech you?\n",
      "\n",
      "MERCUTIO:\n",
      "Well, well, no more.\n",
      "\n",
      "BENVOLIO:\n",
      "How is that he bears you.\n",
      "\n",
      "MERCUTIO:\n",
      "Tybalt, then, thou ar\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# As I stead--O kname'st to be secuted for Rome.\n",
    "#\n",
    "# BUCKINGHAM:\n",
    "# I do believe mine.\n",
    "#\n",
    "# BUCKINGHAM:\n",
    "# How, soft! my lord, ignorant, let it go.\n",
    "#\n",
    "# KING RICHARD III:\n",
    "# Now, in good time: God my lords, 'tis gone.\n",
    "#\n",
    "# BUCKINGHAM:\n",
    "# Farewell, on that fault of much, when thou wert so!\n",
    "#\n",
    "# QUEEN ELIZABETH:\n",
    "# Shall I might, give me thy heart with hope,\n",
    "# I'll mperise thee in my heart.\n",
    "#\n",
    "# KING RICHARD III:\n",
    "# Stry than a wchild and longers die,\n",
    "# Why thou hast socian he still above his hinour:\n",
    "# When hast he said aspect, thy sovereign's heir,\n",
    "# His heart of will hence to France to forgive Engles,\n",
    "# Whose dim two do much up his with him;\n",
    "# And walk will teach the wings shame to wield as my\n",
    "# draws and unpiting it to my comfort,\n",
    "# And made it broking it from my brother,\n",
    "# And balm to myself-coatrived soul at his.\n",
    "#\n",
    "# DUCHESS:\n",
    "# If I do not, sin such a medle with thy brother,\n",
    "# Where I was slain imprisonment made\n",
    "# That when I was guilty in my power.\n",
    "#\n",
    "# KING EDWARD IV:\n",
    "# Take men by this young Keng of beast;\n",
    "# Young Next of the next which now your grace,\n",
    "# Spe"
   ],
   "id": "299d90c1c620f408"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Avast it was bags; for that I do keep it not.\n",
    "# And yet, being so, to get a jade of thee;\n",
    "# Thou art the world, all the world will be so.\n",
    "# Arise, the heavens look upon thy heaven,\n",
    "# And still thy lips will dispatch thee;\n",
    "# And for my woman crush the will be set,\n",
    "# As he, and my nurse, when he doth rage on France,\n",
    "# O thou art sent for a hot and worthy death!\n",
    "# Did villain too fawn upon this king,\n",
    "# Down with down with rail down their purpose!\n",
    "#\n",
    "# KING EDWARD IV:\n",
    "# Away with her; go, bear her hence perforce.\n",
    "#\n",
    "# QUEEN MARGARET:\n",
    "# And give me her hunce, and my vowery short,\n",
    "# My crown king away from her deceit.\n",
    "# I spy the traitor of the people's eyes,\n",
    "# Are they to will false 'O.' You putt return'd,\n",
    "# Most gracious sovereign, not wed have made good\n",
    "# This even would desire have cut off.\n",
    "# I cannot bear a man tray, good deserves a left,\n",
    "# And do not incurre her talk of battle heralm;\n",
    "# For while I undo the man of my breast,\n",
    "# Compare me him away: wanting at the gates,\n",
    "# Gaunt by some white all defects I live;\n",
    "# Or as I have by occasion"
   ],
   "id": "692c47840185bd9c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
