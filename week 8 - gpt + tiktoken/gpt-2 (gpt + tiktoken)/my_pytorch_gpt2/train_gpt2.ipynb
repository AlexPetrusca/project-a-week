{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:35:17.906072Z",
     "start_time": "2025-03-24T16:35:15.767226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# Model Definitions\n",
    "# -------------------------------\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import tiktoken\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer\n",
    "        return torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)"
   ],
   "id": "8c0ea0ee7eae12c1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:35:18.952289Z",
     "start_time": "2025-03-24T16:35:18.947790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# Data Loader\n",
    "# -------------------------------\n",
    "\n",
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = torch.tensor(npt, dtype=torch.long)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # get the shard filenames\n",
    "        data_root = \"../res/edu_fineweb10B\"\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        print(f\"found {len(shards)} shards for split {split}\")\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ],
   "id": "bb2ff2fb4845455b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:35:27.519407Z",
     "start_time": "2025-03-24T16:35:21.630117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# Build Model & Optimizer\n",
    "# -------------------------------\n",
    "\n",
    "device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "B = 8 # micro batch size\n",
    "T = 1024 # sequence length\n",
    "total_batch_size = 64 * B * T // 16\n",
    "assert total_batch_size % (B * T) == 0, \"make sure total_batch_size is divisible by B * T * ddp_world_size\"\n",
    "grad_accum_steps = total_batch_size // (B * T)\n",
    "print(f\"total desired batch size: {total_batch_size}\")\n",
    "print(f\"=> calculated gradient accumulation steps: {grad_accum_steps}\")\n",
    "\n",
    "train_loader = DataLoaderLite(B=B, T=T, split=\"train\")\n",
    "val_loader = DataLoaderLite(B=B, T=T, split=\"val\")\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "# create model\n",
    "model = GPT(GPTConfig(vocab_size=50304))\n",
    "# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n",
    "model.to(device)\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4)"
   ],
   "id": "5b558b20ac989599",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "total desired batch size: 32768\n",
      "=> calculated gradient accumulation steps: 4\n",
      "found 99 shards for split train\n",
      "found 1 shards for split val\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T16:36:26.646864Z",
     "start_time": "2025-03-24T16:35:30.165457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -------------------------------\n",
    "# Train\n",
    "# -------------------------------\n",
    "\n",
    "# create the log directory we will write checkpoints to and log to\n",
    "log_dir = \"log\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_file = os.path.join(log_dir, f\"log.txt\")\n",
    "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
    "    pass\n",
    "\n",
    "# optimize!\n",
    "max_steps = 1000\n",
    "start = datetime.now()\n",
    "for step in range(max_steps):\n",
    "    t0 = time.time()\n",
    "    last_step = (step == max_steps - 1)\n",
    "\n",
    "    # once in a while evaluate our validation loss\n",
    "    if step % 250 == 0 or last_step:\n",
    "        model.eval()\n",
    "        val_loader.reset()\n",
    "        with torch.no_grad():\n",
    "            val_loss_accum = 0.0\n",
    "            val_loss_steps = 20\n",
    "            for _ in range(val_loss_steps):\n",
    "                x, y = val_loader.next_batch()\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(x, y)\n",
    "                loss = loss / val_loss_steps\n",
    "                val_loss_accum += loss.detach()\n",
    "\n",
    "        print(f\"validation loss: {val_loss_accum.item():.4f}\")\n",
    "        with open(log_file, \"a\") as f:\n",
    "            f.write(f\"{step} val {val_loss_accum.item():.4f}\\n\")\n",
    "        if step > 0 and (step % 5000 == 0 or last_step):\n",
    "            # optionally write model checkpoints\n",
    "            checkpoint_path = os.path.join(log_dir, f\"model_{step:05d}.pt\")\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'config': model.config,\n",
    "                'step': step,\n",
    "                'val_loss': val_loss_accum.item()\n",
    "            }\n",
    "            # you might also want to add optimizer.state_dict() and\n",
    "            # rng seeds etc., if you wanted to more exactly resume training\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # once in a while generate from the model (except step 0, which is noise)\n",
    "    if ((step > 0 and step % 250 == 0) or last_step):\n",
    "        model.eval()\n",
    "        num_return_sequences = 4\n",
    "        max_length = 32\n",
    "        tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "        xgen = tokens.to(device)\n",
    "        sample_rng = torch.Generator(device=device)\n",
    "        sample_rng.manual_seed(42)\n",
    "        while xgen.size(1) < max_length:\n",
    "            # forward the model to get the logits\n",
    "            with torch.no_grad():\n",
    "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
    "                # take the logits at the last position\n",
    "                logits = logits[:, -1, :] # (B, vocab_size)\n",
    "                # get the probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                # do top-k sampling of 50 (huggingface pipeline default)\n",
    "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
    "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "                # select a token from the top-k probabilities\n",
    "                # note: multinomial does not demand the input to sum to 1\n",
    "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
    "                # gather the corresponding indices\n",
    "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
    "                # append to the sequence\n",
    "                xgen = torch.cat((xgen, xcol), dim=1)\n",
    "        # print the generated text\n",
    "        for i in range(num_return_sequences):\n",
    "            tokens = xgen[i, :max_length].tolist()\n",
    "            decoded = enc.decode(tokens)\n",
    "            print(f\"sample {i}: {decoded}\")\n",
    "\n",
    "    # do one step of the optimization\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_accum = 0.0\n",
    "    for micro_step in range(grad_accum_steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # added after video, this field is also used by the forward pass.\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "            logits, loss = model(x, y)\n",
    "        # we have to scale the loss to account for gradient accumulation,\n",
    "        # because the gradients just add on each successive backward().\n",
    "        # addition of gradients corresponds to a SUM in the objective, but\n",
    "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss_accum += loss.detach()\n",
    "        loss.backward()\n",
    "\n",
    "    # clip grads to a max norm & update parameters\n",
    "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000 # time difference in milliseconds\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T * grad_accum_steps) / (t1 - t0)\n",
    "    iterations_per_sec = 1 / (t1 - t0)\n",
    "    print(f\"{datetime.now()} - step {step}, loss: {loss:.4f}, norm: {norm:.4f}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"total time: {end - start}\")\n",
    "print(f\"average tokens/sec: {(train_loader.B * train_loader.T * max_steps * grad_accum_steps) / (end.timestamp() - start.timestamp())}\")"
   ],
   "id": "a31db7f69bbae845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-24 09:35:31.809551 - step 0, loss: 10.9636, norm: 15.5922, dt: 1641.21ms, tok/sec: 19965.79 tokens/sec\n",
      "2025-03-24 09:35:33.183788 - step 1, loss: 10.0829, norm: 3.6327, dt: 1373.70ms, tok/sec: 23853.81 tokens/sec\n",
      "2025-03-24 09:35:34.291254 - step 2, loss: 9.6560, norm: 2.7848, dt: 1101.26ms, tok/sec: 29754.95 tokens/sec\n",
      "2025-03-24 09:35:35.395313 - step 3, loss: 9.4537, norm: 3.0677, dt: 1098.92ms, tok/sec: 29818.37 tokens/sec\n",
      "2025-03-24 09:35:36.502855 - step 4, loss: 9.2952, norm: 2.9425, dt: 1103.44ms, tok/sec: 29696.30 tokens/sec\n",
      "2025-03-24 09:35:37.610923 - step 5, loss: 9.1533, norm: 2.7834, dt: 1104.31ms, tok/sec: 29672.70 tokens/sec\n",
      "2025-03-24 09:35:38.722468 - step 6, loss: 8.9446, norm: 2.7572, dt: 1108.04ms, tok/sec: 29572.81 tokens/sec\n",
      "2025-03-24 09:35:39.828018 - step 7, loss: 8.8268, norm: 2.6118, dt: 1103.23ms, tok/sec: 29701.88 tokens/sec\n",
      "2025-03-24 09:35:40.947484 - step 8, loss: 8.6757, norm: 2.5075, dt: 1115.62ms, tok/sec: 29371.93 tokens/sec\n",
      "2025-03-24 09:35:42.072075 - step 9, loss: 8.5220, norm: 2.4053, dt: 1121.80ms, tok/sec: 29210.17 tokens/sec\n",
      "2025-03-24 09:35:43.168200 - step 10, loss: 8.3700, norm: 2.3359, dt: 1095.50ms, tok/sec: 29911.40 tokens/sec\n",
      "2025-03-24 09:35:44.267441 - step 11, loss: 8.2353, norm: 2.1413, dt: 1095.90ms, tok/sec: 29900.46 tokens/sec\n",
      "2025-03-24 09:35:45.366666 - step 12, loss: 8.1251, norm: 2.1386, dt: 1096.89ms, tok/sec: 29873.50 tokens/sec\n",
      "2025-03-24 09:35:46.464685 - step 13, loss: 8.1582, norm: 1.9051, dt: 1094.19ms, tok/sec: 29947.33 tokens/sec\n",
      "2025-03-24 09:35:47.592727 - step 14, loss: 8.0462, norm: 1.8400, dt: 1123.91ms, tok/sec: 29155.26 tokens/sec\n",
      "2025-03-24 09:35:48.728573 - step 15, loss: 8.2066, norm: 1.8054, dt: 1135.03ms, tok/sec: 28869.72 tokens/sec\n",
      "2025-03-24 09:35:49.838253 - step 16, loss: 8.0087, norm: 1.6624, dt: 1109.18ms, tok/sec: 29542.49 tokens/sec\n",
      "2025-03-24 09:35:50.934853 - step 17, loss: 7.7623, norm: 2.1537, dt: 1093.68ms, tok/sec: 29961.12 tokens/sec\n",
      "2025-03-24 09:35:52.082673 - step 18, loss: 8.2369, norm: 1.9986, dt: 1144.56ms, tok/sec: 28629.36 tokens/sec\n",
      "2025-03-24 09:35:53.216100 - step 19, loss: 8.0101, norm: 2.4675, dt: 1132.40ms, tok/sec: 28936.70 tokens/sec\n",
      "2025-03-24 09:35:54.352847 - step 20, loss: 8.3366, norm: 2.0214, dt: 1134.94ms, tok/sec: 28871.96 tokens/sec\n",
      "2025-03-24 09:35:55.475774 - step 21, loss: 7.9769, norm: 1.8862, dt: 1119.53ms, tok/sec: 29269.40 tokens/sec\n",
      "2025-03-24 09:35:56.576598 - step 22, loss: 7.9375, norm: 1.5856, dt: 1096.98ms, tok/sec: 29871.05 tokens/sec\n",
      "2025-03-24 09:35:57.676026 - step 23, loss: 8.0908, norm: 1.9480, dt: 1096.65ms, tok/sec: 29880.14 tokens/sec\n",
      "2025-03-24 09:35:58.778257 - step 24, loss: 7.8175, norm: 2.1159, dt: 1098.68ms, tok/sec: 29824.96 tokens/sec\n",
      "2025-03-24 09:35:59.904612 - step 25, loss: 8.1234, norm: 2.3411, dt: 1124.10ms, tok/sec: 29150.51 tokens/sec\n",
      "2025-03-24 09:36:01.013717 - step 26, loss: 8.1291, norm: 1.6629, dt: 1104.30ms, tok/sec: 29673.10 tokens/sec\n",
      "2025-03-24 09:36:02.127246 - step 27, loss: 8.1496, norm: 1.5857, dt: 1110.54ms, tok/sec: 29506.24 tokens/sec\n",
      "2025-03-24 09:36:03.237666 - step 28, loss: 8.6210, norm: 1.5309, dt: 1108.23ms, tok/sec: 29567.96 tokens/sec\n",
      "2025-03-24 09:36:04.347941 - step 29, loss: 8.1722, norm: 2.1203, dt: 1105.80ms, tok/sec: 29632.82 tokens/sec\n",
      "2025-03-24 09:36:05.459989 - step 30, loss: 8.2441, norm: 2.2260, dt: 1107.65ms, tok/sec: 29583.43 tokens/sec\n",
      "2025-03-24 09:36:06.572159 - step 31, loss: 8.1245, norm: 2.0929, dt: 1109.93ms, tok/sec: 29522.66 tokens/sec\n",
      "2025-03-24 09:36:07.687500 - step 32, loss: 8.0486, norm: 2.0260, dt: 1113.07ms, tok/sec: 29439.30 tokens/sec\n",
      "2025-03-24 09:36:08.799273 - step 33, loss: 8.1808, norm: 1.9691, dt: 1107.63ms, tok/sec: 29583.78 tokens/sec\n",
      "2025-03-24 09:36:09.907917 - step 34, loss: 8.1151, norm: 2.3726, dt: 1106.07ms, tok/sec: 29625.69 tokens/sec\n",
      "2025-03-24 09:36:11.019152 - step 35, loss: 8.0283, norm: 1.9664, dt: 1107.23ms, tok/sec: 29594.54 tokens/sec\n",
      "2025-03-24 09:36:12.146684 - step 36, loss: 8.3184, norm: 2.0352, dt: 1123.18ms, tok/sec: 29174.37 tokens/sec\n",
      "2025-03-24 09:36:13.272694 - step 37, loss: 8.3494, norm: 2.0964, dt: 1124.56ms, tok/sec: 29138.46 tokens/sec\n",
      "2025-03-24 09:36:14.385027 - step 38, loss: 8.0905, norm: 2.0248, dt: 1111.03ms, tok/sec: 29493.22 tokens/sec\n",
      "2025-03-24 09:36:15.508211 - step 39, loss: 8.2011, norm: 2.2707, dt: 1120.09ms, tok/sec: 29254.76 tokens/sec\n",
      "2025-03-24 09:36:16.648408 - step 40, loss: 8.2375, norm: 2.4929, dt: 1138.88ms, tok/sec: 28772.23 tokens/sec\n",
      "2025-03-24 09:36:17.781477 - step 41, loss: 8.3008, norm: 2.1188, dt: 1132.53ms, tok/sec: 28933.42 tokens/sec\n",
      "2025-03-24 09:36:18.892459 - step 42, loss: 8.2798, norm: 1.8373, dt: 1107.66ms, tok/sec: 29583.16 tokens/sec\n",
      "2025-03-24 09:36:20.003150 - step 43, loss: 8.3961, norm: 2.0197, dt: 1108.36ms, tok/sec: 29564.30 tokens/sec\n",
      "2025-03-24 09:36:21.132531 - step 44, loss: 8.4572, norm: 2.1145, dt: 1127.51ms, tok/sec: 29062.29 tokens/sec\n",
      "2025-03-24 09:36:22.240972 - step 45, loss: 8.2844, norm: 2.2640, dt: 1106.43ms, tok/sec: 29615.86 tokens/sec\n",
      "2025-03-24 09:36:23.369221 - step 46, loss: 8.3013, norm: 1.9350, dt: 1124.98ms, tok/sec: 29127.70 tokens/sec\n",
      "2025-03-24 09:36:24.506739 - step 47, loss: 8.5618, norm: 1.9591, dt: 1136.97ms, tok/sec: 28820.46 tokens/sec\n",
      "2025-03-24 09:36:25.623339 - step 48, loss: 8.3824, norm: 1.6385, dt: 1115.81ms, tok/sec: 29367.00 tokens/sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.autocast(device_type=device, dtype=torch.float16):\n\u001B[32m     17\u001B[39m     logits, loss = model(x, y)\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m \u001B[43mloss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# clip grads to a max norm\u001B[39;00m\n\u001B[32m     21\u001B[39m norm = torch.nn.utils.clip_grad_norm_(model.parameters(), \u001B[32m1.0\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 8 - gpt + tiktoken/.venv/lib/python3.13/site-packages/torch/_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 8 - gpt + tiktoken/.venv/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/alpine/project-a-week/week 8 - gpt + tiktoken/.venv/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# -------------------------------\n",
    "# Generate\n",
    "# -------------------------------\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "# encode prefix tokens\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)  # (8 tokens,)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)  # (5 rows, 8 tokens)\n",
    "x = tokens.to(device)\n",
    "\n",
    "# generate! right now x is (B, T) where B = 5, T = 8\n",
    "model.eval()\n",
    "while x.size(1) < max_length:\n",
    "    # forward the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)[0]  # (B, T, vocab_size)\n",
    "        # take the logits at the last position\n",
    "        logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "        # get the probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # do top-k sampling of 50 (huggingface pipeline default)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)  # (B, 50) & (B, 50)\n",
    "        # select a token from the top-k probabilities\n",
    "        ix = torch.multinomial(topk_probs, num_samples=1)  # (B, 1)\n",
    "        # gather the corresponding indices\n",
    "        xcol = torch.gather(topk_indices, dim=-1, index=ix)  # (B, 1)\n",
    "        # append to the sequence\n",
    "        x = torch.cat((x, xcol), dim=1)\n",
    "\n",
    "# print the generated text\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ],
   "id": "816b802faa9c54d7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
