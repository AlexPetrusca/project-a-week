{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T20:40:58.014442Z",
     "start_time": "2025-03-21T20:40:57.922758Z"
    }
   },
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "import tiktoken\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.nn.losses as F\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_heads = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.causal_mask = CausalSelfAttention.create_additive_causal_mask(config.block_size, dtype=config.dtype)\n",
    "\n",
    "        self.query_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.key_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.value_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # calculate query, key, value for all heads\n",
    "        q = self.query_proj(x) # (B, T, C) -> (B, T, C)\n",
    "        k = self.key_proj(x) # (B, T, C) -> (B, T, C)\n",
    "        v = self.value_proj(x) # (B, T, C) -> (B, T, C)\n",
    "\n",
    "        # reshape query, key, value to batch over n_batches x n_heads\n",
    "        #   - this way we can compute attention for all heads at once (i.e. multi-head attention) with a single matrix multiply\n",
    "        #   - nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        q = mx.unflatten(q, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        k = mx.unflatten(k, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        v = mx.unflatten(v, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # causal flash attention\n",
    "        scale = math.sqrt(1 / q.shape[-1])\n",
    "        output = mx.fast.scaled_dot_product_attention(q, k, v, scale=scale, mask=self.causal_mask[:T, :T]) # 3x(B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # re-assemble all head outputs side by side and project out\n",
    "        output = output.transpose(0, 2, 1, 3).flatten(-2, -1) # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C)\n",
    "        return self.out_proj(output) # (B, T, C) -> (B, T, C)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_additive_causal_mask(N: int, dtype = mx.float32):\n",
    "        indices = mx.arange(N)\n",
    "        mask = indices[:, None] < indices[None]\n",
    "        mask = mask.astype(dtype) * mx.finfo(dtype).min\n",
    "        return mask\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 byte tokes + 1<|endoftext|>\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    dtype = mx.bfloat16\n",
    "    # NOTE: head_size = n_embd / n_head = 64  # embedding dimension of each attention head\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = [Block(config) for _ in range(config.n_layer)],\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # # weight sharing scheme (refer to [1] in play.ipynb)\n",
    "        # self.transformer['wte'].weight = self.lm_head.weight\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = mx.arange(0, T, dtype=mx.int32)  # shape (T)\n",
    "        pos_emb = self.transformer['wpe'](pos)  # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer['wte'](idx)  # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embd) + (T, n_embd) -> (B, T, n_embd)\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        return self.lm_head(x)  # (B, T, vocab_size)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:40:51.883549Z",
     "start_time": "2025-03-21T20:40:51.878056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename)\n",
    "    npt = npt.astype(np.int32) # added after video\n",
    "    ptt = mx.array(npt, dtype=mx.int32)\n",
    "    return ptt\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T, split):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {'train', 'val'}\n",
    "\n",
    "        # get the shard filenames\n",
    "        data_root = \"edu_fineweb10B\"\n",
    "        shards = os.listdir(data_root)\n",
    "        shards = [s for s in shards if split in s]\n",
    "        shards = sorted(shards)\n",
    "        shards = [os.path.join(data_root, s) for s in shards]\n",
    "        self.shards = shards\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        print(f\"found {len(shards)} shards for split {split}\")\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # state, init at shard zero\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).reshape(B, T) # inputs\n",
    "        y = (buf[1:]).reshape(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ],
   "id": "51bb1763af4526e8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:41:00.519493Z",
     "start_time": "2025-03-21T20:41:00.338756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpt_config = GPTConfig()\n",
    "n_batch = 16 if gpt_config.dtype == mx.bfloat16 else 8\n",
    "print(gpt_config.dtype)\n",
    "print(f\"batches of shape: ({n_batch}, {gpt_config.block_size})\")\n",
    "train_loader = DataLoaderLite(n_batch, gpt_config.block_size, split='train')\n",
    "\n",
    "model = GPT(gpt_config)\n",
    "model.set_dtype(gpt_config.dtype)\n",
    "mx.eval(model.parameters())\n",
    "nparams = sum(x.size for k, x in tree_flatten(model.parameters()) if \"embedding\" not in k)\n",
    "print(f\"Training a transformer with {nparams / 1024**2:.3f} M parameters\")\n",
    "\n",
    "optimizer = optim.AdamW(learning_rate=3e-5, weight_decay=0.1)"
   ],
   "id": "f7b8c6860043bbc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlx.core.bfloat16\n",
      "batches of shape: (16, 1024)\n",
      "found 99 shards for split train\n",
      "Training a transformer with 167.484 M parameters\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:55:49.817192Z",
     "start_time": "2025-03-21T20:55:49.814588Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = optim.AdamW(learning_rate=0.001)",
   "id": "74e6aaf4458f7269",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T21:05:50.055645Z",
     "start_time": "2025-03-21T21:05:48.105905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_fn(model, x, y, reduce=True):\n",
    "    logits = model(x)\n",
    "    losses = F.cross_entropy(logits, y)\n",
    "    return mx.mean(losses) if reduce else mx.mean(losses, axis=(-1, -2))\n",
    "\n",
    "state = [model.state, optimizer.state]\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def step(inputs, targets):\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "    loss, grads = loss_and_grad_fn(model, inputs, targets)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "start = datetime.now()\n",
    "num_epochs = 2000\n",
    "for i in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "\n",
    "    loss = step(x, y)\n",
    "    mx.eval(state)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time difference in milliseconds\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    iterations_per_sec = 1 / (t1 - t0)\n",
    "    print(f\"{datetime.now()} - step {i}, loss: {loss:.4f}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"total time: {end - start}\")\n",
    "print(f\"average tokens/sec: {(train_loader.B * train_loader.T * num_epochs) / (end.timestamp() - start.timestamp())}\")"
   ],
   "id": "499931aecf3dfa9d",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     18\u001B[39m x, y = train_loader.next_batch()\n\u001B[32m     20\u001B[39m loss = step(x, y)\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m \u001B[43mmx\u001B[49m\u001B[43m.\u001B[49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m t1 = time.time()\n\u001B[32m     24\u001B[39m dt = (t1 - t0) * \u001B[32m1000\u001B[39m  \u001B[38;5;66;03m# time difference in milliseconds\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T20:39:06.719249Z",
     "start_time": "2025-03-21T20:39:06.469691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "# encode prefix tokens\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "# tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = mx.array(tokens, dtype=mx.int32)  # (8 tokens,)\n",
    "x = mx.repeat(mx.expand_dims(tokens, axis=0), num_return_sequences, axis=0)  # (5 rows, 8 tokens)\n",
    "\n",
    "# generate! right now x is (B, T) where B = 5, T = 8\n",
    "while x.shape[1] < max_length:\n",
    "    # forward the model to get the logits\n",
    "    logits = model(x)  # (B, T, vocab_size)\n",
    "    # take the logits at the last position\n",
    "    logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "    # get the top k probabilities\n",
    "    k = 50\n",
    "    topk_indices = mx.argsort(logits, axis=-1)[:, -k:]\n",
    "    topk_logits = mx.sort(logits, axis=-1)[:, -k:]\n",
    "\n",
    "    # select a token from the top probabilities\n",
    "    ix = mx.random.categorical(topk_logits, num_samples=1)  # (B, 1)\n",
    "    xcat = mx.take_along_axis(topk_indices, indices=ix, axis=-1)\n",
    "\n",
    "    # DEBUG\n",
    "    # print('-------')\n",
    "    # print(ix)\n",
    "    # print(xcat)\n",
    "\n",
    "    # append to the sequence\n",
    "    x = mx.concatenate([x, xcat], axis=1)\n",
    "\n",
    "print(x.tolist())\n",
    "# print the generated text\n",
    "for i in range(x.shape[0]):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ],
   "id": "2611912863560ec2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15496, 11, 314, 1101, 257, 3303, 2746, 11, 326, 460, 307, 284, 477, 465, 640, 326, 484, 318, 262, 1660, 13, 357, 42, 14, 20, 13, 198, 32, 262, 749], [15496, 11, 314, 1101, 257, 3303, 2746, 11, 373, 517, 393, 530, 11, 262, 4387, 262, 976, 2568, 287, 262, 1499, 828, 198, 12, 3106, 11, 345, 423, 2077, 326], [15496, 11, 314, 1101, 257, 3303, 2746, 11, 257, 1295, 286, 262, 1218, 636, 13, 314, 779, 13, 383, 749, 8811, 1043, 416, 262, 1660, 284, 262, 2351, 7712, 290], [15496, 11, 314, 1101, 257, 3303, 2746, 11, 428, 355, 543, 290, 262, 1181, 11, 543, 286, 597, 262, 366, 1169, 360, 261, 13, 554, 262, 734, 11, 290, 262], [15496, 11, 314, 1101, 257, 3303, 2746, 11, 11, 13, 198, 1532, 530, 737, 770, 318, 340, 460, 307, 262, 1989, 13, 198, 464, 1573, 13, 198, 12, 1941, 1660]]\n",
      "> Hello, I'm a language model, that can be to all his time that they is the water. (K/5.\n",
      "A the most\n",
      "> Hello, I'm a language model, was more or one, the largest the same energy in the country),\n",
      "-based, you have taken that\n",
      "> Hello, I'm a language model, a place of the second part. I use. The most commonly found by the water to the National Development and\n",
      "> Hello, I'm a language model, this as which and the state, which of any the \"the Don. In the two, and the\n",
      "> Hello, I'm a language model,,.\n",
      "If one). This is it can be the area.\n",
      "The word.\n",
      "-year water\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Debugging\n",
    "# ----------------------------------------------------------------------------------"
   ],
   "id": "aa2de392b05f0284",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# encode prefix tokens\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Second Citizen:\")\n",
    "tokens = mx.array(tokens, dtype=mx.int32)\n",
    "x = mx.repeat(mx.expand_dims(tokens, axis=0), num_return_sequences, axis=0)\n",
    "print(x)"
   ],
   "id": "513837af65ba936f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logits = model(x)  # (B, T, vocab_size)\n",
    "print(logits[0, 2])\n",
    "logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "print(logits)\n",
    "probs = nn.softmax(logits, axis=-1)\n",
    "print(probs)"
   ],
   "id": "4de303e37bfecd0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(mx.sort(probs, axis=-1))",
   "id": "92876167f11c0ca6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k = 50  # Number of top elements\n",
    "# Get the sorted indices in descending order\n",
    "topk_indices = mx.argsort(probs, axis=-1)[:, -k:] # (B, 50)\n",
    "# Use the indices to gather the top K values\n",
    "topk_probs = mx.take_along_axis(probs, indices=topk_indices, axis=-1) # (B, 50)\n",
    "print(topk_indices)\n",
    "print(topk_probs)\n",
    "print(enc.decode(topk_indices[0].tolist()))"
   ],
   "id": "15d5e33a65057ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PROBLEM: as is, the categorical sampling is basically fucking random\n",
    "for i in range(10):\n",
    "    ix = mx.random.categorical(topk_probs, num_samples=1)\n",
    "    ix = ix[0]\n",
    "    print(ix.item())"
   ],
   "id": "6d5d21cd2cadaf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# The fix is to pass just the logits... we could just forget about the topk nonsense ()even though it seems to work\n",
    "print(topk_probs)\n",
    "for i in range(10):\n",
    "    ix = mx.random.categorical(logits, num_samples=1)\n",
    "    ix = ix[0]\n",
    "    print(f\"{ix.item()} {enc.decode([ix.item()])} ({probs[0][ix.item()].item()})\")\n",
    "# much better!"
   ],
   "id": "44683146bb864473",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topk_indices = mx.argsort(logits, axis=-1)[:, -50:]\n",
    "topk_logits = mx.sort(logits, axis=-1)[:, -50:]\n",
    "print(topk_indices)\n",
    "print(topk_logits)"
   ],
   "id": "1eae0beb88a07a62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ix = mx.random.categorical(topk_logits, num_samples=1)\n",
    "print(ix)"
   ],
   "id": "6703ba9cab8448aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "xcat = mx.take_along_axis(topk_indices, indices=ix, axis=-1)\n",
    "print(xcat)"
   ],
   "id": "2994d29696ba0821",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Full run\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Second Citizen:\\n\")\n",
    "tokens = mx.array(tokens, dtype=mx.int32)\n",
    "x = mx.expand_dims(tokens, axis=0)\n",
    "for i in range(100):\n",
    "    logits = model(x)  # (B, T, vocab_size)\n",
    "    logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "    probs = nn.softmax(logits, axis=-1)\n",
    "    ix = mx.random.categorical(2 * logits, num_samples=1)  # (B, 1)\n",
    "    print(enc.decode([ix.item()]), \"->\", probs[0, ix.item()].item())\n",
    "    x = mx.concatenate([x, ix], axis=1)\n",
    "\n",
    "print(x.tolist())\n",
    "print(enc.decode(x.tolist()[0]))"
   ],
   "id": "cd746a6a11f880ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
