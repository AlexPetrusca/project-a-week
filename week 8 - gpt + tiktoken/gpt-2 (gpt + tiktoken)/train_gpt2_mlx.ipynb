{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-21T03:21:38.330719Z",
     "start_time": "2025-03-21T03:21:38.316779Z"
    }
   },
   "source": [
    "import math\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "\n",
    "import tiktoken\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.nn.losses as F\n",
    "import mlx.optimizers as optim\n",
    "from mlx.utils import tree_flatten\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "\n",
    "        self.n_heads = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.causal_mask = CausalSelfAttention.create_additive_causal_mask(config.block_size, dtype=config.dtype)\n",
    "\n",
    "        self.query_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.key_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.value_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "        self.out_proj = nn.Linear(self.n_embd, self.n_embd)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # calculate query, key, value for all heads\n",
    "        q = self.query_proj(x) # (B, T, C) -> (B, T, C)\n",
    "        k = self.key_proj(x) # (B, T, C) -> (B, T, C)\n",
    "        v = self.value_proj(x) # (B, T, C) -> (B, T, C)\n",
    "\n",
    "        # reshape query, key, value to batch over n_batches x n_heads\n",
    "        #   - this way we can compute attention for all heads at once (i.e. multi-head attention) with a single matrix multiply\n",
    "        #   - nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        q = mx.unflatten(q, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        k = mx.unflatten(k, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "        v = mx.unflatten(v, -1, (self.n_heads, -1)).transpose(0, 2, 1, 3) # (B, T, C) -> (B, T, nh, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # causal flash attention\n",
    "        scale = math.sqrt(1 / q.shape[-1])\n",
    "        output = mx.fast.scaled_dot_product_attention(q, k, v, scale=scale, mask=self.causal_mask[:T, :T]) # 3x(B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        # re-assemble all head outputs side by side and project out\n",
    "        output = output.transpose(0, 2, 1, 3).flatten(-2, -1) # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, C)\n",
    "        return self.out_proj(output) # (B, T, C) -> (B, T, C)\n",
    "\n",
    "    @staticmethod\n",
    "    def create_additive_causal_mask(N: int, dtype = mx.float32):\n",
    "        indices = mx.arange(N)\n",
    "        mask = indices[:, None] < indices[None]\n",
    "        mask = mask.astype(dtype) * mx.finfo(dtype).min\n",
    "        return mask\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 byte tokes + 1<|endoftext|>\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "    dtype = mx.bfloat16\n",
    "    # NOTE: head_size = n_embd / n_head = 64  # embedding dimension of each attention head\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = [Block(config) for _ in range(config.n_layer)],\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        )\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme (refer to [1] in play.ipynb)\n",
    "        self.transformer['wte'].weight = self.lm_head.weight\n",
    "\n",
    "    def __call__(self, idx):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.shape\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and position embeddings\n",
    "        pos = mx.arange(0, T, dtype=mx.int32)  # shape (T)\n",
    "        pos_emb = self.transformer['wpe'](pos)  # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer['wte'](idx)  # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embd) + (T, n_embd) -> (B, T, n_embd)\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer['h']:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer['ln_f'](x)\n",
    "        return self.lm_head(x)  # (B, T, vocab_size)"
   ],
   "outputs": [],
   "execution_count": 255
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T23:22:45.944967Z",
     "start_time": "2025-03-20T23:22:45.941536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, path, batch_shape):\n",
    "        self.B = batch_shape[0]\n",
    "        self.T = batch_shape[1]\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open(path, 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = mx.array(tokens)\n",
    "        print(f\"loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"1 epoch = {len(self.tokens) // (self.B * self.T)} batches\")\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).reshape((B, T)) # inputs\n",
    "        y = (buf[1:]).reshape((B, T)) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ],
   "id": "51bb1763af4526e8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T23:22:58.449812Z",
     "start_time": "2025-03-20T23:22:58.108389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_batch = 10\n",
    "\n",
    "gpt_config = GPTConfig()\n",
    "train_loader = DataLoaderLite('res/tinyshakespeare.txt', (16, gpt_config.block_size))\n",
    "\n",
    "model = GPT(gpt_config)\n",
    "model.set_dtype(gpt_config.dtype)\n",
    "mx.eval(model.parameters())\n",
    "nparams = sum(x.size for k, x in tree_flatten(model.parameters()) if \"embedding\" not in k)\n",
    "print(f\"Training a transformer with {nparams / 1024**2:.3f} M parameters\")\n",
    "\n",
    "optimizer = optim.AdamW(learning_rate=3e-4, betas=[0.9, 0.95], eps=1e-8, weight_decay=0.1)"
   ],
   "id": "f7b8c6860043bbc0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 338025 tokens\n",
      "1 epoch = 20 batches\n",
      "Training a transformer with 167.484 M parameters\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:23:10.306667Z",
     "start_time": "2025-03-21T03:21:50.243621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_fn(model, x, y, reduce=True):\n",
    "    logits = model(x)\n",
    "    losses = F.cross_entropy(logits, y)\n",
    "    return mx.mean(losses) if reduce else mx.mean(losses, axis=(-1, -2))\n",
    "\n",
    "state = [model.state, optimizer.state]\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def step(inputs, targets):\n",
    "    loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "    loss, grads = loss_and_grad_fn(model, inputs, targets)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "start = datetime.now()\n",
    "num_epochs = 1000\n",
    "for i in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    x, y = train_loader.next_batch()\n",
    "\n",
    "    loss = step(x, y)\n",
    "    mx.eval(state)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = (t1 - t0) * 1000  # time difference in milliseconds\n",
    "    tokens_per_sec = (train_loader.B * train_loader.T) / (t1 - t0)\n",
    "    iterations_per_sec = 1 / (t1 - t0)\n",
    "    print(f\"{datetime.now()} - step {i}, loss: {loss:.4f}, dt: {dt:.2f}ms, tok/sec: {tokens_per_sec:.2f} tokens/sec\")\n",
    "\n",
    "end = datetime.now()\n",
    "print(f\"total time: {end - start}\")\n",
    "print(f\"average tokens/sec: {(train_loader.B * train_loader.T * num_epochs) / (end.timestamp() - start.timestamp())}\")"
   ],
   "id": "499931aecf3dfa9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-20 20:21:55.071798 - step 0, loss: 0.3066, dt: 4822.96ms, tok/sec: 3397.08 tokens/sec\n",
      "2025-03-20 20:21:56.897179 - step 1, loss: 0.2832, dt: 1781.57ms, tok/sec: 9196.38 tokens/sec\n",
      "2025-03-20 20:21:58.716430 - step 2, loss: 0.3809, dt: 1782.89ms, tok/sec: 9189.56 tokens/sec\n",
      "2025-03-20 20:22:00.542471 - step 3, loss: 0.3809, dt: 1789.15ms, tok/sec: 9157.40 tokens/sec\n",
      "2025-03-20 20:22:02.360690 - step 4, loss: 0.3262, dt: 1777.10ms, tok/sec: 9219.50 tokens/sec\n",
      "2025-03-20 20:22:04.139384 - step 5, loss: 0.3379, dt: 1778.26ms, tok/sec: 9213.50 tokens/sec\n",
      "2025-03-20 20:22:05.919853 - step 6, loss: 0.3047, dt: 1780.05ms, tok/sec: 9204.24 tokens/sec\n",
      "2025-03-20 20:22:07.697025 - step 7, loss: 0.3555, dt: 1776.77ms, tok/sec: 9221.21 tokens/sec\n",
      "2025-03-20 20:22:09.474194 - step 8, loss: 0.3379, dt: 1776.74ms, tok/sec: 9221.41 tokens/sec\n",
      "2025-03-20 20:22:11.266750 - step 9, loss: 0.3027, dt: 1792.14ms, tok/sec: 9142.13 tokens/sec\n",
      "2025-03-20 20:22:13.075549 - step 10, loss: 0.2988, dt: 1778.50ms, tok/sec: 9212.25 tokens/sec\n",
      "2025-03-20 20:22:14.888989 - step 11, loss: 0.3184, dt: 1785.98ms, tok/sec: 9173.69 tokens/sec\n",
      "2025-03-20 20:22:16.705210 - step 12, loss: 0.3730, dt: 1788.60ms, tok/sec: 9160.22 tokens/sec\n",
      "2025-03-20 20:22:18.533704 - step 13, loss: 0.3281, dt: 1786.53ms, tok/sec: 9170.86 tokens/sec\n",
      "2025-03-20 20:22:20.302849 - step 14, loss: 0.2969, dt: 1768.74ms, tok/sec: 9263.08 tokens/sec\n",
      "2025-03-20 20:22:22.117400 - step 15, loss: 0.3125, dt: 1775.30ms, tok/sec: 9228.86 tokens/sec\n",
      "2025-03-20 20:22:23.902494 - step 16, loss: 0.2969, dt: 1784.68ms, tok/sec: 9180.37 tokens/sec\n",
      "2025-03-20 20:22:25.683245 - step 17, loss: 0.3047, dt: 1780.31ms, tok/sec: 9202.88 tokens/sec\n",
      "2025-03-20 20:22:27.460544 - step 18, loss: 0.2852, dt: 1776.90ms, tok/sec: 9220.53 tokens/sec\n",
      "2025-03-20 20:22:29.266839 - step 19, loss: 0.3203, dt: 1805.86ms, tok/sec: 9072.69 tokens/sec\n",
      "2025-03-20 20:22:31.106848 - step 20, loss: 0.3438, dt: 1817.16ms, tok/sec: 9016.27 tokens/sec\n",
      "2025-03-20 20:22:32.945200 - step 21, loss: 0.2832, dt: 1801.95ms, tok/sec: 9092.35 tokens/sec\n",
      "2025-03-20 20:22:34.758934 - step 22, loss: 0.3418, dt: 1776.35ms, tok/sec: 9223.39 tokens/sec\n",
      "2025-03-20 20:22:36.533995 - step 23, loss: 0.3730, dt: 1774.65ms, tok/sec: 9232.25 tokens/sec\n",
      "2025-03-20 20:22:38.310736 - step 24, loss: 0.3516, dt: 1776.30ms, tok/sec: 9223.64 tokens/sec\n",
      "2025-03-20 20:22:40.087236 - step 25, loss: 0.3320, dt: 1776.04ms, tok/sec: 9225.04 tokens/sec\n",
      "2025-03-20 20:22:41.852512 - step 26, loss: 0.3047, dt: 1764.83ms, tok/sec: 9283.62 tokens/sec\n",
      "2025-03-20 20:22:43.617831 - step 27, loss: 0.3535, dt: 1764.92ms, tok/sec: 9283.13 tokens/sec\n",
      "2025-03-20 20:22:45.378961 - step 28, loss: 0.3398, dt: 1760.64ms, tok/sec: 9305.69 tokens/sec\n",
      "2025-03-20 20:22:47.148151 - step 29, loss: 0.3340, dt: 1768.76ms, tok/sec: 9262.96 tokens/sec\n",
      "2025-03-20 20:22:48.933561 - step 30, loss: 0.3281, dt: 1784.99ms, tok/sec: 9178.77 tokens/sec\n",
      "2025-03-20 20:22:50.736602 - step 31, loss: 0.3496, dt: 1802.57ms, tok/sec: 9089.22 tokens/sec\n",
      "2025-03-20 20:22:52.516374 - step 32, loss: 0.3770, dt: 1779.33ms, tok/sec: 9207.99 tokens/sec\n",
      "2025-03-20 20:22:54.290907 - step 33, loss: 0.3477, dt: 1774.11ms, tok/sec: 9235.07 tokens/sec\n",
      "2025-03-20 20:22:56.064991 - step 34, loss: 0.3262, dt: 1773.61ms, tok/sec: 9237.67 tokens/sec\n",
      "2025-03-20 20:22:57.838334 - step 35, loss: 0.3555, dt: 1772.93ms, tok/sec: 9241.21 tokens/sec\n",
      "2025-03-20 20:22:59.621276 - step 36, loss: 0.3086, dt: 1782.45ms, tok/sec: 9191.83 tokens/sec\n",
      "2025-03-20 20:23:01.395298 - step 37, loss: 0.3281, dt: 1773.58ms, tok/sec: 9237.84 tokens/sec\n",
      "2025-03-20 20:23:03.171313 - step 38, loss: 0.3184, dt: 1775.58ms, tok/sec: 9227.40 tokens/sec\n",
      "2025-03-20 20:23:04.947235 - step 39, loss: 0.3789, dt: 1775.50ms, tok/sec: 9227.84 tokens/sec\n",
      "2025-03-20 20:23:06.725665 - step 40, loss: 0.3770, dt: 1778.00ms, tok/sec: 9214.86 tokens/sec\n",
      "2025-03-20 20:23:08.501971 - step 41, loss: 0.3262, dt: 1775.87ms, tok/sec: 9225.91 tokens/sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[256]\u001B[39m\u001B[32m, line 21\u001B[39m\n\u001B[32m     18\u001B[39m x, y = train_loader.next_batch()\n\u001B[32m     20\u001B[39m loss = step(x, y)\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m \u001B[43mmx\u001B[49m\u001B[43m.\u001B[49m\u001B[43meval\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     23\u001B[39m t1 = time.time()\n\u001B[32m     24\u001B[39m dt = (t1 - t0) * \u001B[32m1000\u001B[39m  \u001B[38;5;66;03m# time difference in milliseconds\u001B[39;00m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 256
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:23:12.565149Z",
     "start_time": "2025-03-21T03:23:12.298751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "# encode prefix tokens\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "# tokens = enc.encode(\"Hello, I'm a language model,\")\n",
    "tokens = enc.encode(\"Second Citizen:\")\n",
    "tokens = mx.array(tokens, dtype=mx.int32)  # (8 tokens,)\n",
    "x = mx.repeat(mx.expand_dims(tokens, axis=0), num_return_sequences, axis=0)  # (5 rows, 8 tokens)\n",
    "\n",
    "# generate! right now x is (B, T) where B = 5, T = 8\n",
    "while x.shape[1] < max_length:\n",
    "    # forward the model to get the logits\n",
    "    logits = model(x)  # (B, T, vocab_size)\n",
    "    # take the logits at the last position\n",
    "    logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "\n",
    "    # # get the probabilities\n",
    "    # probs = nn.softmax(logits, axis=-1)\n",
    "    # # do top-k sampling of 50 (huggingface pipeline default)\n",
    "    # k = 50  # Number of top elements\n",
    "    # # Get the sorted indices in descending order\n",
    "    # topk_indices = mx.argsort(probs, axis=-1)[:, -k:] # (B, 50)\n",
    "    # # Use the indices to gather the top K values\n",
    "    # topk_probs = mx.take_along_axis(probs, indices=topk_indices, axis=-1) # (B, 50)\n",
    "\n",
    "    # select a token from the top probabilities\n",
    "    ix = mx.random.categorical(logits, num_samples=1)  # (B, 1)\n",
    "    # append to the sequence\n",
    "    x = mx.concatenate([x, ix], axis=1)\n",
    "\n",
    "# print the generated text\n",
    "for i in range(x.shape[0]):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\", decoded)"
   ],
   "id": "2611912863560ec2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Second Citizen:\n",
      "Here anchors! the hair\n",
      "Exceptity and we receive\n",
      "All: that! my hand not true twenty times from my knees for\n",
      "> Second Citizen:\n",
      "Before we calls me! another! Was of theOUGH,,\n",
      "under traitor, hear not obedient be Men them:\n",
      "words\n",
      "> Second Citizen:\n",
      "They did us down before but theps.\n",
      "\n",
      "Third Gentleman: he hath't? he was another floods?\n",
      "That!--\n",
      "> Second Citizen:\n",
      "Before we proceed yourlander to live, my power till\n",
      "And I desperately we must quite forth hat or from his ruin and my\n",
      "> Second Citizen:\n",
      "Before I meant, sir; ye could a\n",
      "The strokes, which pilgr' service'st, few Rome, to are faults\n"
     ]
    }
   ],
   "execution_count": 257
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# Debugging\n",
    "# ----------------------------------------------------------------------------------"
   ],
   "id": "aa2de392b05f0284"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:19:54.485817Z",
     "start_time": "2025-03-21T03:19:54.482939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# encode prefix tokens\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Second Citizen:\\n\")\n",
    "tokens = mx.array(tokens, dtype=mx.int32)\n",
    "x = mx.expand_dims(tokens, axis=0)\n",
    "print(x)"
   ],
   "id": "513837af65ba936f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[12211, 22307, 25, 198]], dtype=int32)\n"
     ]
    }
   ],
   "execution_count": 246
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:19:55.945278Z",
     "start_time": "2025-03-21T03:19:55.656759Z"
    }
   },
   "cell_type": "code",
   "source": [
    "logits = model(x)  # (B, T, vocab_size)\n",
    "print(logits[0, 2])\n",
    "logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "print(logits)\n",
    "probs = nn.softmax(logits, axis=-1)\n",
    "print(probs)"
   ],
   "id": "4de303e37bfecd0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([2.4375, -16.75, -16.75, ..., -16.875, -5.90625, -16.75], dtype=bfloat16)\n",
      "array([[1.58594, -6.09375, -6.34375, ..., -6.25, -1.85938, -6.28125]], dtype=bfloat16)\n",
      "array([[9.53674e-05, 4.23752e-08, 3.28291e-08, ..., 3.74857e-08, 2.96533e-06, 3.74857e-08]], dtype=bfloat16)\n"
     ]
    }
   ],
   "execution_count": 247
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:19:56.960640Z",
     "start_time": "2025-03-21T03:19:56.957030Z"
    }
   },
   "cell_type": "code",
   "source": "print(mx.sort(probs, axis=-1))",
   "id": "92876167f11c0ca6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[2.1304e-08, 2.2701e-08, 2.2701e-08, ..., 0.0654297, 0.12207, 0.177734]], dtype=bfloat16)\n"
     ]
    }
   ],
   "execution_count": 248
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:19:58.172027Z",
     "start_time": "2025-03-21T03:19:58.156827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "k = 50  # Number of top elements\n",
    "# Get the sorted indices in descending order\n",
    "topk_indices = mx.argsort(probs, axis=-1)[:, -k:] # (B, 50)\n",
    "# Use the indices to gather the top K values\n",
    "topk_probs = mx.take_along_axis(probs, indices=topk_indices, axis=-1) # (B, 50)\n",
    "print(topk_indices)\n",
    "print(topk_probs)\n",
    "print(enc.decode(topk_indices[0].tolist()))\n",
    "print(\"\\\\n =\", enc.encode(\"\\n\"))"
   ],
   "id": "15d5e33a65057ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[22788, 11486, 17821, ..., 817, 1135, 8421]], dtype=uint32)\n",
      "array([[0.00174713, 0.00180054, 0.00180054, ..., 0.0654297, 0.12207, 0.177734]], dtype=bfloat16)\n",
      "SirYetTrueAyClGRThirdyourSecondMyThatKINGItPleaseHowFSVIfAnTheyToHereMoreWhereThetheSoY\n",
      "QMadHDGoodWellNPMENButFirstAndWhyNoYouWhatIThWeBefore\n",
      "\\n = [198]\n"
     ]
    }
   ],
   "execution_count": 249
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:19:59.413140Z",
     "start_time": "2025-03-21T03:19:59.393424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PROBLEM: as is, the categorical sampling is basically fucking random\n",
    "for i in range(10):\n",
    "    ix = mx.random.categorical(topk_probs, num_samples=1)\n",
    "    print(ix.item())"
   ],
   "id": "6d5d21cd2cadaf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "20\n",
      "37\n",
      "47\n",
      "4\n",
      "44\n",
      "44\n",
      "45\n",
      "34\n",
      "41\n"
     ]
    }
   ],
   "execution_count": 250
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:20:41.387002Z",
     "start_time": "2025-03-21T03:20:41.359141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The fix is to pass just the logits... we could just forget about the topk nonsense ()even though it seems to work\n",
    "print(topk_probs)\n",
    "for i in range(10):\n",
    "    ix = mx.random.categorical(logits, num_samples=1)\n",
    "    print(ix.item(), enc.decode([ix.item()]), probs[0][ix.item()].item())\n",
    "# much better!"
   ],
   "id": "44683146bb864473",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0.00174713, 0.00180054, 0.00180054, ..., 0.0654297, 0.12207, 0.177734]], dtype=bfloat16)\n",
      "2514 To 0.004730224609375\n",
      "5195 Why 0.0247802734375\n",
      "49370 Mist 0.0010223388671875\n",
      "20840  surveyed 3.748573362827301e-08\n",
      "8421 Before 0.177734375\n",
      "534  your 0.0003871917724609375\n",
      "1537 But 0.0205078125\n",
      "4053 well 0.000705718994140625\n",
      "1135 We 0.1220703125\n",
      "8421 Before 0.177734375\n"
     ]
    }
   ],
   "execution_count": 253
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:02:11.951048Z",
     "start_time": "2025-03-21T03:02:11.948038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ix = mx.random.categorical(logits, num_samples=1)\n",
    "x = mx.concatenate([x, ix], axis=1)\n",
    "print(x)"
   ],
   "id": "6703ba9cab8448aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[12211, 22307, 25, ..., 198, 198, 198]], dtype=int64)\n"
     ]
    }
   ],
   "execution_count": 191
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T03:21:10.574260Z",
     "start_time": "2025-03-21T03:21:09.540372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Full run\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"Second Citizen:\\n\")\n",
    "tokens = mx.array(tokens, dtype=mx.int32)\n",
    "x = mx.expand_dims(tokens, axis=0)\n",
    "for i in range(100):\n",
    "    logits = model(x)  # (B, T, vocab_size)\n",
    "    logits = logits[:, -1, :]  # (B, vocab_size)\n",
    "    probs = nn.softmax(logits, axis=-1)\n",
    "    ix = mx.random.categorical(2 * logits, num_samples=1)  # (B, 1)\n",
    "    print(enc.decode([ix.item()]), \"->\", probs[0, ix.item()].item())\n",
    "    x = mx.concatenate([x, ix], axis=1)\n",
    "\n",
    "print(x.tolist())\n",
    "print(enc.decode(x.tolist()[0]))"
   ],
   "id": "cd746a6a11f880ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We -> 0.1220703125\n",
      " hear -> 0.080078125\n",
      " me -> 1.0\n",
      ", -> 0.92578125\n",
      " not -> 0.296875\n",
      ", -> 0.111328125\n",
      " see -> 0.5234375\n",
      " your -> 0.1962890625\n",
      "woman -> 0.013427734375\n",
      " do -> 0.5859375\n",
      "; -> 0.1640625\n",
      "\n",
      " -> 1.0\n",
      "The -> 0.203125\n",
      " service -> 0.05419921875\n",
      ", -> 0.87109375\n",
      "' -> 0.1494140625\n",
      "er -> 0.93359375\n",
      "\n",
      " -> 0.5078125\n",
      "I -> 0.390625\n",
      "'ll -> 0.671875\n",
      " prove -> 0.0830078125\n",
      " his -> 0.9921875\n",
      " own -> 0.12890625\n",
      " heart -> 0.28125\n",
      ", -> 1.0\n",
      "' -> 0.93359375\n",
      "er -> 0.76171875\n",
      "\n",
      " -> 0.984375\n",
      "Like -> 0.1796875\n",
      " worth -> 0.0018157958984375\n",
      " all -> 0.265625\n",
      " are -> 0.9921875\n",
      " not -> 0.734375\n",
      " in -> 0.119140625\n",
      "'t -> 0.1953125\n",
      "\n",
      " -> 0.36328125\n",
      "Even -> 0.48046875\n",
      " in -> 1.0\n",
      " your -> 0.9921875\n",
      " times -> 0.212890625\n",
      " i -> 0.60546875\n",
      "' -> 1.0\n",
      "er -> 0.90625\n",
      " I -> 0.69140625\n",
      " have -> 0.671875\n",
      " left -> 0.197265625\n",
      "; -> 1.0\n",
      "\n",
      " -> 1.0\n",
      "So -> 0.21875\n",
      " locks -> 0.0400390625\n",
      " o -> 0.53515625\n",
      "' -> 0.984375\n",
      " the -> 0.9140625\n",
      " court -> 0.0966796875\n",
      ". -> 0.921875\n",
      "\n",
      " -> 0.9921875\n",
      "How -> 0.0322265625\n",
      " might -> 0.48046875\n",
      " have -> 0.031494140625\n",
      " touch -> 0.2119140625\n",
      " are -> 0.62109375\n",
      " heavy -> 0.0390625\n",
      " hands -> 0.255859375\n",
      "\n",
      " -> 1.0\n",
      "Like -> 0.29296875\n",
      " right -> 0.0291748046875\n",
      ",' -> 0.2255859375\n",
      "\n",
      " -> 1.0\n",
      "Like -> 0.4375\n",
      " k -> 0.033935546875\n",
      "ingly -> 0.6796875\n",
      " days -> 0.1162109375\n",
      " of -> 0.90625\n",
      " high -> 0.0255126953125\n",
      ", -> 0.72265625\n",
      " or -> 0.33984375\n",
      " seem -> 0.01422119140625\n",
      "'d -> 0.58203125\n",
      " against -> 0.11328125\n",
      "\n",
      " -> 0.310546875\n",
      "And -> 0.2197265625\n",
      " follow -> 0.2001953125\n",
      " like -> 0.431640625\n",
      " your -> 0.314453125\n",
      " own -> 0.039306640625\n",
      " place -> 0.00909423828125\n",
      ". -> 0.61328125\n",
      "\n",
      " -> 1.0\n",
      "\n",
      " -> 1.0\n",
      "The -> 0.058837890625\n",
      " King -> 0.054931640625\n",
      " Henry -> 0.08349609375\n",
      ", -> 1.0\n",
      " sir -> 0.1826171875\n",
      ": -> 0.94140625\n",
      "\n",
      " -> 0.9921875\n",
      "I -> 1.0\n",
      "OL -> 0.8046875\n",
      "YC -> 0.72265625\n",
      "US -> 0.9765625\n",
      "[[12211, 22307, 25, 198, 1135, 3285, 502, 11, 407, 11, 766, 534, 8580, 466, 26, 198, 464, 2139, 11, 6, 263, 198, 40, 1183, 5879, 465, 898, 2612, 11, 6, 263, 198, 7594, 2861, 477, 389, 407, 287, 470, 198, 6104, 287, 534, 1661, 1312, 6, 263, 314, 423, 1364, 26, 198, 2396, 19253, 267, 6, 262, 2184, 13, 198, 2437, 1244, 423, 3638, 389, 4334, 2832, 198, 7594, 826, 4032, 198, 7594, 479, 4420, 1528, 286, 1029, 11, 393, 1283, 1549, 1028, 198, 1870, 1061, 588, 534, 898, 1295, 13, 198, 198, 464, 2677, 8616, 11, 15967, 25, 198, 40, 3535, 44816, 2937]]\n",
      "Second Citizen:\n",
      "We hear me, not, see yourwoman do;\n",
      "The service,'er\n",
      "I'll prove his own heart,'er\n",
      "Like worth all are not in't\n",
      "Even in your times i'er I have left;\n",
      "So locks o' the court.\n",
      "How might have touch are heavy hands\n",
      "Like right,'\n",
      "Like kingly days of high, or seem'd against\n",
      "And follow like your own place.\n",
      "\n",
      "The King Henry, sir:\n",
      "IOLYCUS\n"
     ]
    }
   ],
   "execution_count": 254
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
