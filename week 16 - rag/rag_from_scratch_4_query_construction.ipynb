{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:02:14.310934Z",
     "start_time": "2025-07-14T20:02:14.306757Z"
    }
   },
   "cell_type": "code",
   "source": "# Query construction is the process of parsing natural language questions and extracting structured filters that can be used in the query to a vector store, narrowing down the documents to search before similarity search even happens.",
   "id": "5946543cc3634208",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-14T20:02:14.417324Z",
     "start_time": "2025-07-14T20:02:14.411548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # load environment variables"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem Statement\n",
    "\n",
    "Vector search retrieves results only based on semantic similarity, ignoring structured metadata that might dramatically improve precision.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "Query: \"What did Steve Jobs say about innovation in 2007?\"\n",
    "\n",
    "A naive vector search may return general quotes from Jobs, or innovation-related paragraphs — from any year, any speaker.\n",
    "\n",
    "You could extract:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"speaker\": \"Steve Jobs\",\n",
    "  \"topic\": \"innovation\",\n",
    "  \"year\": 2007\n",
    "}\n",
    "```\n",
    "\n",
    "Then run the vector search only over chunks with metadata matching that filter, improving both relevance and precision."
   ],
   "id": "e63176e65de0dc2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Query Construction\n",
    "\n",
    "![Image ](rsc/jupyter/query_construction.png)\n",
    "\n",
    "**Idea**: Many vectorstores contain metadata fields. This makes it possible to filter for specific chunks based on metadata. We want to convert natural language into structured search queries that leverage these metadata fields."
   ],
   "id": "e851105287ffd487"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:02:14.652708Z",
     "start_time": "2025-07-14T20:02:14.425330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Let's look at some example metadata we might see in a database of YouTube transcripts.\n",
    "\n",
    "import requests\n",
    "\n",
    "def get_youtube_metadata(video_url: str) -> dict:\n",
    "    \"\"\"Fetch YouTube metadata using oEmbed.\"\"\"\n",
    "    oembed_endpoint = \"https://www.youtube.com/oembed\"\n",
    "    params = {\n",
    "        \"url\": video_url,\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(oembed_endpoint, params=params)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "        return response.json()\n",
    "    except requests.HTTPError as e:\n",
    "        print(f\"HTTP Error: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example usage\n",
    "video_url = \"https://www.youtube.com/watch?v=pbAd8O1Lvm4\"\n",
    "metadata = get_youtube_metadata(video_url)\n",
    "\n",
    "print(metadata)"
   ],
   "id": "ef5768bf7acba39d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Self-reflective RAG with LangGraph: Self-RAG and CRAG', 'author_name': 'LangChain', 'author_url': 'https://www.youtube.com/@LangChain', 'type': 'video', 'height': 150, 'width': 200, 'version': '1.0', 'provider_name': 'YouTube', 'provider_url': 'https://www.youtube.com/', 'thumbnail_height': 360, 'thumbnail_width': 480, 'thumbnail_url': 'https://i.ytimg.com/vi/pbAd8O1Lvm4/hqdefault.jpg', 'html': '<iframe width=\"200\" height=\"150\" src=\"https://www.youtube.com/embed/pbAd8O1Lvm4?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"Self-reflective RAG with LangGraph: Self-RAG and CRAG\"></iframe>'}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's say we have a database of transcripts of tutorial videos on YouTube. Each document in the database has a:\n",
    "- text transcript (content)\n",
    "- metadata about the video (title, length, view count, publish date)\n",
    "\n",
    "Let’s also say we’ve built an index over this database such that we can:\n",
    "1. Perform unstructured search over the contents and title of each document.\n",
    "2. Use range filtering on view count, publication date, and length.\n",
    "\n",
    "**Our goal is to convert natural language into structured search queries.**"
   ],
   "id": "8511f5f6abbe3788"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:02:14.736024Z",
     "start_time": "2025-07-14T20:02:14.667709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's define a schema for structured search queries (TutorialSearch schema).\n",
    "\n",
    "import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class TutorialSearch(BaseModel):\n",
    "    \"\"\"Search over a database of tutorial videos about a software library.\"\"\"\n",
    "\n",
    "    content_search: str = Field(\n",
    "        ...,\n",
    "        description=\"Similarity search query applied to video transcripts.\",\n",
    "    )\n",
    "    title_search: str = Field(\n",
    "        ...,\n",
    "        description=(\n",
    "            \"Alternate version of the content search query to apply to video titles. \"\n",
    "            \"Should be succinct and only include key words that could be in a video \"\n",
    "            \"title.\"\n",
    "        ),\n",
    "    )\n",
    "    min_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum view count filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_view_count: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum view count filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    earliest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Earliest publish date filter, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    latest_publish_date: Optional[datetime.date] = Field(\n",
    "        None,\n",
    "        description=\"Latest publish date filter, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    min_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Minimum video length in seconds, inclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "    max_length_sec: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Maximum video length in seconds, exclusive. Only use if explicitly specified.\",\n",
    "    )\n",
    "\n",
    "    def pretty_print(self) -> None:\n",
    "        for field_name, field_info in self.__class__.model_fields.items():\n",
    "            value = getattr(self, field_name)\n",
    "            if value is not None and value != getattr(field_info, \"default\", None):\n",
    "                print(f\"{field_name}: {value}\")"
   ],
   "id": "3ce2f1ff4ed4396d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:49:01.495120Z",
     "start_time": "2025-07-14T20:49:01.472452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's set up a chain to generate TutorialSearch queries from plain text queries.\n",
    "#   - I modified this part with some few-shot prompts and saw a significant performance boost.\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\"input\": \"videos on chat langchain longer than 10 minutes\", \"output\": '{\"content_search\":\"chat langchain\",\"title_search\":\"langchain\",\"min_length_sec\":\"600\"}'},\n",
    "    {\"input\": \"videos on chat langchain published in 2021\", \"output\": '{\"content_search\":\"chat langchain\",\"title_search\":\"langchain\",\"earliest_publish_date\":\"2021-01-01\",\"latest_publish_date\":\"2021-12-31\"}'},\n",
    "    {\"input\": \"videos on chat langchain\", \"output\": '{\"content_search\":\"chat langchain\",\"title_search\":\"langchain\"}'},\n",
    "]\n",
    "example_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('human', '{input}'),\n",
    "    ('ai', '{output}')\n",
    "])\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt, # formats each individual example\n",
    ")\n",
    "\n",
    "system = \"\"\"You are an expert at converting user questions into TutorialSearch queries to a vector database. \\\n",
    "You have access to a database of tutorial videos about a software library for building LLM-powered applications. \\\n",
    "Given a question, return a query optimized to retrieve the most relevant results.\n",
    "\n",
    "Rules:\n",
    "- If there are acronyms or words you are not familiar with, do not try to rephrase them.\n",
    "- Only set fields if they are mentioned in the user questions.\n",
    "- Don't use SQL. Use the fields of the TutorialSearch model provided.\"\"\"\n",
    "final_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    few_shot_prompt,\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "llm = ChatOllama(model=\"gemma3n\")\n",
    "structured_llm = llm.with_structured_output(TutorialSearch)\n",
    "query_analyzer = final_prompt | structured_llm\n",
    "\n",
    "final_prompt.invoke({\"question\": \"videos on chat langchain published in 2023\"})"
   ],
   "id": "6f9d7552aa3411ee",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content=\"You are an expert at converting user questions into TutorialSearch queries to a vector database. You have access to a database of tutorial videos about a software library for building LLM-powered applications. Given a question, return a query optimized to retrieve the most relevant results.\\n\\nRules:\\n- If there are acronyms or words you are not familiar with, do not try to rephrase them.\\n- Only set fields if they are mentioned in the user questions.\\n- Don't use SQL. Use the fields of the TutorialSearch model provided.\", additional_kwargs={}, response_metadata={}), HumanMessage(content='videos on chat langchain longer than 10 minutes', additional_kwargs={}, response_metadata={}), AIMessage(content='{\"content_search\":\"chat langchain\",\"title_search\":\"langchain\",\"min_length_sec\":\"600\"}', additional_kwargs={}, response_metadata={}), HumanMessage(content='videos on chat langchain published in 2021', additional_kwargs={}, response_metadata={}), AIMessage(content='{\"content_search\":\"chat langchain\",\"title_search\":\"langchain\",\"earliest_publish_date\":\"2021-01-01\",\"latest_publish_date\":\"2021-12-31\"}', additional_kwargs={}, response_metadata={}), HumanMessage(content='videos on chat langchain', additional_kwargs={}, response_metadata={}), AIMessage(content='{\"content_search\":\"chat langchain\",\"title_search\":\"langchain\"}', additional_kwargs={}, response_metadata={}), HumanMessage(content='videos on chat langchain published in 2023', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 156
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:48:42.864583Z",
     "start_time": "2025-07-14T20:48:41.889699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# general search\n",
    "query_analyzer.invoke(\n",
    "    {\"question\": \"rag from scratch\"}\n",
    ").pretty_print()"
   ],
   "id": "4c3382811a3a4ee3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: rag from scratch\n",
      "title_search: rag\n"
     ]
    }
   ],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:48:45.987795Z",
     "start_time": "2025-07-14T20:48:44.119704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# search based on specific publish date\n",
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos on chat langchain published in 2023\"}\n",
    ").pretty_print()"
   ],
   "id": "4b53c95430a73879",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: chat langchain\n",
      "title_search: langchain\n",
      "earliest_publish_date: 2023-01-01\n",
      "latest_publish_date: 2023-12-31\n"
     ]
    }
   ],
   "execution_count": 155
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:49:05.592167Z",
     "start_time": "2025-07-14T20:49:04.045646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# search based on publish date window\n",
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos that are focused on the topic of chat langchain that are published before 2024\"}\n",
    ").pretty_print()"
   ],
   "id": "5e6ec0b6819471e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: chat langchain\n",
      "title_search: langchain\n",
      "latest_publish_date: 2024-01-01\n"
     ]
    }
   ],
   "execution_count": 157
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:49:09.527618Z",
     "start_time": "2025-07-14T20:49:08.233959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# search based on max-length\n",
    "query_analyzer.invoke(\n",
    "    {\"question\": \"how to use multi-modal models in an agent, only videos under 5 minutes\"}\n",
    ").pretty_print()"
   ],
   "id": "6b0de853ace1031f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models\n",
      "title_search: agent\n",
      "max_length_sec: 300\n"
     ]
    }
   ],
   "execution_count": 158
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:51:00.777735Z",
     "start_time": "2025-07-14T20:50:59.474046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# search based on min-length\n",
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos on multi-modal models in an agent, only videos over 5 minutes\"}\n",
    ").pretty_print()"
   ],
   "id": "92cbc157e1612b7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: multi-modal models\n",
      "title_search: agent\n",
      "min_length_sec: 300\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T20:54:39.952552Z",
     "start_time": "2025-07-14T20:54:38.428718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# search based on multiple criteria\n",
    "query_analyzer.invoke(\n",
    "    {\"question\": \"videos on llm agents that have at least 100 views, under 8 minutes\"}\n",
    ").pretty_print()"
   ],
   "id": "d451dd5535c8aef2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_search: llm agents\n",
      "title_search: llm agents\n",
      "min_view_count: 100\n",
      "max_length_sec: 480\n"
     ]
    }
   ],
   "execution_count": 176
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
