{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Indexing is the process of preparing and organizing data (like documents, web pages, or text chunks) so that it can be efficiently searched and retrieved later."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem Statement\n",
    "\n",
    "RAG systems rely on effective indexing. Weak indexing can bury relevant content, omit key context, or fragment meaning—leading to incomplete or misleading results.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "User query:\n",
    "- \"How do I reset my password?\"\n",
    "\n",
    "Problem:\n",
    "- The document containing the answer says: “Click ‘Forgot Password’ on the login screen.”\n",
    "- But the phrase “reset your password” doesn't appear anywhere in the text.\n",
    "\n",
    "Potential bad outcome:\n",
    "- The retriever misses the relevant document.\n",
    "- The LLM responds: “Sorry, I couldn’t find anything about that,” even though the answer exists in the corpus."
   ],
   "id": "9d2fbee31528fa9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #1: Chunking Optimization\n",
    "\n",
    "**Idea**:"
   ],
   "id": "fc0530d434ebb866"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e3e7cecbf1cd41fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #2: Multi-Representation Indexing\n",
    "\n",
    "![Image](rsc/jupyter/multi_representational_indexing.png)\n",
    "\n",
    "**Idea**: Instead of storing one embedding per document chunk, you store several, each capturing a different perspective, style, or abstraction level (e.g. original text, LLM short summary, LLM semantic summary, keyword list, etc.).\n",
    "\n",
    "_**Note**: We'll focus on a simple case of this technique, in which we store a semantic summary of the document in our vector DB and use it to retrieve the original document (which we store separately in a document DB)._"
   ],
   "id": "8a35b3e4b7cfd4ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a148501b7077023"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #3: RAPTOR\n",
    "\n",
    "![Image](rsc/jupyter/raptor_indexing.png)\n",
    "\n",
    "**Idea**: Improve chunking and indexing by aligning document segmentation with natural semantic structure — especially paragraphs, sections, and topic boundaries — rather than arbitrary fixed-size chunks.\n",
    "\n",
    "Traditional chunking often splits documents into fixed-length tokens (e.g., 512 tokens), which can:\n",
    "- Cut across logical boundaries (e.g., mid-sentence or mid-paragraph)\n",
    "- Separate relevant context\n",
    "- Create confusing or meaningless chunks\n",
    "\n",
    "RAPTOR uses LLMs to identify semantic chunk boundaries that make each chunk:\n",
    "- Coherent\n",
    "- Contextually meaningful\n",
    "- Aligned with how people ask questions\n",
    "\n",
    "Then, it indexes multiple representations of those chunks — such as:\n",
    "- Original text\n",
    "- LLM-generated summaries\n",
    "- Headings or titles\n",
    "- Hierarchical relationships (e.g., section > subsection)"
   ],
   "id": "e7237d8ee749268b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2a117f0cee3b581c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #4: ColBERT\n",
    "\n",
    "**Idea**: Enable fine-grained, late-interaction retrieval, where each token in a query is matched against each token in a document, allowing for more precise and expressive retrieval — without sacrificing efficiency.\n",
    "\n",
    "With late interaction, the model doesn't collapse the input into a single vector. Instead:\n",
    "- Query is encoded into a sequence of token vectors: `q₁, q₂, ..., qₙ`\n",
    "- Document is encoded into a sequence of token vectors: `d₁, d₂, ..., dₘ`\n",
    "- You match each query token against each document token, using cosine similarity.\n",
    "\n",
    "Then, for each query token, you take the maximum similarity across all document tokens (this is the `MaxSim` operation), and sum the results to get the final score.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Query:\n",
    "- “When did Tesla begin?”\n",
    "\n",
    "Two document chunks:\n",
    "1. “Tesla was founded in 2003 by engineers...”\n",
    "2. “Tesla makes electric vehicles and solar panels.”\n",
    "\n",
    "In ColBERT:\n",
    "- \"when\" in the query might match \"2003\" in document 1.\n",
    "- \"begin\" might match \"founded\" in document 1.\n",
    "- These specific token matches dominate the score."
   ],
   "id": "8c534eedee366107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "92b3ae172b3f2431"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
