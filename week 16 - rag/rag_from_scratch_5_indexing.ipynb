{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "# Indexing is the process of preparing and organizing data (like documents, web pages, or text chunks) so that it can be efficiently searched and retrieved later."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem Statement\n",
    "\n",
    "RAG systems rely on effective indexing. Weak indexing can bury relevant content, omit key context, or fragment meaning—leading to incomplete or misleading results.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "User query:\n",
    "- \"How do I reset my password?\"\n",
    "\n",
    "Problem:\n",
    "- The document containing the answer says: “Click ‘Forgot Password’ on the login screen.”\n",
    "- But the phrase “reset your password” doesn't appear anywhere in the text.\n",
    "\n",
    "Potential bad outcome:\n",
    "- The retriever misses the relevant document.\n",
    "- The LLM responds: “Sorry, I couldn’t find anything about that,” even though the answer exists in the corpus."
   ],
   "id": "9d2fbee31528fa9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #1: Chunking Optimization\n",
    "\n",
    "**Idea**:"
   ],
   "id": "fc0530d434ebb866"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# todo: not covered in course; do separately\n",
    "# https://www.youtube.com/watch?v=8OJC21T2SL4"
   ],
   "id": "e3e7cecbf1cd41fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #2: Multi-Representation Indexing\n",
    "\n",
    "![Image](rsc/jupyter/multi_representational_indexing.png)\n",
    "\n",
    "**Idea**: Instead of storing one embedding per document chunk, you store several, each capturing a different perspective, style, or abstraction level (e.g. original text, LLM short summary, LLM semantic summary, keyword list, etc.).\n",
    "\n",
    "_**Note**: We'll focus on a simple case of this technique, in which we store a semantic summary of the document in our vector DB and use it to retrieve the original document (which we store separately in a document DB)._"
   ],
   "id": "8a35b3e4b7cfd4ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a148501b7077023"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #3: RAPTOR\n",
    "\n",
    "![Image](rsc/jupyter/raptor_indexing.png)\n",
    "\n",
    "**Idea**: Create a hierarchy of documents summaries with high-level summaries at the top of the hierarchy, lower-level summaries in the middle, and the original documents at the bottom. When user queries come in:\n",
    "- Abstract high-level queries are handled by the high-level and mid-level summaries.\n",
    "- Specific low-level queries are handled by the original documents.\n",
    "\n",
    "---\n",
    "\n",
    "Goals of RAPTOR (Retrieval-Aware Pretraining for Targets of Retrieval)\n",
    "1. Create meaningful, retriever-friendly chunks (not arbitrary token splits).\n",
    "2. Organize them hierarchically to reflect document structure.\n",
    "3. Use LLM-generated summaries to describe higher-level sections.\n",
    "\n",
    "At index time:\n",
    "- Chunk documents semantically using heuristics or LLMs.\n",
    "- Organize chunks into a tree (paragraphs → sections → document).\n",
    "- Summarize each node with an LLM.\n",
    "- Index each level’s summary as a separate vector.\n",
    "\n",
    "At query time:\n",
    "- You can match the query against all levels.\n",
    "- Traverse the hierarchy based on relevance.\n",
    "- Retrieve the most informative chunk(s) at the right level of detail."
   ],
   "id": "e7237d8ee749268b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# todo: revisit, implement later\n",
    "# Reference: https://github.com/langchain-ai/langchain/blob/master/cookbook/RAPTOR.ipynb"
   ],
   "id": "2a117f0cee3b581c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #4: ColBERT\n",
    "\n",
    "![Image](rsc/jupyter/colbert_indexing.png)\n",
    "\n",
    "**Idea**: A fundamentally different algorithm for determining document-query similarity: each token in a query is matched against each token in a document, allowing for more precise and expressive retrieval (fine-grained, late-interaction retrieval).\n",
    "\n",
    "---\n",
    "\n",
    "With late interaction, the model doesn't collapse the input into a single vector. Instead:\n",
    "- Query is encoded into a sequence of token vectors: `q₁, q₂, ..., qₙ`\n",
    "- Document is encoded into a sequence of token vectors: `d₁, d₂, ..., dₘ`\n",
    "- You match each query token against each document token, using cosine similarity.\n",
    "\n",
    "Then, for each query token, you take the maximum similarity across all document tokens (this is the `MaxSim` operation), and sum the results to get the final score.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Query:\n",
    "- “When did Tesla begin?”\n",
    "\n",
    "Two document chunks:\n",
    "1. “Tesla was founded in 2003 by engineers...”\n",
    "2. “Tesla makes electric vehicles and solar panels.”\n",
    "\n",
    "In ColBERT:\n",
    "- \"when\" in the query might match \"2003\" in document 1.\n",
    "- \"begin\" might match \"founded\" in document 1.\n",
    "- These specific token matches dominate the score."
   ],
   "id": "8c534eedee366107"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "92b3ae172b3f2431"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
