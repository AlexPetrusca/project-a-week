{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:51.975801Z",
     "start_time": "2025-07-14T02:08:51.971937Z"
    }
   },
   "cell_type": "code",
   "source": "# Let's expand on the basic RAG system we developed from scratch",
   "id": "9d8f1ab66c0acb92",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:51.990987Z",
     "start_time": "2025-07-14T02:08:51.983982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # load environment variables"
   ],
   "id": "eac3a3e92091b4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:53.881407Z",
     "start_time": "2025-07-14T02:08:52.053364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Do all the setup here\n",
    "\n",
    "import bs4\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Fetch blog content\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50) # uses tiktoken before splitting\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index (embed and store) the chunks into a vector db\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Start llama3.2 model\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")"
   ],
   "id": "e7297bf2105a6f8a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:53.894702Z",
     "start_time": "2025-07-14T02:08:53.892251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define some reusable prompts & queries\n",
    "\n",
    "# User query\n",
    "query = \"What is task decomposition for LLM agents?\"\n",
    "query_cot = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "# RAG template\n",
    "rag_template = PromptTemplate.from_template('''\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "''')"
   ],
   "id": "99411d1893b7cfc0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem Statement\n",
    "\n",
    "User queries are a challenge: _If a user provides an ambiguous query, they'll get ambiguous matches._\n",
    "\n",
    "LLMs just follow what was in the context and hallucinate answers as a result.\n",
    "\n",
    "#### Example\n",
    "\n",
    "User query:\n",
    "- \"Jaguar speed\"\n",
    "\n",
    "Problem:\n",
    "- \"Jaguar\" could refer to the animal or the car.\n",
    "- \"Speed\" could mean top speed, acceleration, or agility.\n",
    "\n",
    "Potential bad outcome:\n",
    "- Retrieval returns mixed results (e.g., car specs and animal facts).\n",
    "- LLM hallucinates by combining facts:\n",
    "    - “The Jaguar can reach 150 mph in the wild.”"
   ],
   "id": "2afde4d34820d6d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #1: Multi-Query\n",
    "\n",
    "![Image](rsc/jupyter/multi-query.png)\n",
    "\n",
    "**Idea**: Instead of relying on just one query, let's generate _multiple semantically different queries_ that represent various plausible interpretations or reformulations of the original user question."
   ],
   "id": "a6c600c9679ac949"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:54.668779Z",
     "start_time": "2025-07-14T02:08:53.905579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template_multi = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions as a numbered list. Don't include text before or after\n",
    "this list. Original question: {question}\"\"\"\n",
    "prompt_multi_query = PromptTemplate.from_template(template_multi)\n",
    "\n",
    "generate_queries_multi = (\n",
    "    prompt_multi_query\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "multi_queries = generate_queries_multi.invoke(query)\n",
    "print(len(multi_queries))\n",
    "print(multi_queries)"
   ],
   "id": "a069ad14147d3549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['1. What are tasks decomposed into in large language models (LLMs) like myself?', '2. How do I identify the individual components of a task in LLMs?', \"3. What's the process behind splitting complex tasks into smaller sub-tasks within LLMs?\", '4. In what ways can I understand and decompose tasks given their representation in vector space?', '5. Can you explain how to decompose a given task from its input representations or embeddings?']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:55.589927Z",
     "start_time": "2025-07-14T02:08:54.684591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retrieval_chain_multi = generate_queries_multi | retriever.map()\n",
    "\n",
    "multi_retrievals = retrieval_chain_multi.invoke(query)\n",
    "print(len(multi_retrievals))  # 5 questions\n",
    "print(len(multi_retrievals[0]))  # 4 documents per question"
   ],
   "id": "27d36c1baa82043b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:58.044756Z",
     "start_time": "2025-07-14T02:08:55.595290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [doc.page_content for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return unique_docs\n",
    "\n",
    "rag_chain_multi = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_multi | get_unique_union,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | rag_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_multi.invoke({\"question\": query})"
   ],
   "id": "962661f2f8c330d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This process typically involves decomposing the problem into multiple thought steps and generating multiple thoughts per step, creating a tree structure to explore possible solutions. The goal is to reduce the complexity of the task by breaking it down into smaller, more manageable parts, and then providing multiple options or paths to reach the desired outcome.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #2: RAG-Fusion\n",
    "\n",
    "![Image](rsc/jupyter/rag-fusion.png)\n",
    "\n",
    "**Idea**: Extension of Multi-Query - we not only generate multiple semantically different queries, but also combine (_fuse_) the results by _ranking and removing duplicates_ before passing them to the language model."
   ],
   "id": "9c9a8eb2103f7b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:58.393781Z",
     "start_time": "2025-07-14T02:08:58.060198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG-Fusion: Related\n",
    "template_fusion = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. Provide these queries as a numbered list. Don't include text before or after this list. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_fusion = PromptTemplate.from_template(template_fusion)\n",
    "\n",
    "generate_queries_fusion = (\n",
    "    prompt_fusion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "fusion_queries = generate_queries_fusion.invoke(query)\n",
    "print(len(fusion_queries))\n",
    "print(fusion_queries)"
   ],
   "id": "d2c5d529663ff92f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['1. what is task decomposition in language model learning models', '2. task decomposition definition llm algorithmic approach', '3. how does task decomposition work for llm models', '4. what tasks can be decomposed by llm models']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:08:58.783767Z",
     "start_time": "2025-07-14T02:08:58.405276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_fusion = generate_queries_fusion | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "docs = retrieval_chain_fusion.invoke({\"question\": query})\n",
    "len(docs)"
   ],
   "id": "95411b629ab8961",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/9t5yl6pn2cv9wlv3dmqsgd300000gq/T/ipykernel_92055/2132239143.py:24: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  (loads(doc), score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:09:00.771694Z",
     "start_time": "2025-07-14T02:08:58.798258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_chain_fusion = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_fusion,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | rag_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_fusion.invoke({\"question\": query})"
   ],
   "id": "17144cec4c522b82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition for Large Language Model (LLM) agents involves breaking down complex tasks into smaller, manageable subgoals to enhance efficiency and improve model performance. This process typically requires a chain of thought or tree of thoughts to explore multiple possible solutions and generate multiple thoughts per step. The goal is to transform big tasks into multiple manageable tasks that can be solved more effectively using the LLM's capabilities.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #3: Decomposition\n",
    "\n",
    "![Image](rsc/jupyter/decomposition.png)\n",
    "\n",
    "**Idea**: Break down a complex query into simpler, more specific sub-queries that each target a different facet of the information need.\n",
    "\n",
    "_Note: This is especially helpful in multi-hop questions or those requiring reasoning over multiple pieces of information._"
   ],
   "id": "38a78bfd20682267"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:09:01.329448Z",
     "start_time": "2025-07-14T02:09:00.788425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template_decomposition_generate = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Provide these sub-questions as a numbered list. Don't include text before or after this list. \\n\n",
    "\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition_generate = PromptTemplate.from_template(template_decomposition_generate)\n",
    "\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition_generate\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "decomposition_queries = generate_queries_decomposition.invoke(query_cot)\n",
    "print(len(decomposition_queries))\n",
    "print(decomposition_queries)"
   ],
   "id": "99992273ad822005",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['1. What are the primary components required for a large language model (LLM) to function effectively as a self-driving car?', '2. How do different architectures contribute to the overall performance and flexibility of an LLM-based autonomous driving system?', '3. What role does pre-trained knowledge retrieval in LLMs play when it comes to integrating with external sensor data in a self-driving vehicle?']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:09:32.080864Z",
     "start_time": "2025-07-14T02:09:01.344017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template_decomposition_cot = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {qa_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_decomposition_cot = PromptTemplate.from_template(template_decomposition_cot)\n",
    "\n",
    "rag_chain_decomposition = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever.map(),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"qa_pairs\": itemgetter(\"qa_pairs\")\n",
    "    }\n",
    "    | prompt_decomposition_cot\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "qa_pairs = \"\"\n",
    "for question in decomposition_queries:\n",
    "    answer = rag_chain_decomposition.invoke({\"question\": question, \"qa_pairs\": qa_pairs})  # sub-problem answers\n",
    "    qa_pair = format_qa_pair(question, answer)\n",
    "    qa_pairs = qa_pairs + \"\\n---\\n\" + qa_pair\n",
    "\n",
    "answer = rag_chain_decomposition.invoke({\"question\": query_cot, \"qa_pairs\": qa_pairs})  # final answer to original question\n",
    "\n",
    "print(answer)"
   ],
   "id": "620354efdbce6a85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I'll break down the main components of an LLM (Large Language Model)-powered autonomous agent system:\n",
      "\n",
      "1. **Language Model (LLM):** This is a crucial component that allows the agent to understand and process human language. In this case, it's mentioned as being fine-tuned for specific tasks like verbal math problems.\n",
      "\n",
      "2. **Memory:** As discussed in the context of Long-Term Memory (LTM), LLMs are equipped with two types of memory:\n",
      "   - Explicit / declarative memory: Stores facts and events that can be consciously recalled.\n",
      "   - Implicit / procedural memory: Enables automatic skills and routines, such as riding a bike or typing on a keyboard.\n",
      "\n",
      "3. **Maximum Inner Product Search (MIPS):** This is an algorithm used for fast maximum inner-product search, which is crucial for tasks like visual recognition in images. MIPS typically employs approximation nearest neighbors algorithms to find nearby items quickly, while sacrificing some accuracy.\n",
      "\n",
      "4. **External APIs:** The ability of the LLM to learn and utilize external tool APIs indicates that it can integrate with various tools and systems to perform specific tasks. This could include natural language processing (NLP) tools for text analysis, machine learning models for image recognition, or even more complex frameworks like TALM (Tool Augmented Language Models) for fine-tuning LLMs.\n",
      "\n",
      "5. **External Data Store:** A vector store database is mentioned as a standard practice to support fast maximum inner-product search. This suggests that the system utilizes external memory storage for efficient data retrieval and update.\n",
      "\n",
      "6. **File Output:** File output could be another aspect of resource management, possibly related to storing or retrieving large amounts of data in the agent's environment.\n",
      "\n",
      "7. **Long Term Memory (LTM):** As discussed earlier, LTM is divided into two subtypes: explicit/declarative memory and implicit/procedural memory. The long-term storage capacity for both types is essentially unlimited, indicating a high level of information retention.\n",
      "\n",
      "8. **Short-Term Memory (STM):** STM stores up to 7 items that are current and relevant to the agent's task at hand. This indicates a limited capacity for information processing but sufficient for most everyday tasks.\n",
      "\n",
      "9. **Sensory Memory:** The ability to retain impressions of sensory inputs is an essential component, particularly in visual or auditory contexts.\n",
      "\n",
      "10. **Categorization of Human Memory:** This refers to mapping human memory into more structured and understandable categories, such as explicit/declarative vs. implicit/procedural memory.\n",
      "\n",
      "These components work together to enable the LLM-powered autonomous agent system to interact with its environment effectively, perform tasks efficiently, and adapt to changing situations.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #4: Step-Back\n",
    "\n",
    "![Image](rsc/jupyter/step-back.png)\n",
    "\n",
    "**Idea**: Instead of directly answering a complex question with a single retrieval call, \"step back\" and ask: What do I need to know to answer this? Then use those simpler () subquestions to drive retrieval and reasoning."
   ],
   "id": "f7f831f70a412b8f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:09:32.253785Z",
     "start_time": "2025-07-14T02:09:32.090297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt_step_back = PromptTemplate.from_template(\"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer.\n",
    "\n",
    "Here are a few examples:\n",
    "    1. input: Could the members of the police perform lawful arrests?\n",
    "       output: What can the members of the police do?\n",
    "    2. input: Jan Sindel’s was born in what country?\n",
    "       output: What is Jan Sindel’s personal history?\n",
    "\n",
    "Rules:\n",
    "- Don't include the examples above in your output\n",
    "- Only include the output.\n",
    "    - Don't include text before or after.\n",
    "    - Don't put quotes around the output.\n",
    "\n",
    "Here is your input:\n",
    "\"{question}\"\n",
    "\n",
    "output:\n",
    "\"\"\")\n",
    "\n",
    "generate_queries_step_back = prompt_step_back | llm | StrOutputParser()\n",
    "\n",
    "generate_queries_step_back.invoke({\"question\": query})"
   ],
   "id": "c40d30ca8c3c38e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are task decomposition for Large Language Model (LLM) agents?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:09:36.909807Z",
     "start_time": "2025-07-14T02:09:32.265481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Response prompt from paper\n",
    "rag_template_step_back = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer: \"\"\"\n",
    "\n",
    "rag_prompt_step_back = PromptTemplate.from_template(rag_template_step_back)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": itemgetter(\"question\") | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | rag_prompt_step_back\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": query})"
   ],
   "id": "7aa09318428ba491",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a critical component of building autonomous agents using large language models (LLMs). Task decomposition refers to the process of breaking down complex tasks into smaller, manageable subtasks that can be executed by the agent.\\n\\nIn the context provided, task decomposition involves several key steps:\\n\\n1. **Subgoal and decomposition**: The agent breaks down large tasks into smaller, more specific subgoals.\\n2. **Reflection and refinement**: The agent reflects on past actions, learns from mistakes, and refines them for future steps to improve the quality of final results.\\n3. **Task decomposition with human inputs**: In some cases, task decomposition can be done using human-provided instructions or prompts.\\n\\nFinite context length limitations are a challenge in task decomposition, as they restrict the inclusion of historical information, detailed instructions, API call context, and responses. To mitigate this issue, mechanisms like self-reflection can learn from past mistakes and improve future planning.\\n\\nRegarding reliability issues, LLMs may struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error. Additionally, the current agent system relies heavily on natural language as an interface between LLMs and external components, which can be prone to errors.\\n\\nThe proposed modular reasoning, knowledge, and language (MRKL) architecture is a neuro-symbolic approach that uses a collection of \"expert\" modules and a general-purpose LLM as a router. The MRKL system has been fine-tuned for tasks like arithmetic calculation, and it highlights the importance of choosing the right tools for the job.\\n\\nBoiko et al.\\'s (2023) work on LLM-empowered agents demonstrates the potential of using LLMs for scientific discovery, autonomous design, planning, and performance of complex experiments. Their example shows how a model can use a variety of tools to generate reasoning steps for developing novel anticancer drugs.\\n\\nIn summary, task decomposition is a crucial aspect of building LLM-powered autonomous agents, requiring careful consideration of finite context length limitations, reliability issues, and the choice of external tools and interfaces. The proposed MRKL architecture highlights the potential benefits of using modular reasoning, knowledge, and language to improve agent performance.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #5: Hyde\n",
    "\n",
    "![Image](rsc/jupyter/hyde.png)\n",
    "\n",
    "**Idea**: Instead of retrieving documents based on the query, retrieve based on a hypothetical answer (based on the query)."
   ],
   "id": "e75e4f48f1e3841c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:09:39.751456Z",
     "start_time": "2025-07-14T02:09:36.927887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# HyDE document generation\n",
    "template_hyde = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage: \"\"\"\n",
    "prompt_hyde = PromptTemplate.from_template(template_hyde)\n",
    "\n",
    "generate_hyde_docs = prompt_hyde | llm | StrOutputParser()\n",
    "\n",
    "generate_hyde_docs.invoke({\"question\": query})"
   ],
   "id": "446ee349e02bbdd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition in Large Language Models (LLMs): A Framework for Multitasking and Autonomy\\n\\nTask decomposition, a fundamental concept in cognitive psychology and artificial intelligence, refers to the process of breaking down complex tasks into smaller, manageable subtasks. In the context of Large Language Models (LLMs), task decomposition has emerged as a critical component for achieving multitask learning and autonomy. By decomposing a given task or domain into constituent subtasks, LLMs can exploit their parallel processing capabilities, reducing computational overhead and improving overall performance.\\n\\nAt its core, task decomposition involves identifying the relevant subtasks that comprise the original task, typically represented through a hierarchical representation of the task space (i.e., a set of high-level concepts and fine-grained details). This identification is followed by a process of refinement, where each subtask is assigned to a specific LLM agent or component, ensuring optimal utilization of their computational resources.\\n\\nThe benefits of task decomposition for LLMs are multifaceted. Firstly, it enables the LLM to exploit its parallel processing capabilities, facilitating improved multitask performance and reducing computational overhead. Secondly, task decomposition allows for more flexible and autonomous operation, as each LLM agent is responsible for executing a specific subset of subtasks. This autonomy fosters greater adaptability and robustness, making LLMs more suitable for tasks that require dynamic reconfiguration or adaptation to changing environments.\\n\\nFurthermore, task decomposition facilitates the integration of diverse knowledge sources and domain expertise, enriching the overall performance of the LLM system. By decomposing complex tasks into constituent subtasks, each LLM agent can leverage its unique strengths and capabilities, leading to a more comprehensive understanding of the task at hand.\\n\\nIn conclusion, task decomposition is a critical framework for achieving multitask learning and autonomy in Large Language Models. By exploiting their parallel processing capabilities and assigning specific subtasks to individual agents, LLMs can improve overall performance, increase adaptability, and enhance robustness. As research continues to explore the nuances of task decomposition in LLMs, it is essential to address the challenges associated with integrating diverse knowledge sources and domain expertise, ultimately driving the development of more sophisticated and effective AI systems.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:09:42.202722Z",
     "start_time": "2025-07-14T02:09:39.768848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve using HyDE document\n",
    "retrieval_chain_hyde = generate_hyde_docs | retriever\n",
    "\n",
    "retrieved_docs = retrieval_chain_hyde.invoke({\"question\": query})\n",
    "print(retrieved_docs)"
   ],
   "id": "46c067aca69a450b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'), Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Or\\n@article{weng2023agent,\\n  title   = \"LLM-powered Autonomous Agents\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Jun\",\\n  url     = \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\\n}\\nReferences#\\n[1] Wei et al. “Chain of thought prompting elicits reasoning in large language models.” NeurIPS 2022\\n[2] Yao et al. “Tree of Thoughts: Dliberate Problem Solving with Large Language Models.” arXiv preprint arXiv:2305.10601 (2023).\\n[3] Liu et al. “Chain of Hindsight Aligns Language Models with Feedback\\n“ arXiv preprint arXiv:2302.02676 (2023).\\n[4] Liu et al. “LLM+P: Empowering Large Language Models with Optimal Planning Proficiency” arXiv preprint arXiv:2304.11477 (2023).')]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T02:09:46.315145Z",
     "start_time": "2025-07-14T02:09:42.223287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_chain_hyde = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_hyde,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | rag_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_hyde.invoke({\"question\": query})"
   ],
   "id": "51a931cd2edabc3c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents involves breaking down large tasks into smaller, manageable subgoals to improve efficiency and quality of results. This process typically involves several steps such as planning, reflection, and refinement, using techniques like Chain of Thought (CoT), Tree of Thoughts (Yao et al.), and model-specific instructions.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
