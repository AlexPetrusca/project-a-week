{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:55:54.824224Z",
     "start_time": "2025-07-13T23:55:54.820858Z"
    }
   },
   "cell_type": "code",
   "source": "# Let's expand on the basic RAG system we developed from scratch",
   "id": "9d8f1ab66c0acb92",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:55:54.836018Z",
     "start_time": "2025-07-13T23:55:54.830357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # load environment variables"
   ],
   "id": "eac3a3e92091b4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:55:57.035183Z",
     "start_time": "2025-07-13T23:55:54.895983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Do all the setup here\n",
    "\n",
    "import bs4\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Fetch blog content\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50) # uses tiktoken before splitting\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index (embed and store) the chunks into a vector db\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Start llama3.2 model\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")"
   ],
   "id": "e7297bf2105a6f8a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T00:15:19.718039Z",
     "start_time": "2025-07-14T00:15:19.716107Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define some reusable prompts & queries\n",
    "\n",
    "# User query\n",
    "query = \"What is task decomposition for LLM agents?\"\n",
    "query_cot = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "\n",
    "# RAG template\n",
    "rag_template = PromptTemplate.from_template('''\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "''')"
   ],
   "id": "99411d1893b7cfc0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem Statement\n",
    "\n",
    "User queries are a challenge: _If a user provides an ambiguous query, they'll get ambiguous matches._\n",
    "\n",
    "LLMs just follow what was in the context and hallucinate answers as a result.\n",
    "\n",
    "#### Example\n",
    "\n",
    "User query:\n",
    "- \"Jaguar speed\"\n",
    "\n",
    "Problem:\n",
    "- \"Jaguar\" could refer to the animal or the car.\n",
    "- \"Speed\" could mean top speed, acceleration, or agility.\n",
    "\n",
    "Potential bad outcome:\n",
    "- Retrieval returns mixed results (e.g., car specs and animal facts).\n",
    "- LLM hallucinates by combining facts:\n",
    "    - “The Jaguar can reach 150 mph in the wild.”"
   ],
   "id": "2afde4d34820d6d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #1: Multi-Query\n",
    "\n",
    "![Image](rsc/jupyter/multi-query.png)\n",
    "\n",
    "**Idea**: Instead of relying on just one query, let's generate _multiple semantically different queries_ that represent various plausible interpretations or reformulations of the original user question."
   ],
   "id": "a6c600c9679ac949"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:55:58.643728Z",
     "start_time": "2025-07-13T23:55:57.045702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template_multi = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions as a numbered list. Don't include text before or after\n",
    "this list. Original question: {question}\"\"\"\n",
    "prompt_multi_query = PromptTemplate.from_template(template_multi)\n",
    "\n",
    "generate_queries_multi = (\n",
    "    prompt_multi_query\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "multi_queries = generate_queries_multi.invoke(query)\n",
    "print(len(multi_queries))\n",
    "print(multi_queries)"
   ],
   "id": "a069ad14147d3549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['1. How do I decompose tasks into smaller sub-tasks using large language models?', '2. What are the steps involved in breaking down complex tasks using transformer-based language models like LLMs?', '3. In natural language processing, how does task decomposition help in modeling and understanding multi-step processes?', '4. Can you explain task decomposition for LLM agents in terms of their information retrieval capabilities?', '5. What is the theoretical framework behind task decomposition and its relevance to information retrieval tasks?']\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:55:59.600747Z",
     "start_time": "2025-07-13T23:55:58.659388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retrieval_chain_multi = generate_queries_multi | retriever.map()\n",
    "\n",
    "multi_retrievals = retrieval_chain_multi.invoke(query)\n",
    "print(len(multi_retrievals))  # 5 questions\n",
    "print(len(multi_retrievals[0]))  # 4 documents per question"
   ],
   "id": "27d36c1baa82043b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:56:01.816418Z",
     "start_time": "2025-07-13T23:55:59.603742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [doc.page_content for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return unique_docs\n",
    "\n",
    "rag_chain_multi = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_multi | get_unique_union,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | rag_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_multi.invoke({\"question\": query})"
   ],
   "id": "962661f2f8c330d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for Large Language Model (LLM) agents involves breaking down complex tasks into smaller, manageable parts or \"thought steps\" to enable effective execution and planning. This process transforms big tasks into multiple manageable tasks that can be explored by the model using techniques such as chain of thought (CoT) and tree of thoughts (Yao et al., 2022). The goal is to identify the necessary subgoals for achieving a specific task, allowing LLM agents to break down complex problems into more manageable parts.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #2: RAG-Fusion\n",
    "\n",
    "![Image](rsc/jupyter/rag-fusion.png)\n",
    "\n",
    "**Idea**: Extension of Multi-Query - we not only generate multiple semantically different queries, but also combine (_fuse_) the results by _ranking and removing duplicates_ before passing them to the language model."
   ],
   "id": "9c9a8eb2103f7b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:56:02.199741Z",
     "start_time": "2025-07-13T23:56:01.825948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RAG-Fusion: Related\n",
    "template_fusion = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. Provide these queries as a numbered list. Don't include text before or after this list. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_fusion = PromptTemplate.from_template(template_fusion)\n",
    "\n",
    "generate_queries_fusion = (\n",
    "    prompt_fusion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "fusion_queries = generate_queries_fusion.invoke(query)\n",
    "print(len(fusion_queries))\n",
    "print(fusion_queries)"
   ],
   "id": "d2c5d529663ff92f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "['1\"What are the steps involved in task decomposition for large language models?\"', '2\"Can you explain task decomposition algorithms used by LLMs?\"', '3\"What\\'s the purpose of task decomposition in natural language processing?\"', '4\"How do large language models achieve task decomposition?\"']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:56:02.783293Z",
     "start_time": "2025-07-13T23:56:02.217674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "\n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_fusion = generate_queries_fusion | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "docs = retrieval_chain_fusion.invoke({\"question\": query})\n",
    "len(docs)"
   ],
   "id": "95411b629ab8961",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wk/9t5yl6pn2cv9wlv3dmqsgd300000gq/T/ipykernel_90261/2132239143.py:24: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  (loads(doc), score)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T23:56:05.146076Z",
     "start_time": "2025-07-13T23:56:02.792434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rag_chain_fusion = (\n",
    "    {\n",
    "        \"context\": retrieval_chain_fusion,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | rag_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_fusion.invoke({\"question\": query})"
   ],
   "id": "17144cec4c522b82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Task decomposition for LLM (Large Language Models) agents involves breaking down complex tasks into smaller, manageable steps to improve their performance and reliability. This process typically involves the following stages:\\n\\n1. **Task planning**: The agent identifies the task and its dependencies.\\n2. **Instruction selection**: The agent selects an appropriate instruction or model based on the user's request and the call command.\\n3. **Model execution**: The selected model executes on the specific task, generating results and logs.\\n\\nThis process is crucial for LLM agents to achieve optimal planning proficiency and reliability.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #3: Decomposition\n",
    "\n",
    "![Image](rsc/jupyter/decomposition.png)\n",
    "\n",
    "**Idea**: Break down a complex query into simpler, more specific sub-queries that each target a different facet of the information need.\n",
    "\n",
    "_Note: This is especially helpful in multi-hop questions or those requiring reasoning over multiple pieces of information._"
   ],
   "id": "38a78bfd20682267"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T00:16:00.990826Z",
     "start_time": "2025-07-14T00:16:00.404332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template_decomposition_generate = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Provide these sub-questions as a numbered list. Don't include text before or after this list. \\n\n",
    "\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition_generate = PromptTemplate.from_template(template_decomposition_generate)\n",
    "\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition_generate\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "decomposition_queries = generate_queries_decomposition.invoke(query_cot)\n",
    "print(len(decomposition_queries))\n",
    "print(decomposition_queries)"
   ],
   "id": "99992273ad822005",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "['1. What are the primary architecture components of a Large Language Model (LLM) powered autonomous agent?', '2. How do different component types contribute to the overall performance and functionality of an LLM-based autonomous agent?', '3. What are some key technologies that enable the integration of AI models like LLMs into autonomous agent systems?']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T00:31:34.122988Z",
     "start_time": "2025-07-14T00:31:12.178843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template_decomposition_cot = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {qa_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question:\n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt_decomposition_cot = PromptTemplate.from_template(template_decomposition_cot)\n",
    "\n",
    "rag_chain_decomposition = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever.map(),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"qa_pairs\": itemgetter(\"qa_pairs\")\n",
    "    }\n",
    "    | prompt_decomposition_cot\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "\n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "qa_pairs = \"\"\n",
    "for question in decomposition_queries:\n",
    "    answer = rag_chain_decomposition.invoke({\"question\": question, \"qa_pairs\": qa_pairs})  # sub-problem answers\n",
    "    qa_pair = format_qa_pair(question, answer)\n",
    "    qa_pairs = qa_pairs + \"\\n---\\n\" + qa_pair\n",
    "\n",
    "answer = rag_chain_decomposition.invoke({\"question\": query_cot, \"qa_pairs\": qa_pairs})  # final answer to original question\n",
    "\n",
    "print(answer)"
   ],
   "id": "620354efdbce6a85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here are the main components of an LLM (Large Language Model) powered autonomous agent system:\n",
      "\n",
      "1. **External Memory**: The ability of an LLM to store information in external memory, such as vector stores or databases, which can be used for fast and efficient retrieval.\n",
      "2. **Long-Term Memory (LTM)**: Long-term storage capacity that allows the agent to retain information for a remarkable long time, potentially ranging from a few days to decades.\n",
      "3. **External APIs**: Access to external tools and platforms that enable the LLM-powered autonomous agent system to perform tasks autonomously, such as text processing, data analysis, or machine learning.\n",
      "4. **Adaptability**: The ability of the LLM-powered autonomous agent system to adapt and learn from new experiences, environments, and tasks.\n",
      "5. **Autonomy**: The capacity of the agent to operate independently and make decisions without human intervention.\n",
      "\n",
      "These components work together to enable an LLM-powered autonomous agent system to:\n",
      "\n",
      "* Process and understand complex data\n",
      "* Learn from experience and improve performance over time\n",
      "* Adapt to changing environments and situations\n",
      "* Perform tasks autonomously, making decisions without human input\n",
      "\n",
      "By leveraging the strengths of LLMs in processing and understanding large amounts of data, while using external memory for faster retrieval and adaptive learning, an LLM-powered autonomous agent system can achieve impressive capabilities.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T00:18:06.810935Z",
     "start_time": "2025-07-14T00:18:06.809158Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c7b395274b8fe3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
