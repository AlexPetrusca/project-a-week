{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:56:52.882664Z",
     "start_time": "2025-07-13T03:56:52.880287Z"
    }
   },
   "cell_type": "code",
   "source": "# Let's expand on the basic RAG system we developed from scratch",
   "id": "9d8f1ab66c0acb92",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:56:52.897338Z",
     "start_time": "2025-07-13T03:56:52.893691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # load environment variables"
   ],
   "id": "eac3a3e92091b4b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:56:54.446051Z",
     "start_time": "2025-07-13T03:56:52.912921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Do all the setup here\n",
    "\n",
    "import bs4\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Fetch blog content\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50) # uses tiktoken before splitting\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index (embed and store) the chunks into a vector db\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Start llama3.2 model\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\")\n",
    "\n",
    "# User query\n",
    "query = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "# RAG template\n",
    "rag_template = PromptTemplate.from_template('''\n",
    "    You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "''')"
   ],
   "id": "e7297bf2105a6f8a",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem Statement\n",
    "\n",
    "User queries are a challenge: _If a user provides an ambiguous query, they'll get ambiguous matches._\n",
    "\n",
    "LLMs just follow what was in the context and hallucinate answers as a result."
   ],
   "id": "2afde4d34820d6d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Strategy #1: Multi-Query",
   "id": "7aae85f884d68a9d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![Image](rsc/jupyter/multi-query.png)\n",
    "\n",
    "**Motivation**: A single query may not capture all the nuances of an information need. It might:\n",
    "- Use suboptimal phrasing.\n",
    "- Miss alternative wordings or interpretations.\n",
    "- Retrieve documents from only one angle or perspective.\n",
    "\n",
    "**Idea**: Instead of relying on just one query, let's generate _multiple semantically different queries_ that represent various plausible interpretations or reformulations of the original user question."
   ],
   "id": "a6c600c9679ac949"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:56:56.375050Z",
     "start_time": "2025-07-13T03:56:54.454179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions as a numbered list. Don't include text before or after\n",
    "this list. Original question: {question}\"\"\"\n",
    "prompt_perspectives = PromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "multi_queries = generate_queries.invoke(query)\n",
    "print(len(multi_queries))\n",
    "print(multi_queries)"
   ],
   "id": "a069ad14147d3549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "['1. Task decomposition in language models refers to the process of breaking down complex tasks into smaller, more manageable sub-tasks, allowing for improved efficiency and effectiveness.', '2. Can you explain how task decomposition affects the performance of large language model (LLM) agents, particularly in terms of resource allocation and learning strategies?', \"3. What are some common techniques used in task decomposition for LLMs, such as hierarchical or graph-based approaches, and how do they impact the model's ability to generalize?\", '4. How does task decomposition compare to other methods like feature extraction or attribute selection in terms of achieving better performance on downstream tasks like language translation or question answering?', '5. Can you provide an example of a specific problem where task decomposition is applied for LLMs, such as sentiment analysis or text classification, and discuss the benefits of using this approach?']\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:56:57.117502Z",
     "start_time": "2025-07-13T03:56:56.391699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "retrieval_chain = generate_queries | retriever.map()\n",
    "\n",
    "multi_retrievals = retrieval_chain.invoke(query)\n",
    "print(len(multi_retrievals))  # 5 questions\n",
    "print(len(multi_retrievals[0]))  # 4 documents per question"
   ],
   "id": "27d36c1baa82043b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T03:58:56.828410Z",
     "start_time": "2025-07-13T03:58:54.962219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [doc.page_content for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return unique_docs\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\n",
    "        \"context\": retrieval_chain | get_unique_union,\n",
    "        \"question\": itemgetter(\"question\")\n",
    "    }\n",
    "    | rag_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\": query})"
   ],
   "id": "962661f2f8c330d2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents involves breaking down complex tasks into smaller and simpler steps to enable them to learn from past mistakes and improve their performance over time. This process can be done using various techniques, including chain of thought (CoT), tree of thoughts (Yao et al., 2022), or using human inputs (Task-specific instructions).'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
