{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:32.792287Z",
     "start_time": "2025-07-14T18:05:32.790331Z"
    }
   },
   "source": "# Routing is the process of deciding how to handle a user query by directing it to the most appropriate retriever, knowledge source, or processing path.",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:32.806877Z",
     "start_time": "2025-07-14T18:05:32.801196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv() # load environment variables"
   ],
   "id": "e30b0c622eba3f69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:35.201841Z",
     "start_time": "2025-07-14T18:05:32.894975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Do all the setup here\n",
    "\n",
    "import bs4\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Fetch blog content\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50) # uses tiktoken before splitting\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "# Index (embed and store) the chunks into a vector db\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OllamaEmbeddings(model=\"nomic-embed-text\"))\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "9c071211e14ecf6a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Problem Statement\n",
    "\n",
    "RAG systems must route queries correctly: If routed poorly, even accurate retrieval won't help.\n",
    "\n",
    "Wrong routing can trigger irrelevant tools, prompts, or retrievers—leading to confusing results.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "User query:\n",
    "- \"Add John to the CRM\"\n",
    "\n",
    "Problem:\n",
    "- This is an action request, not a search query.\n",
    "- If routed to a retriever, it may return docs like “What is a CRM?”\n",
    "\n",
    "Potential bad outcome:\n",
    "- The LLM replies: “A CRM is a system for managing customer relationships...”\n",
    "- But the user expected the system to perform the action, not explain it."
   ],
   "id": "9f46fd3ca783a163"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #1: Logical Routing\n",
    "\n",
    "![Image 1](rsc/jupyter/logical_routing_1.png)\n",
    "\n",
    "**Idea**: Use explicit reasoning or decision logic (usually powered by an LLM) to select the best tool, retriever, or knowledge source based on the user's query.\n",
    "\n",
    "![Image 2](rsc/jupyter/logical_routing_2.png)\n",
    "\n",
    "_**Note**: Nowadays, models are fine-tuned to understand, invoke, and integrate external tools during their operation. That is to say, this idea is already baked into modern LLMs, and we just have to \"bind\" the relevant tools to the LLM, so that it knows they exist and how to use them._"
   ],
   "id": "a8a63dd3a1370623"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:35.253941Z",
     "start_time": "2025-07-14T18:05:35.213110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Route a user's query to either python_docs, js_docs, or golang_docs, based on the context of the query.\n",
    "#   - Use LLM structured output to constrain the model's output to a pydantic object.\n",
    "\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Data model (the structured output)\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\", \"golang_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with structured output\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\\n\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt.invoke({\"question\": \"What is a pydantic object?\"})"
   ],
   "id": "d0ef6e9691304102",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are an expert at routing a user question to the appropriate data source.\\n\\nBased on the programming language the question is referring to, route it to the relevant data source.', additional_kwargs={}, response_metadata={}), HumanMessage(content='What is a pydantic object?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:36.294115Z",
     "start_time": "2025-07-14T18:05:35.263367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define router\n",
    "router = prompt | structured_llm\n",
    "\n",
    "question = \"\"\"\n",
    "Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "router.invoke({\"question\": question})"
   ],
   "id": "ff80e3bd5b89fb0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python_docs')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:36.638202Z",
     "start_time": "2025-07-14T18:05:36.297886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Once we have this, it is trivial to define a branch that uses `result.datasource`\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def choose_route(result):\n",
    "    if \"python_docs\" in result.datasource.lower():\n",
    "        ### Logic here\n",
    "        return \"chain for python_docs\"\n",
    "    elif \"js_docs\" in result.datasource.lower():\n",
    "        ### Logic here\n",
    "        return \"chain for js_docs\"\n",
    "    else:\n",
    "        ### Logic here\n",
    "        return \"golang_docs\"\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)\n",
    "\n",
    "full_chain.invoke({\"question\": question})"
   ],
   "id": "1e60085950dfb9e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python_docs'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Strategy #2: Semantic Routing\n",
    "\n",
    "![Image](rsc/jupyter/semantic_routing.png)\n",
    "\n",
    "**Idea**: Use embedding similarity to route a user's query to the correct tool, agent, or sub-system based on its description (meaning)."
   ],
   "id": "7fa854dbed93500d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:36.709297Z",
     "start_time": "2025-07-14T18:05:36.652046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "# Embed prompts\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "print(len(prompt_embeddings))  # 2 embeddings\n",
    "print(len(prompt_embeddings[0]))  # 768 dimensions per embedding"
   ],
   "id": "8d91d5a0a71114f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "768\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:36.745975Z",
     "start_time": "2025-07-14T18:05:36.724524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Route question to appropriate prompt template\n",
    "def prompt_router(input):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    # Chosen prompt\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\", \"\\n\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "routing_chain = {\"query\": RunnablePassthrough()} | RunnableLambda(prompt_router)\n",
    "\n",
    "routing_chain.invoke(\"How do I find the GCD of two numbers?\")"
   ],
   "id": "4ec3d3a8aa563c49",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MATH \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='You are a very good mathematician. You are great at answering math questions. You are so good because you are able to break down hard problems into their component parts, answer the component parts, and then put them together to answer the broader question.\\n\\nHere is a question:\\nHow do I find the GCD of two numbers?')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-14T18:05:37.453839Z",
     "start_time": "2025-07-14T18:05:36.751753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "full_chain = routing_chain | llm | StrOutputParser()\n",
    "\n",
    "print(full_chain.invoke(\"What's a black hole\"))"
   ],
   "id": "a554efb455b75201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS \n",
      "\n",
      "A black hole is a region in space where the gravitational pull is so strong that nothing, including light, can escape. It's formed when a massive star collapses in on itself and its gravity becomes so strong that it warps the fabric of spacetime around it.\n",
      "\n",
      "Imagine you're standing near a super-powerful vacuum cleaner that sucks up everything that gets too close. That's essentially what a black hole is - a void in spacetime that has such intense gravitational pull that nothing can escape once it falls inside.\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
